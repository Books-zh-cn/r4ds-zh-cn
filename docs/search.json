[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Data Science (2e)",
    "section": "",
    "text": "这是正在进行的第 2 版 “R for Data Science” 的网站。 本书将教您如何使用 R 进行数据科学分析：您将学习如何将数据导入 R，将其转化为最有用的结构，对其进行转换和可视化。\n在本书中，您将找到数据科学技能的实践。 正如化学家学习如何清洁试管和储存实验室物品一样，您将学习如何清理数据和绘制图表—以及许多其他东西。 这些是让数据科学发生的技能，在这里您将找到使用 R 完成这些事情的最佳实践。 您将学习如何使用图形语法、文学编程和可重复的研究来节省时间。 您还将学习如何管理认知资源，以便在整理、可视化和探索数据时促进发现。\n本网站是永久免费的，遵守 CC BY-NC-ND 3.0 协议。如 果您想要这本书的实体版，可以在 Amazon 上订购第 1 版，或者等到 2023 年年中再订购第 2 版。如 果您喜欢免费阅读这本书并愿意回馈，请向 Kākāpō Recovery 捐款：kākāpō（出现在 R4DS 的封面上）是一种极度濒危的新西兰本土鹦鹉；只剩下 248 个了。\n如果您说另一种语言，您可能会对第 1 版的免费翻译感兴趣：\n请注意，R4DS 使用 Contributor Code of Conduct。 通过为本书做出贡献，您同意遵守其条款。"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "R for Data Science (2e)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nR4DS 由 https://www.netlify.com 托管，作为他们对开源软件和社区支持的一部分。"
  },
  {
    "objectID": "webscraping.html#introduction",
    "href": "webscraping.html#introduction",
    "title": "25  Web scraping",
    "section": "\n25.1 Introduction",
    "text": "25.1 Introduction\nThis chapter introduces you to the basics of web scraping with rvest. Web scraping is a very useful tool for extracting data from web pages. Some websites will offer an API, a set of structured HTTP requests that return data as JSON, which you handle using the techniques from Chapter 24. Where possible, you should use the API1, because typically it will give you more reliable data. Unfortunately, however, programming with web APIs is out of scope for this book. Instead, we are teaching scraping, a technique that works whether or not a site provides an API.\nIn this chapter, we’ll first discuss the ethics and legalities of scraping before we dive into the basics of HTML. You’ll then learn the basics of CSS selectors to locate specific elements on the page, and how to use rvest functions to get data from text and attributes out of HTML and into R. We’ll then discuss some techniques to figure out what CSS selector you need for the page you’re scraping, before finishing up with a couple of case studies, and a brief discussion of dynamic websites.\n\n25.1.1 Prerequisites\nIn this chapter, we’ll focus on tools provided by rvest. rvest is a member of the tidyverse, but is not a core member so you’ll need to load it explicitly. We’ll also load the full tidyverse since we’ll find it generally useful working with the data we’ve scraped.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(rvest)\n#> Warning: package 'rvest' was built under R version 4.2.1"
  },
  {
    "objectID": "webscraping.html#scraping-ethics-and-legalities",
    "href": "webscraping.html#scraping-ethics-and-legalities",
    "title": "25  Web scraping",
    "section": "\n25.2 Scraping ethics and legalities",
    "text": "25.2 Scraping ethics and legalities\nBefore we get started discussing the code you’ll need to perform web scraping, we need to talk about whether it’s legal and ethical for you to do so. Overall, the situation is complicated with regards to both of these.\nLegalities depend a lot on where you live. However, as a general principle, if the data is public, non-personal, and factual, you’re likely to be ok2. These three factors are important because they’re connected to the site’s terms and conditions, personally identifiable information, and copyright, as we’ll discuss below.\nIf the data isn’t public, non-personal, or factual or you’re scraping the data specifically to make money with it, you’ll need to talk to a lawyer. In any case, you should be respectful of the resources of the server hosting the pages you are scraping. Most importantly, this means that if you’re scraping many pages, you should make sure to wait a little between each request. One easy way to do so is to use the polite package by Dmytro Perepolkin. It will automatically pause between requests and cache the results so you never ask for the same page twice.\n\n25.2.1 Terms of service\nIf you look closely, you’ll find many websites include a “terms and conditions” or “terms of service” link somewhere on the page, and if you read that page closely you’ll often discover that the site specifically prohibits web scraping. These pages tend to be a legal land grab where companies make very broad claims. It’s polite to respect these terms of service where possible, but take any claims with a grain of salt.\nUS courts have generally found that simply putting the terms of service in the footer of the website isn’t sufficient for you to be bound by them, e.g., HiQ Labs v. LinkedIn. Generally, to be bound to the terms of service, you must have taken some explicit action like creating an account or checking a box. This is why whether or not the data is public is important; if you don’t need an account to access them, it is unlikely that you are bound to the terms of service. Note, however, the situation is rather different in Europe where courts have found that terms of service are enforceable even if you don’t explicitly agree to them.\n\n25.2.2 Personally identifiable information\nEven if the data is public, you should be extremely careful about scraping personally identifiable information like names, email addresses, phone numbers, dates of birth, etc. Europe has particularly strict laws about the collection or storage of such data (GDPR), and regardless of where you live you’re likely to be entering an ethical quagmire. For example, in 2016, a group of researchers scraped public profile information (e.g., usernames, age, gender, location, etc.) about 70,000 people on the dating site OkCupid and they publicly released these data without any attempts for anonymization. While the researchers felt that there was nothing wrong with this since the data were already public, this work was widely condemned due to ethics concerns around identifiability of users whose information was released in the dataset. If your work involves scraping personally identifiable information, we strongly recommend reading about the OkCupid study3 as well as similar studies with questionable research ethics involving the acquisition and release of personally identifiable information.\n\n25.2.3 Copyright\nFinally, you also need to worry about copyright law. Copyright law is complicated, but it’s worth taking a look at the US law which describes exactly what’s protected: “[…] original works of authorship fixed in any tangible medium of expression, […]”. It then goes on to describe specific categories that it applies like literary works, musical works, motion pictures and more. Notably absent from copyright protection are data. This means that as long as you limit your scraping to facts, copyright protection does not apply. (But note that Europe has a separate “sui generis” right that protects databases.)\nAs a brief example, in the US, lists of ingredients and instructions are not copyrightable, so copyright can not be used to protect a recipe. But if that list of recipes is accompanied by substantial novel literary content, that is copyrightable. This is why when you’re looking for a recipe on the internet there’s always so much content beforehand.\nIf you do need to scrape original content (like text or images), you may still be protected under the doctrine of fair use. Fair use is not a hard and fast rule, but weighs up a number of factors. It’s more likely to apply if you are collecting the data for research or non-commercial purposes and if you limit what you scrape to just what you need."
  },
  {
    "objectID": "webscraping.html#html-basics",
    "href": "webscraping.html#html-basics",
    "title": "25  Web scraping",
    "section": "\n25.3 HTML basics",
    "text": "25.3 HTML basics\nTo scrape webpages, you need to first understand a little bit about HTML, the language that describes web pages. HTML stands for HyperText Markup Language and looks something like this:\n<html>\n<head>\n  <title>Page title</title>\n</head>\n<body>\n  <h1 id='first'>A heading</h1>\n  <p>Some text &amp; <b>some bold text.</b></p>\n  <img src='myimg.png' width='100' height='100'>\n</body>\nHTML has a hierarchical structure formed by elements which consist of a start tag (e.g., <tag>), optional attributes (id='first'), an end tag4 (like </tag>), and contents (everything in between the start and end tag).\nSince < and > are used for start and end tags, you can’t write them directly. Instead you have to use the HTML escapes &gt; (greater than) and &lt; (less than). And since those escapes use &, if you want a literal ampersand you have to escape it as &amp;. There are a wide range of possible HTML escapes but you don’t need to worry about them too much because rvest automatically handles them for you.\nWeb scraping is possible because most pages that contain data that you want to scrape generally have a consistent structure.\n\n25.3.1 Elements\nThere are over 100 HTML elements. Some of the most important are:\n\nEvery HTML page must be in an <html> element, and it must have two children: <head>, which contains document metadata like the page title, and <body>, which contains the content you see in the browser.\nBlock tags like <h1> (heading 1), <section> (section), <p> (paragraph), and <ol> (ordered list) form the overall structure of the page.\nInline tags like <b> (bold), <i> (italics), and <a> (link) format text inside block tags.\n\nIf you encounter a tag that you’ve never seen before, you can find out what it does with a little googling. Another good place to start are the MDN Web Docs which describe just about every aspect of web programming.\nMost elements can have content in between their start and end tags. This content can either be text or more elements. For example, the following HTML contains paragraph of text, with one word in bold.\n<p>\n  Hi! My <b>name</b> is Hadley.\n</p>\nThe children are the elements it contains, so the <p> element above has one child, the <b> element. The <b> element has no children, but it does have contents (the text “name”).\n\n25.3.2 Attributes\nTags can have named attributes which look like name1='value1' name2='value2'. Two of the most important attributes are id and class, which are used in conjunction with CSS (Cascading Style Sheets) to control the visual appearance of the page. These are often useful when scraping data off a page. Attributes are also used to record the destination of links (the href attribute of <a> elements) and the source of images (the src attribute of the <img> element)."
  },
  {
    "objectID": "webscraping.html#extracting-data",
    "href": "webscraping.html#extracting-data",
    "title": "25  Web scraping",
    "section": "\n25.4 Extracting data",
    "text": "25.4 Extracting data\nTo get started scraping, you’ll need the URL of the page you want to scrape, which you can usually copy from your web browser. You’ll then need to read the HTML for that page into R with read_html(). This returns an xml_document5 object which you’ll then manipulate using rvest functions:\n\n#html <- read_html(\"http://rvest.tidyverse.org/\")\n#html\n\nrvest also includes a function that lets you write HTML inline. We’ll use this a bunch in this chapter as we teach how the various rvest functions work with simple examples.\n\n#html <- minimal_html(\"\n#  <p>This is a paragraph</p>\n#  <ul>\n#    <li>This is a bulleted list</li>\n#  </ul>\n#\")\n#html\n\nNow that you have the HTML in R, it’s time to extract the data of interest. You’ll first learn about the CSS selectors that allow you to identify the elements of interest and the rvest functions that you can use to extract data from them. Then we’ll briefly cover HTML tables, which have some special tools.\n\n25.4.1 Find elements\nCSS is short for cascading style sheets, and is a tool for defining the visual styling of HTML documents. CSS includes a miniature language for selecting elements on a page called CSS selectors. CSS selectors define patterns for locating HTML elements, and are useful for scraping because they provide a concise way of describing which elements you want to extract.\nWe’ll come back to CSS selectors in more detail in Section 25.5, but luckily you can get a long way with just three:\n\np selects all <p> elements.\n.title selects all elements with class “title”.\n#title selects the element with the id attribute that equals “title”. Id attributes must be unique within a document, so this will only ever select a single element.\n\nLet’s try out these selectors with a simple example:\n\n#html <- minimal_html(\"\n#  <h1>This is a heading</h1>\n#  <p id='first'>This is a paragraph</p>\n#  <p class='important'>This is an important paragraph</p>\n#\")\n\nUse html_elements() to find all elements that match the selector:\n\n#html |> html_elements(\"p\")\n#html |> html_elements(\".important\")\n#html |> html_elements(\"#first\")\n\nAnother important function is html_element() which always returns the same number of outputs as inputs. If you apply it to a whole document it’ll give you the first match:\n\n#html |> html_element(\"p\")\n\nThere’s an important difference between html_element() and html_elements() when you use a selector that doesn’t match any elements. html_elements() returns a vector of length 0, where html_element() returns a missing value. This will be important shortly.\n\n#html |> html_elements(\"b\")\n#html |> html_element(\"b\")\n\n\n25.4.2 Nesting selections\nIn most cases, you’ll use html_elements() and html_element() together, typically using html_elements() to identify elements that will become observations then using html_element() to find elements that will become variables. Let’s see this in action using a simple example. Here we have an unordered list (<ul>) where each list item (<li>) contains some information about four characters from StarWars:\n\n#html <- minimal_html(\"\n#  <ul>\n#    <li><b>C-3PO</b> is a <i>droid</i> that weighs <span class='weight'>167 #kg</span></li>\n#    <li><b>R4-P17</b> is a <i>droid</i></li>\n#    <li><b>R2-D2</b> is a <i>droid</i> that weighs <span class='weight'>96 #kg</span></li>\n#    <li><b>Yoda</b> weighs <span class='weight'>66 kg</span></li>\n#  </ul>\n#  \")\n\nWe can use html_elements() to make a vector where each element corresponds to a different character:\n\n#characters <- html |> html_elements(\"li\")\n#characters\n\nTo extract the name of each character, we use html_element(), because when applied to the output of html_elements() it’s guaranteed to return one response per element:\n\n#characters |> html_element(\"b\")\n\nThe distinction between html_element() and html_elements() isn’t important for name, but it is important for weight. We want to get one weight for each character, even if there’s no weight <span>. That’s what html_element() does:\n\n#characters |> html_element(\".weight\")\n\nhtml_elements() finds all weight <span>s that are children of characters. There’s only three of these, so we lose the connection between names and weights:\n\n#characters |> html_elements(\".weight\")\n\nNow that you’ve selected the elements of interest, you’ll need to extract the data, either from the text contents or some attributes.\n\n25.4.3 Text and attributes\nhtml_text2()6 extracts the plain text contents of an HTML element:\n\n#characters |> \n#  html_element(\"b\") |> \n#  html_text2()\n\n#characters |> \n#  html_element(\".weight\") |> \n#  html_text2()\n\nNote that any escapes will be automatically handled; you’ll only ever see HTML escapes in the source HTML, not in the data returned by rvest.\nhtml_attr() extracts data from attributes:\n\n#html <- minimal_html(\"\n#  <p><a href='https://en.wikipedia.org/wiki/Cat'>cats</a></p>\n#  <p><a href='https://en.wikipedia.org/wiki/Dog'>dogs</a></p>\n#\")\n\n#html |> \n#  html_elements(\"p\") |> \n#  html_element(\"a\") |> \n#  html_attr(\"href\")\n\nhtml_attr() always returns a string, so if you’re extracting numbers or dates, you’ll need to do some post-processing.\n\n25.4.4 Tables\nIf you’re lucky, your data will be already stored in an HTML table, and it’ll be a matter of just reading it from that table. It’s usually straightforward to recognize a table in your browser: it’ll have a rectangular structure of rows and columns, and you can copy and paste it into a tool like Excel.\nHTML tables are built up from four main elements: <table>, <tr> (table row), <th> (table heading), and <td> (table data). Here’s a simple HTML table with two columns and three rows:\n\n#html <- minimal_html(\"\n#  <table class='mytable'>\n#    <tr><th>x</th>   <th>y</th></tr>\n#    <tr><td>1.5</td> <td>2.7</td></tr>\n#    <tr><td>4.9</td> <td>1.3</td></tr>\n#    <tr><td>7.2</td> <td>8.1</td></tr>\n#  </table>\n#  \")\n\nrvest provides a function that knows how to read this sort of data: html_table(). It returns a list containing one tibble for each table found on the page. Use html_element() to identify the table you want to extract:\n\n#html |> \n#  html_element(\".mytable\") |> \n#  html_table()\n\nNote that x and y have automatically been converted to numbers. This automatic conversion doesn’t always work, so in more complex scenarios you may want to turn it off with convert = FALSE and then do your own conversion."
  },
  {
    "objectID": "webscraping.html#sec-css-selectors",
    "href": "webscraping.html#sec-css-selectors",
    "title": "25  Web scraping",
    "section": "\n25.5 Finding the right selectors",
    "text": "25.5 Finding the right selectors\nFiguring out the selector you need for your data is typically the hardest part of the problem. You’ll often need to do some experimenting to find a selector that is both specific (i.e. it doesn’t select things you don’t care about) and sensitive (i.e. it does select everything you care about). Lots of trial and error is a normal part of the process! There are two main tools that are available to help you with this process: SelectorGadget and your browser’s developer tools.\nSelectorGadget is a javascript bookmarklet that automatically generates CSS selectors based on the positive and negative examples that you provide. It doesn’t always work, but when it does, it’s magic! You can learn how to install and use SelectorGadget either by reading https://rvest.tidyverse.org/articles/selectorgadget.html or watching Mine’s video at https://www.youtube.com/watch?v=PetWV5g1Xsc.\nEvery modern browser comes with some toolkit for developers, but we recommend Chrome, even if it isn’t your regular browser: its web developer tools are some of the best and they’re immediately available. Right click on an element on the page and click Inspect. This will open an expandable view of the complete HTML page, centered on the element that you just clicked. You can use this to explore the page and get a sense of what selectors might work. Pay particular attention to the class and id attributes, since these are often used to form the visual structure of the page, and hence make for good tools to extract the data that you’re looking for.\nInside the Elements view, you can also right click on an element and choose Copy as Selector to generate a selector that will uniquely identify the element of interest.\nIf either SelectorGadget or Chrome DevTools have generated a CSS selector that you don’t understand, try Selectors Explained which translates CSS selectors into plain English. If you find yourself doing this a lot, you might want to learn more about CSS selectors generally. We recommend starting with the fun CSS dinner tutorial and then referring to the MDN web docs."
  },
  {
    "objectID": "webscraping.html#putting-it-all-together",
    "href": "webscraping.html#putting-it-all-together",
    "title": "25  Web scraping",
    "section": "\n25.6 Putting it all together",
    "text": "25.6 Putting it all together\nLet’s put this all together to scrape some websites. There’s some risk that these examples may no longer work when you run them — that’s the fundamental challenge of web scraping; if the structure of the site changes, then you’ll have to change your scraping code.\n\n25.6.1 StarWars\nrvest includes a very simple example in vignette(\"starwars\"). This is a simple page with minimal HTML so it’s a good place to start. I’d encourage you to navigate to that page now and use “Inspect Element” to inspect one of the headings that’s the title of a Star Wars movie. Use the keyboard or mouse to explore the hierarchy of the HTML and see if you can get a sense of the shared structure used by each movie.\nYou should be able to see that each movie has a shared structure that looks like this:\n<section>\n  <h2 data-id=\"1\">The Phantom Menace</h2>\n  <p>Released: 1999-05-19</p>\n  <p>Director: <span class=\"director\">George Lucas</span></p>\n  \n  <div class=\"crawl\">\n    <p>...</p>\n    <p>...</p>\n    <p>...</p>\n  </div>\n</section>\nOur goal is to turn this data into a 7 row data frame with variables title, year, director, and intro. We’ll start by reading the HTML and extracting all the <section> elements:\n\nurl <- \"https://rvest.tidyverse.org/articles/starwars.html\"\nhtml <- read_html(url)\n\nsection <- html |> html_elements(\"section\")\nsection\n#> {xml_nodeset (7)}\n#> [1] <section><h2 data-id=\"1\">\\nThe Phantom Menace\\n</h2>\\n<p>\\nReleased: 1 ...\n#> [2] <section><h2 data-id=\"2\">\\nAttack of the Clones\\n</h2>\\n<p>\\nReleased: ...\n#> [3] <section><h2 data-id=\"3\">\\nRevenge of the Sith\\n</h2>\\n<p>\\nReleased:  ...\n#> [4] <section><h2 data-id=\"4\">\\nA New Hope\\n</h2>\\n<p>\\nReleased: 1977-05-2 ...\n#> [5] <section><h2 data-id=\"5\">\\nThe Empire Strikes Back\\n</h2>\\n<p>\\nReleas ...\n#> [6] <section><h2 data-id=\"6\">\\nReturn of the Jedi\\n</h2>\\n<p>\\nReleased: 1 ...\n#> [7] <section><h2 data-id=\"7\">\\nThe Force Awakens\\n</h2>\\n<p>\\nReleased: 20 ...\n\nThis retrieves seven elements matching the seven movies found on that page, suggesting that using section as a selector is good. Extracting the individual elements is straightforward since the data is always found in the text. It’s just a matter of finding the right selector:\n\nsection |> html_element(\"h2\") |> html_text2()\n#> [1] \"The Phantom Menace\"      \"Attack of the Clones\"   \n#> [3] \"Revenge of the Sith\"     \"A New Hope\"             \n#> [5] \"The Empire Strikes Back\" \"Return of the Jedi\"     \n#> [7] \"The Force Awakens\"\n\nsection |> html_element(\".director\") |> html_text2()\n#> [1] \"George Lucas\"     \"George Lucas\"     \"George Lucas\"    \n#> [4] \"George Lucas\"     \"Irvin Kershner\"   \"Richard Marquand\"\n#> [7] \"J. J. Abrams\"\n\nOnce we’ve done that for each component, we can wrap all the results up into a tibble:\n\ntibble(\n  title = section |> \n    html_element(\"h2\") |> \n    html_text2(),\n  released = section |> \n    html_element(\"p\") |> \n    html_text2() |> \n    str_remove(\"Released: \") |> \n    parse_date(),\n  director = section |> \n    html_element(\".director\") |> \n    html_text2(),\n  intro = section |> \n    html_element(\".crawl\") |> \n    html_text2()\n)\n#> # A tibble: 7 × 4\n#>   title                   released   director         intro                  \n#>   <chr>                   <date>     <chr>            <chr>                  \n#> 1 The Phantom Menace      1999-05-19 George Lucas     \"Turmoil has engulfed …\n#> 2 Attack of the Clones    2002-05-16 George Lucas     \"There is unrest in th…\n#> 3 Revenge of the Sith     2005-05-19 George Lucas     \"War! The Republic is …\n#> 4 A New Hope              1977-05-25 George Lucas     \"It is a period of civ…\n#> 5 The Empire Strikes Back 1980-05-17 Irvin Kershner   \"It is a dark time for…\n#> 6 Return of the Jedi      1983-05-25 Richard Marquand \"Luke Skywalker has re…\n#> # ℹ 1 more row\n\nWe did a little more processing of released to get a variable that will be easy to use later in our analysis.\n\n25.6.2 IMDB top films\nFor our next task we’ll tackle something a little trickier, extracting the top 250 movies from the internet movie database (IMDb). At the time we wrote this chapter, the page looked like Figure 25.1.\n\n\n\n\nFigure 25.1: Screenshot of the IMDb top movies web page taken on 2022-12-05.\n\n\n\n\nThis data has a clear tabular structure so it’s worth starting with html_table():\n\n#url <- \"https://www.imdb.com/chart/top\"\n#html <- read_html(url)\n\n#table <- html |> \n#  html_element(\"table\") |> \n#  html_table()\n#table\n\nThis includes a few empty columns, but overall does a good job of capturing the information from the table. However, we need to do some more processing to make it easier to use. First, we’ll rename the columns to be easier to work with, and remove the extraneous whitespace in rank and title. We will do this with select() (instead of rename()) to do the renaming and selecting of just these two columns in one step. Then we’ll remove the new lines and extra spaces, and then apply separate_wider_regex() (from Section 16.3.4) to pull out the title, year, and rank into their own variables.\n\n#ratings <- table |>\n#  select(\n#    rank_title_year = `Rank & Title`,\n#    rating = `IMDb Rating`\n#  ) |> \n#  mutate(\n#    rank_title_year = str_replace_all(rank_title_year, \"\\n +\", \" \")\n#  ) |> \n#  separate_wider_regex(\n#    rank_title_year,\n#    patterns = c(\n#      rank = \"\\\\d+\", \"\\\\. \",\n#      title = \".+\", \" +\\\\(\",\n#      year = \"\\\\d+\", \"\\\\)\"\n#    )\n#  )\n#ratings\n\nEven in this case where most of the data comes from table cells, it’s still worth looking at the raw HTML. If you do so, you’ll discover that we can add a little extra data by using one of the attributes. This is one of the reasons it’s worth spending a little time spelunking the source of the page; you might find extra data, or might find a parsing route that’s slightly easier.\n\n#html |> \n#  html_elements(\"td strong\") |> \n#  head() |> \n#  html_attr(\"title\")\n\nWe can combine this with the tabular data and again apply separate_wider_regex() to extract out the bit of data we care about:\n\n#ratings |>\n#  mutate(\n#    rating_n = html |> html_elements(\"td strong\") |> html_attr(\"title\")\n#  ) |> \n#  separate_wider_regex(\n#    rating_n,\n#    patterns = c(\n#      \"[0-9.]+ based on \",\n#      number = \"[0-9,]+\",\n#      \" user ratings\"\n#    )\n#  ) |> \n#  mutate(\n#    number = parse_number(number)\n#  )"
  },
  {
    "objectID": "webscraping.html#dynamic-sites",
    "href": "webscraping.html#dynamic-sites",
    "title": "25  Web scraping",
    "section": "\n25.7 Dynamic sites",
    "text": "25.7 Dynamic sites\nSo far we have focused on websites where html_elements() returns what you see in the browser and discussed how to parse what it returns and how to organize that information in tidy data frames. From time-to-time, however, you’ll hit a site where html_elements() and friends don’t return anything like what you see in the browser. In many cases, that’s because you’re trying to scrape a website that dynamically generates the content of the page with javascript. This doesn’t currently work with rvest, because rvest downloads the raw HTML and doesn’t run any javascript.\nIt’s still possible to scrape these types of sites, but rvest needs to use a more expensive process: fully simulating the web browser including running all javascript. This functionality is not available at the time of writing, but it’s something we’re actively working on and might be available by the time you read this. It uses the chromote package which actually runs the Chrome browser in the background, and gives you additional tools to interact with the site, like a human typing text and clicking buttons. Check out the rvest website for more details."
  },
  {
    "objectID": "webscraping.html#summary",
    "href": "webscraping.html#summary",
    "title": "25  Web scraping",
    "section": "\n25.8 Summary",
    "text": "25.8 Summary\nIn this chapter, you’ve learned about the why, the why not, and the how of scraping data from web pages. First, you’ve learned about the basics of HTML and using CSS selectors to refer to specific elements, then you’ve learned about using the rvest package to get data out of HTML into R. We then demonstrated web scraping with two case studies: a simpler scenario on scraping data on StarWars films from the rvest package website and a more complex scenario on scraping the top 250 films from IMDB.\nTechnical details of scraping data off the web can be complex, particularly when dealing with sites, however legal and ethical considerations can be even more complex. It’s important for you to educate yourself about both of these before setting out to scrape data.\nThis brings us to the end of the import part of the book where you’ve learned techniques to get data from where it lives (spreadsheets, databases, JSON files, and web sites) into a tidy form in R. Now it’s time to turn our sights to a new topic: making the most of R as a programming language."
  },
  {
    "objectID": "webscraping.html#footnotes",
    "href": "webscraping.html#footnotes",
    "title": "25  Web scraping",
    "section": "",
    "text": "And many popular APIs already have CRAN packages that wrap them, so start with a little research first!↩︎\nObviously we’re not lawyers, and this is not legal advice. But this is the best summary we can give having read a bunch about this topic.↩︎\nOne example of an article on the OkCupid study was published by Wired, https://www.wired.com/2016/05/okcupid-study-reveals-perils-big-data-science.↩︎\nA number of tags (including &lt;p&gt; and &lt;li&gt;) don’t require end tags, but we think it’s best to include them because it makes seeing the structure of the HTML a little easier.↩︎\nThis class comes from the xml2 package. xml2 is a low-level package that rvest builds on top of.↩︎\nrvest also provides html_text() but you should almost always use html_text2() since it does a better job of converting nested HTML to text.↩︎"
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "In this part of the book, you’ll improve your programming skills. Programming is a cross-cutting skill needed for all data science work: you must use a computer to do data science; you cannot do it in your head, or with pencil and paper.\n\n\n\n\nFigure 1: Programming is the water in which all the other components swim.\n\n\n\n\nProgramming produces code, and code is a tool of communication. Obviously code tells the computer what you want it to do. But it also communicates meaning to other humans. Thinking about code as a vehicle for communication is important because every project you do is fundamentally collaborative. Even if you’re not working with other people, you’ll definitely be working with future-you! Writing clear code is important so that others (like future-you) can understand why you tackled an analysis in the way you did. That means getting better at programming also involves getting better at communicating. Over time, you want your code to become not just easier to write, but easier for others to read.\nIn the following three chapters, you’ll learn skills to improve your programming skills:\n\nCopy-and-paste is a powerful tool, but you should avoid doing it more than twice. Repeating yourself in code is dangerous because it can easily lead to errors and inconsistencies. Instead, in Chapter 26, you’ll learn how to write functions which let you extract out repeated tidyverse code so that it can be easily reused.\nFunctions extract out repeated code, but you often need to repeat the same actions on different inputs. You need tools for iteration that let you do similar things again and again. These tools include for loops and functional programming, which you’ll learn about in Chapter 27.\nAs you read more code written by others, you’ll see more code that doesn’t use the tidyverse. In ?sec-base-r, you’ll learn some of the most important base R functions that you’ll see in the wild.\n\nThe goal of these chapters is to teach you the minimum about programming that you need for data science. Once you have mastered the material here, we strongly recommend that you continue to invest in your programming skills. We’ve written two books that you might find helpful. Hands on Programming with R, by Garrett Grolemund, is an introduction to R as a programming language and is a great place to start if R is your first programming language. Advanced R by Hadley Wickham dives into the details of R the programming language; it’s great place to start if you have existing programming experience and great next step once you’ve internalized the ideas in these chapters."
  },
  {
    "objectID": "functions.html#introduction",
    "href": "functions.html#introduction",
    "title": "26  Functions",
    "section": "\n26.1 Introduction",
    "text": "26.1 Introduction\nOne of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\nIt makes it easier to reuse work from project-to-project, increasing your productivity over time.\n\nA good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code). In this chapter, you’ll learn about three useful types of functions:\n\nVector functions take one or more vectors as input and return a vector as output.\nData frame functions take a data frame as input and return a data frame as output.\nPlot functions that take a data frame as input and return a plot as output.\n\nEach of these sections include many examples to help you generalize the patterns that you see. These examples wouldn’t be possible without the help of folks of twitter, and we encourage follow the links in the comment to see original inspirations. You might also want to read the original motivating tweets for general functions and plotting functions to see even more functions.\n\n26.1.1 Prerequisites\nWe’ll wrap up a variety of functions from around the tidyverse. We’ll also use nycflights13 as a source of familiar data to use our functions with.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3"
  },
  {
    "objectID": "functions.html#vector-functions",
    "href": "functions.html#vector-functions",
    "title": "26  Functions",
    "section": "\n26.2 Vector functions",
    "text": "26.2 Vector functions\nWe’ll begin with vector functions: functions that take one or more vectors and return a vector result. For example, take a look at this code. What does it do?\n\ndf <- tibble(\n  a = rnorm(5),\n  b = rnorm(5),\n  c = rnorm(5),\n  d = rnorm(5),\n)\n\ndf |> mutate(\n  a = (a - min(a, na.rm = TRUE)) / \n    (max(a, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  b = (b - min(b, na.rm = TRUE)) / \n    (max(b, na.rm = TRUE) - min(a, na.rm = TRUE)),\n  c = (c - min(c, na.rm = TRUE)) / \n    (max(c, na.rm = TRUE) - min(c, na.rm = TRUE)),\n  d = (d - min(d, na.rm = TRUE)) / \n    (max(d, na.rm = TRUE) - min(d, na.rm = TRUE)),\n)\n#> # A tibble: 5 × 4\n#>       a     b     c     d\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1 0.339  2.59 0.291 0    \n#> 2 0.880  0    0.611 0.557\n#> 3 0      1.37 1     0.752\n#> 4 0.795  1.37 0     1    \n#> 5 1      1.34 0.580 0.394\n\nYou might be able to puzzle out that this rescales each column to have a range from 0 to 1. But did you spot the mistake? When Hadley wrote this code he made an error when copying-and-pasting and forgot to change an a to a b. Preventing this type of mistake is one very good reason to learn how to write functions.\n\n26.2.1 Writing a function\nTo write a function you need to first analyse your repeated code to figure what parts are constant and what parts vary. If we take the code above and pull it outside of mutate(), it’s a little easier to see the pattern because each repetition is now one line:\n\n(a - min(a, na.rm = TRUE)) / (max(a, na.rm = TRUE) - min(a, na.rm = TRUE))\n(b - min(b, na.rm = TRUE)) / (max(b, na.rm = TRUE) - min(b, na.rm = TRUE))\n(c - min(c, na.rm = TRUE)) / (max(c, na.rm = TRUE) - min(c, na.rm = TRUE))\n(d - min(d, na.rm = TRUE)) / (max(d, na.rm = TRUE) - min(d, na.rm = TRUE))  \n\nTo make this a bit clearer we can replace the bit that varies with █:\n\n(█ - min(█, na.rm = TRUE)) / (max(█, na.rm = TRUE) - min(█, na.rm = TRUE))\n\nTo turn this into a function you need three things:\n\nA name. Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.\nThe arguments. The arguments are things that vary across calls and our analysis above tells us that we have just one. We’ll call it x because this is the conventional name for a numeric vector.\nThe body. The body is the code that’s repeated across all the calls.\n\nThen you create a function by following the template:\n\nname <- function(arguments) {\n  body\n}\n\nFor this case that leads to:\n\nrescale01 <- function(x) {\n  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n}\n\nAt this point you might test with a few simple inputs to make sure you’ve captured the logic correctly:\n\nrescale01(c(-10, 0, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(1, 2, 3, NA, 5))\n#> [1] 0.00 0.25 0.50   NA 1.00\n\nThen you can rewrite the call to mutate() as:\n\ndf |> mutate(\n  a = rescale01(a),\n  b = rescale01(b),\n  c = rescale01(c),\n  d = rescale01(d),\n)\n#> # A tibble: 5 × 4\n#>       a     b     c     d\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1 0.339 1     0.291 0    \n#> 2 0.880 0     0.611 0.557\n#> 3 0     0.530 1     0.752\n#> 4 0.795 0.531 0     1    \n#> 5 1     0.518 0.580 0.394\n\n(In Chapter 27, you’ll learn how to use across() to reduce the duplication even further so all you need is df |> mutate(across(a:d, rescale01))).\n\n26.2.2 Improving our function\nYou might notice that the rescale01() function does some unnecessary work — instead of computing min() twice and max() once we could instead compute both the minimum and maximum in one step with range():\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\nOr you might try this function on a vector that includes an infinite value:\n\nx <- c(1:10, Inf)\nrescale01(x)\n#>  [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nThat result is not particularly useful so we could ask range() to ignore infinite values:\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\nrescale01(x)\n#>  [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n#>  [8] 0.7777778 0.8888889 1.0000000       Inf\n\nThese changes illustrate an important benefit of functions: because we’ve moved the repeated code into a function, we only need to make the change in one place.\n\n26.2.3 Mutate functions\nNow you’ve got the basic idea of functions, let’s take a look at a whole bunch of examples. We’ll start by looking at “mutate” functions, i.e. functions that work well inside of mutate() and filter() because they return an output of the same length as the input.\nLet’s start with a simple variation of rescale01(). Maybe you want to compute the Z-score, rescaling a vector to have a mean of zero and a standard deviation of one:\n\nz_score <- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nOr maybe you want to wrap up a straightforward case_when() and give it a useful name. For example, this clamp() function ensures all values of a vector lie in between a minimum or a maximum:\n\nclamp <- function(x, min, max) {\n  case_when(\n    x < min ~ min,\n    x > max ~ max,\n    .default = x\n  )\n}\n\nclamp(1:10, min = 3, max = 7)\n#>  [1] 3 3 3 4 5 6 7 7 7 7\n\nOf course functions don’t just need to work with numeric variables. You might want to do some repeated string manipulation. Maybe you need to make the first character upper case:\n\nfirst_upper <- function(x) {\n  str_sub(x, 1, 1) <- str_to_upper(str_sub(x, 1, 1))\n  x\n}\n\nfirst_upper(\"hello\")\n#> [1] \"Hello\"\n\nOr maybe you want to strip percent signs, commas, and dollar signs from a string before converting it into a number:\n\n# https://twitter.com/NVlabormarket/status/1571939851922198530\nclean_number <- function(x) {\n  is_pct <- str_detect(x, \"%\")\n  num <- x |> \n    str_remove_all(\"%\") |> \n    str_remove_all(\",\") |> \n    str_remove_all(fixed(\"$\")) |> \n    as.numeric(x)\n  if_else(is_pct, num / 100, num)\n}\n\nclean_number(\"$12,300\")\n#> [1] 12300\nclean_number(\"45%\")\n#> [1] 0.45\n\nSometimes your functions will be highly specialized for one data analysis step. For example, if you have a bunch of variables that record missing values as 997, 998, or 999, you might want to write a function to replace them with NA:\n\nfix_na <- function(x) {\n  if_else(x %in% c(997, 998, 999), NA, x)\n}\n\nWe’ve focused on examples that take a single vector because we think they’re the most common. But there’s no reason that your function can’t take multiple vector inputs.\n\n26.2.4 Summary functions\nAnother important family of vector functions is summary functions, functions that return a single value for use in summarize(). Sometimes this can just be a matter of setting a default argument or two:\n\ncommas <- function(x) {\n  str_flatten(x, collapse = \", \", last = \" and \")\n}\n\ncommas(c(\"cat\", \"dog\", \"pigeon\"))\n#> [1] \"cat, dog and pigeon\"\n\nOr you might wrap up a simple computation, like for the coefficient of variation, which divides the standard deviation by the mean:\n\ncv <- function(x, na.rm = FALSE) {\n  sd(x, na.rm = na.rm) / mean(x, na.rm = na.rm)\n}\n\ncv(runif(100, min = 0, max = 50))\n#> [1] 0.5196276\ncv(runif(100, min = 0, max = 500))\n#> [1] 0.5652554\n\nOr maybe you just want to make a common pattern easier to remember by giving it a memorable name:\n\n# https://twitter.com/gbganalyst/status/1571619641390252033\nn_missing <- function(x) {\n  sum(is.na(x))\n} \n\nYou can also write functions with multiple vector inputs. For example, maybe you want to compute the mean absolute prediction error to help you compare model predictions with actual values:\n\n# https://twitter.com/neilgcurrie/status/1571607727255834625\nmape <- function(actual, predicted) {\n  sum(abs((actual - predicted) / actual)) / length(actual)\n}\n\n\n\n\n\n\n\nRStudio\n\n\n\nOnce you start writing functions, there are two RStudio shortcuts that are super useful:\n\nTo find the definition of a function that you’ve written, place the cursor on the name of the function and press F2.\nTo quickly jump to a function, press Ctrl + . to open the fuzzy file and function finder and type the first few letters of your function name. You can also navigate to files, Quarto sections, and more, making it a very handy navigation tool.\n\n\n\n\n26.2.5 Exercises\n\n\nPractice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need?\n\nmean(is.na(x))\nmean(is.na(y))\nmean(is.na(z))\n\nx / sum(x, na.rm = TRUE)\ny / sum(y, na.rm = TRUE)\nz / sum(z, na.rm = TRUE)\n\nround(x / sum(x, na.rm = TRUE) * 100, 1)\nround(y / sum(y, na.rm = TRUE) * 100, 1)\nround(z / sum(z, na.rm = TRUE) * 100, 1)\n\n\nIn the second variant of rescale01(), infinite values are left unchanged. Can you rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1?\nGiven a vector of birthdates, write a function to compute the age in years.\nWrite your own functions to compute the variance and skewness of a numeric vector. You can look up the definitions on Wikipedia or elsewhere.\nWrite both_na(), a summary function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.\n\nRead the documentation to figure out what the following functions do. Why are they useful even though they are so short?\n\nis_directory <- function(x) {\n  file.info(x)$isdir\n}\nis_readable <- function(x) {\n  file.access(x, 4) == 0\n}"
  },
  {
    "objectID": "functions.html#data-frame-functions",
    "href": "functions.html#data-frame-functions",
    "title": "26  Functions",
    "section": "\n26.3 Data frame functions",
    "text": "26.3 Data frame functions\nVector functions are useful for pulling out code that’s repeated within a dplyr verb. But you’ll often also repeat the verbs themselves, particularly within a large pipeline. When you notice yourself copying and pasting multiple verbs multiple times, you might think about writing a data frame function. Data frame functions work like dplyr verbs: they take a data frame as the first argument, some extra arguments that say what to do with it, and return a data frame or vector.\nTo let you write a function that uses dplyr verbs, we’ll first introduce you to the challenge of indirection and how you can overcome it with embracing, {{ }}. With this theory under your belt, we’ll then show you a bunch of examples to illustrate what you might do with it.\n\n26.3.1 Indirection and tidy evaluation\nWhen you start writing functions that use dplyr verbs you rapidly hit the problem of indirection. Let’s illustrate the problem with a very simple function: grouped_mean(). The goal of this function is to compute the mean of mean_var grouped by group_var:\n\ngrouped_mean <- function(df, group_var, mean_var) {\n  df |> \n    group_by(group_var) |> \n    summarize(mean(mean_var))\n}\n\nIf we try and use it, we get an error:\n\ndiamonds |> grouped_mean(cut, carat)\n#> Error in `group_by()`:\n#> ! Must group by variables found in `.data`.\n#> ✖ Column `group_var` is not found.\n\nTo make the problem a bit more clear, we can use a made up data frame:\n\ndf <- tibble(\n  mean_var = 1,\n  group_var = \"g\",\n  group = 1,\n  x = 10,\n  y = 100\n)\n\ndf |> grouped_mean(group, x)\n#> # A tibble: 1 × 2\n#>   group_var `mean(mean_var)`\n#>   <chr>                <dbl>\n#> 1 g                        1\ndf |> grouped_mean(group, y)\n#> # A tibble: 1 × 2\n#>   group_var `mean(mean_var)`\n#>   <chr>                <dbl>\n#> 1 g                        1\n\nRegardless of how we call grouped_mean() it always does df |> group_by(group_var) |> summarize(mean(mean_var)), instead of df |> group_by(group) |> summarize(mean(x)) or df |> group_by(group) |> summarize(mean(y)). This is a problem of indirection, and it arises because dplyr uses tidy evaluation to allow you to refer to the names of variables inside your data frame without any special treatment.\nTidy evaluation is great 95% of the time because it makes your data analyses very concise as you never have to say which data frame a variable comes from; it’s obvious from the context. The downside of tidy evaluation comes when we want to wrap up repeated tidyverse code into a function. Here we need some way to tell group_mean() and summarize() not to treat group_var and mean_var as the name of the variables, but instead look inside them for the variable we actually want to use.\nTidy evaluation includes a solution to this problem called embracing 🤗. Embracing a variable means to wrap it in braces so (e.g.) var becomes {{ var }}. Embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name. One way to remember what’s happening is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var.\nSo to make grouped_mean() work, we need to surround group_var and mean_var with {{ }}:\n\ngrouped_mean <- function(df, group_var, mean_var) {\n  df |> \n    group_by({{ group_var }}) |> \n    summarize(mean({{ mean_var }}))\n}\n\ndf |> grouped_mean(group, x)\n#> # A tibble: 1 × 2\n#>   group `mean(x)`\n#>   <dbl>     <dbl>\n#> 1     1        10\n\nSuccess!\n\n26.3.2 When to embrace?\nSo the key challenge in writing data frame functions is figuring out which arguments need to be embraced. Fortunately, this is easy because you can look it up from the documentation 😄. There are two terms to look for in the docs which correspond to the two most common sub-types of tidy evaluation:\n\nData-masking: this is used in functions like arrange(), filter(), and summarize() that compute with variables.\nTidy-selection: this is used for functions like select(), relocate(), and rename() that select variables.\n\nYour intuition about which arguments use tidy evaluation should be good for many common functions — just think about whether you can compute (e.g., x + 1) or select (e.g., a:x).\nIn the following sections, we’ll explore the sorts of handy functions you might write once you understand embracing.\n\n26.3.3 Common use cases\nIf you commonly perform the same set of summaries when doing initial data exploration, you might consider wrapping them up in a helper function:\n\nsummary6 <- function(data, var) {\n  data |> summarize(\n    min = min({{ var }}, na.rm = TRUE),\n    mean = mean({{ var }}, na.rm = TRUE),\n    median = median({{ var }}, na.rm = TRUE),\n    max = max({{ var }}, na.rm = TRUE),\n    n = n(),\n    n_miss = sum(is.na({{ var }})),\n    .groups = \"drop\"\n  )\n}\n\ndiamonds |> summary6(carat)\n#> # A tibble: 1 × 6\n#>     min  mean median   max     n n_miss\n#>   <dbl> <dbl>  <dbl> <dbl> <int>  <int>\n#> 1   0.2 0.798    0.7  5.01 53940      0\n\n(Whenever you wrap summarize() in a helper, we think it’s good practice to set .groups = \"drop\" to both avoid the message and leave the data in an ungrouped state.)\nThe nice thing about this function is, because it wraps summarize(), you can use it on grouped data:\n\ndiamonds |> \n  group_by(cut) |> \n  summary6(carat)\n#> # A tibble: 5 × 7\n#>   cut         min  mean median   max     n n_miss\n#>   <ord>     <dbl> <dbl>  <dbl> <dbl> <int>  <int>\n#> 1 Fair       0.22 1.05    1     5.01  1610      0\n#> 2 Good       0.23 0.849   0.82  3.01  4906      0\n#> 3 Very Good  0.2  0.806   0.71  4    12082      0\n#> 4 Premium    0.2  0.892   0.86  4.01 13791      0\n#> 5 Ideal      0.2  0.703   0.54  3.5  21551      0\n\nFurthermore, since the arguments to summarize are data-masking also means that the var argument to summary6() is data-masking. That means you can also summarize computed variables:\n\ndiamonds |> \n  group_by(cut) |> \n  summary6(log10(carat))\n#> # A tibble: 5 × 7\n#>   cut          min    mean  median   max     n n_miss\n#>   <ord>      <dbl>   <dbl>   <dbl> <dbl> <int>  <int>\n#> 1 Fair      -0.658 -0.0273  0      0.700  1610      0\n#> 2 Good      -0.638 -0.133  -0.0862 0.479  4906      0\n#> 3 Very Good -0.699 -0.164  -0.149  0.602 12082      0\n#> 4 Premium   -0.699 -0.125  -0.0655 0.603 13791      0\n#> 5 Ideal     -0.699 -0.225  -0.268  0.544 21551      0\n\nTo summarize multiple variables, you’ll need to wait until Section 27.2, where you’ll learn how to use across().\nAnother popular summarize() helper function is a version of count() that also computes proportions:\n\n# https://twitter.com/Diabb6/status/1571635146658402309\ncount_prop <- function(df, var, sort = FALSE) {\n  df |>\n    count({{ var }}, sort = sort) |>\n    mutate(prop = n / sum(n))\n}\n\ndiamonds |> count_prop(clarity)\n#> # A tibble: 8 × 3\n#>   clarity     n   prop\n#>   <ord>   <int>  <dbl>\n#> 1 I1        741 0.0137\n#> 2 SI2      9194 0.170 \n#> 3 SI1     13065 0.242 \n#> 4 VS2     12258 0.227 \n#> 5 VS1      8171 0.151 \n#> 6 VVS2     5066 0.0939\n#> # ℹ 2 more rows\n\nThis function has three arguments: df, var, and sort, and only var needs to be embraced because it’s passed to count() which uses data-masking for all variables. Note that we use a default value for sort so that if the user doesn’t supply their own value it will default to FALSE.\nOr maybe you want to find the sorted unique values of a variable for a subset of the data. Rather than supplying a variable and a value to do the filtering, we’ll allow the user to supply a condition:\n\nunique_where <- function(df, condition, var) {\n  df |> \n    filter({{ condition }}) |> \n    distinct({{ var }}) |> \n    arrange({{ var }})\n}\n\n# Find all the destinations in December\nflights |> unique_where(month == 12, dest)\n#> # A tibble: 96 × 1\n#>   dest \n#>   <chr>\n#> 1 ABQ  \n#> 2 ALB  \n#> 3 ATL  \n#> 4 AUS  \n#> 5 AVL  \n#> 6 BDL  \n#> # ℹ 90 more rows\n\nHere we embrace condition because it’s passed to filter() and var because it’s passed to distinct() and arrange().\nWe’ve made all these examples to take a data frame as the first argument, but if you’re working repeatedly with the same data, it can make sense to hardcode it. For example, the following function always works with the flights dataset and always selects time_hour, carrier, and flight since they form the compound primary key that allows you to identify a row.\n\nsubset_flights <- function(rows, cols) {\n  flights |> \n    filter({{ rows }}) |> \n    select(time_hour, carrier, flight, {{ cols }})\n}\n\n\n26.3.4 Data-masking vs. tidy-selection\nSometimes you want to select variables inside a function that uses data-masking. For example, imagine you want to write a count_missing() that counts the number of missing observations in rows. You might try writing something like:\n\ncount_missing <- function(df, group_vars, x_var) {\n  df |> \n    group_by({{ group_vars }}) |> \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n    )\n}\n\nflights |> \n  count_missing(c(year, month, day), dep_time)\n#> Error in `group_by()`:\n#> ℹ In argument: `c(year, month, day)`.\n#> Caused by error:\n#> ! `c(year, month, day)` must be size 336776 or 1, not 1010328.\n\nThis doesn’t work because group_by() uses data-masking, not tidy-selection. We can work around that problem by using the handy pick() function, which allows you to use tidy-selection inside data-masking functions:\n\ncount_missing <- function(df, group_vars, x_var) {\n  df |> \n    group_by(pick({{ group_vars }})) |> \n    summarize(\n      n_miss = sum(is.na({{ x_var }})),\n      .groups = \"drop\"\n  )\n}\n\nflights |> \n  count_missing(c(year, month, day), dep_time)\n#> # A tibble: 365 × 4\n#>    year month   day n_miss\n#>   <int> <int> <int>  <int>\n#> 1  2013     1     1      4\n#> 2  2013     1     2      8\n#> 3  2013     1     3     10\n#> 4  2013     1     4      6\n#> 5  2013     1     5      3\n#> 6  2013     1     6      1\n#> # ℹ 359 more rows\n\nAnother convenient use of pick() is to make a 2d table of counts. Here we count using all the variables in the rows and columns, then use pivot_wider() to rearrange the counts into a grid:\n\n# https://twitter.com/pollicipes/status/1571606508944719876\ncount_wide <- function(data, rows, cols) {\n  data |> \n    count(pick(c({{ rows }}, {{ cols }}))) |> \n    pivot_wider(\n      names_from = {{ cols }}, \n      values_from = n,\n      names_sort = TRUE,\n      values_fill = 0\n    )\n}\n\ndiamonds |> count_wide(c(clarity, color), cut)\n#> # A tibble: 56 × 7\n#>   clarity color  Fair  Good `Very Good` Premium Ideal\n#>   <ord>   <ord> <int> <int>       <int>   <int> <int>\n#> 1 I1      D         4     8           5      12    13\n#> 2 I1      E         9    23          22      30    18\n#> 3 I1      F        35    19          13      34    42\n#> 4 I1      G        53    19          16      46    16\n#> 5 I1      H        52    14          12      46    38\n#> 6 I1      I        34     9           8      24    17\n#> # ℹ 50 more rows\n\nWhile our examples have mostly focused on dplyr, tidy evaluation also underpins tidyr, and if you look at the pivot_wider() docs you can see that names_from uses tidy-selection.\n\n26.3.5 Exercises\n\n\nUsing the datasets from nycflights13, write a function that:\n\n\nFinds all flights that were cancelled (i.e. is.na(arr_time)) or delayed by more than an hour.\n\nflights |> filter_severe()\n\n\n\nCounts the number of cancelled flights and the number of flights delayed by more than an hour.\n\nflights |> group_by(dest) |> summarize_severe()\n\n\n\nFinds all flights that were cancelled or delayed by more than a user supplied number of hours:\n\nflights |> filter_severe(hours = 2)\n\n\n\nSummarizes the weather to compute the minimum, mean, and maximum, of a user supplied variable:\n\nweather |> summarize_weather(temp)\n\n\n\nConverts the user supplied variable that uses clock time (e.g., dep_time, arr_time, etc.) into a decimal time (i.e. hours + (minutes / 60)).\n\nflights |> standardize_time(sched_dep_time)\n\n\n\n\nFor each of the following functions list all arguments that use tidy evaluation and describe whether they use data-masking or tidy-selection: distinct(), count(), group_by(), rename_with(), slice_min(), slice_sample().\n\nGeneralize the following function so that you can supply any number of variables to count.\n\ncount_prop <- function(df, var, sort = FALSE) {\n  df |>\n    count({{ var }}, sort = sort) |>\n    mutate(prop = n / sum(n))\n}"
  },
  {
    "objectID": "functions.html#plot-functions",
    "href": "functions.html#plot-functions",
    "title": "26  Functions",
    "section": "\n26.4 Plot functions",
    "text": "26.4 Plot functions\nInstead of returning a data frame, you might want to return a plot. Fortunately, you can use the same techniques with ggplot2, because aes() is a data-masking function. For example, imagine that you’re making a lot of histograms:\n\ndiamonds |> \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\n\ndiamonds |> \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.05)\n\nWouldn’t it be nice if you could wrap this up into a histogram function? This is easy as pie once you know that aes() is a data-masking function and you need to embrace:\n\nhistogram <- function(df, var, binwidth = NULL) {\n  df |> \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\ndiamonds |> histogram(carat, 0.1)\n\n\n\n\nNote that histogram() returns a ggplot2 plot, meaning you can still add on additional components if you want. Just remember to switch from |> to +:\n\ndiamonds |> \n  histogram(carat, 0.1) +\n  labs(x = \"Size (in carats)\", y = \"Number of diamonds\")\n\n\n26.4.1 More variables\nIt’s straightforward to add more variables to the mix. For example, maybe you want an easy way to eyeball whether or not a dataset is linear by overlaying a smooth line and a straight line:\n\n# https://twitter.com/tyler_js_smith/status/1574377116988104704\nlinearity_check <- function(df, x, y) {\n  df |>\n    ggplot(aes(x = {{ x }}, y = {{ y }})) +\n    geom_point() +\n    geom_smooth(method = \"loess\", formula = y ~ x, color = \"red\", se = FALSE) +\n    geom_smooth(method = \"lm\", formula = y ~ x, color = \"blue\", se = FALSE) \n}\n\nstarwars |> \n  filter(mass < 1000) |> \n  linearity_check(mass, height)\n\n\n\n\nOr maybe you want an alternative to colored scatterplots for very large datasets where overplotting is a problem:\n\n# https://twitter.com/ppaxisa/status/1574398423175921665\nhex_plot <- function(df, x, y, z, bins = 20, fun = \"mean\") {\n  df |> \n    ggplot(aes(x = {{ x }}, y = {{ y }}, z = {{ z }})) + \n    stat_summary_hex(\n      aes(color = after_scale(fill)), # make border same color as fill\n      bins = bins, \n      fun = fun,\n    )\n}\n\ndiamonds |> hex_plot(carat, price, depth)\n#> Warning: Computation failed in `stat_summary_hex()`\n#> Caused by error in `compute_group()`:\n#> ! The package \"hexbin\" is required for `stat_summary_hex()`\n\n\n\n\n\n26.4.2 Combining with other tidyverse\nSome of the most useful helpers combine a dash of data manipulation with ggplot2. For example, if you might want to do a vertical bar chart where you automatically sort the bars in frequency order using fct_infreq(). Since the bar chart is vertical, we also need to reverse the usual order to get the highest values at the top:\n\nsorted_bars <- function(df, var) {\n  df |> \n    mutate({{ var }} := fct_rev(fct_infreq({{ var }})))  |>\n    ggplot(aes(y = {{ var }})) +\n    geom_bar()\n}\n\ndiamonds |> sorted_bars(clarity)\n\n\n\n\nWe have to use a new operator here, :=, because we are generating the variable name based on user-supplied data. Variable names go on the left hand side of =, but R’s syntax doesn’t allow anything to the left of = except for a single literal name. To work around this problem, we use the special operator := which tidy evaluation treats in exactly the same way as =.\nOr maybe you want to make it easy to draw a bar plot just for a subset of the data:\n\nconditional_bars <- function(df, condition, var) {\n  df |> \n    filter({{ condition }}) |> \n    ggplot(aes(x = {{ var }})) + \n    geom_bar()\n}\n\ndiamonds |> conditional_bars(cut == \"Good\", clarity)\n\n\n\n\nYou can also get creative and display data summaries in other ways. You can find a cool application at https://gist.github.com/GShotwell/b19ef520b6d56f61a830fabb3454965b; it uses the axis labels to display the highest value. As you learn more about ggplot2, the power of your functions will continue to increase.\nWe’ll finish with a more complicated case: labelling the plots you create.\n\n26.4.3 Labeling\nRemember the histogram function we showed you earlier?\n\nhistogram <- function(df, var, binwidth = NULL) {\n  df |> \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n\nWouldn’t it be nice if we could label the output with the variable and the bin width that was used? To do so, we’re going to have to go under the covers of tidy evaluation and use a function from the package we haven’t talked about yet: rlang. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).\nTo solve the labeling problem we can use rlang::englue(). This works similarly to str_glue(), so any value wrapped in { } will be inserted into the string. But it also understands {{ }}, which automatically inserts the appropriate variable name:\n\nhistogram <- function(df, var, binwidth) {\n  label <- rlang::englue(\"A histogram of {{var}} with binwidth {binwidth}\")\n  \n  df |> \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth) + \n    labs(title = label)\n}\n\ndiamonds |> histogram(carat, 0.1)\n\n\n\n\nYou can use the same approach in any other place where you want to supply a string in a ggplot2 plot.\n\n26.4.4 Exercises\nBuild up a rich plotting function by incrementally implementing each of the steps below:\n\nDraw a scatterplot given dataset and x and y variables.\nAdd a line of best fit (i.e. a linear model with no standard errors).\nAdd a title."
  },
  {
    "objectID": "functions.html#style",
    "href": "functions.html#style",
    "title": "26  Functions",
    "section": "\n26.5 Style",
    "text": "26.5 Style\nR doesn’t care what your function or arguments are called but the names make a big difference for humans. Ideally, the name of your function will be short, but clearly evoke what the function does. That’s hard! But it’s better to be clear than short, as RStudio’s autocomplete makes it easy to type long names.\nGenerally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). Use your best judgement and don’t be afraid to rename a function if you figure out a better name later.\n\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n\nR also doesn’t care about how you use white space in your functions but future readers will. Continue to follow the rules from Chapter 5. Additionally, function() should always be followed by squiggly brackets ({}), and the contents should be indented by an additional two spaces. This makes it easier to see the hierarchy in your code by skimming the left-hand margin.\n\n# Missing extra two spaces\ndensity <- function(color, facets, binwidth = 0.1) {\ndiamonds |> \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\n\n# Pipe indented incorrectly\ndensity <- function(color, facets, binwidth = 0.1) {\n  diamonds |> \n  ggplot(aes(x = carat, y = after_stat(density), color = {{ color }})) +\n  geom_freqpoly(binwidth = binwidth) +\n  facet_wrap(vars({{ facets }}))\n}\n\nAs you can see we recommend putting extra spaces inside of {{ }}. This makes it very obvious that something unusual is happening.\n\n26.5.1 Exercises\n\n\nRead the source code for each of the following two functions, puzzle out what they do, and then brainstorm better names.\n\nf1 <- function(string, prefix) {\n  str_sub(string, 1, str_length(prefix)) == prefix\n}\n\nf3 <- function(x, y) {\n  rep(y, length.out = length(x))\n}\n\n\nTake a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.\nMake a case for why norm_r(), norm_d() etc. would be better than rnorm(), dnorm(). Make a case for the opposite. How could you make the names even clearer?"
  },
  {
    "objectID": "functions.html#summary",
    "href": "functions.html#summary",
    "title": "26  Functions",
    "section": "\n26.6 Summary",
    "text": "26.6 Summary\nIn this chapter, you learned how to write functions for three useful scenarios: creating a vector, creating a data frames, or creating a plot. Along the way you saw many examples, which hopefully started to get your creative juices flowing, and gave you some ideas for where functions might help your analysis code.\nWe have only shown you the bare minimum to get started with functions and there’s much more to learn. A few places to learn more are:\n\nTo learn more about programming with tidy evaluation, see useful recipes in programming with dplyr and programming with tidyr and learn more about the theory in What is data-masking and why do I need {{?.\nTo learn more about reducing duplication in your ggplot2 code, read the Programming with ggplot2 chapter of the ggplot2 book.\nFor more advice on function style, see the tidyverse style guide.\n\nIn the next chapter, we’ll dive into iteration which gives you further tools for reducing code duplication."
  },
  {
    "objectID": "iteration.html#introduction",
    "href": "iteration.html#introduction",
    "title": "27  Iteration",
    "section": "\n27.1 Introduction",
    "text": "27.1 Introduction\nIn this chapter, you’ll learn tools for iteration, repeatedly performing the same action on different objects. Iteration in R generally tends to look rather different from other programming languages because so much of it is implicit and we get it for free. For example, if you want to double a numeric vector x in R, you can just write 2 * x. In most other languages, you’d need to explicitly double each element of x using some sort of for loop.\nThis book has already given you a small but powerful number of tools that perform the same action for multiple “things”:\n\n\nfacet_wrap() and facet_grid() draws a plot for each subset.\n\ngroup_by() plus summarize() computes summary statistics for each subset.\n\nunnest_wider() and unnest_longer() create new rows and columns for each element of a list-column.\n\nNow it’s time to learn some more general tools, often called functional programming tools because they are built around functions that take other functions as inputs. Learning functional programming can easily veer into the abstract, but in this chapter we’ll keep things concrete by focusing on three common tasks: modifying multiple columns, reading multiple files, and saving multiple objects.\n\n27.1.1 Prerequisites\nIn this chapter, we’ll focus on tools provided by dplyr and purrr, both core members of the tidyverse. You’ve seen dplyr before, but purrr is new. We’re just going to use a couple of purrr functions in this chapter, but it’s a great package to explore as you improve your programming skills.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "iteration.html#sec-across",
    "href": "iteration.html#sec-across",
    "title": "27  Iteration",
    "section": "\n27.2 Modifying multiple columns",
    "text": "27.2 Modifying multiple columns\nImagine you have this simple tibble and you want to count the number of observations and compute the median of every column.\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\nYou could do it with copy-and-paste:\n\ndf |> summarize(\n  n = n(),\n  a = median(a),\n  b = median(b),\n  c = median(c),\n  d = median(d),\n)\n#> # A tibble: 1 × 5\n#>       n      a      b       c     d\n#>   <int>  <dbl>  <dbl>   <dbl> <dbl>\n#> 1    10 -0.246 -0.287 -0.0567 0.144\n\nThat breaks our rule of thumb to never copy and paste more than twice, and you can imagine that this will get very tedious if you have tens or even hundreds of columns. Instead, you can use across():\n\ndf |> summarize(\n  n = n(),\n  across(a:d, median),\n)\n#> # A tibble: 1 × 5\n#>       n      a      b       c     d\n#>   <int>  <dbl>  <dbl>   <dbl> <dbl>\n#> 1    10 -0.246 -0.287 -0.0567 0.144\n\nacross() has three particularly important arguments, which we’ll discuss in detail in the following sections. You’ll use the first two every time you use across(): the first argument, .cols, specifies which columns you want to iterate over, and the second argument, .fns, specifies what to do with each column. You can use the .names argument when you need additional control over the names of output columns, which is particularly important when you use across() with mutate(). We’ll also discuss two important variations, if_any() and if_all(), which work with filter().\n\n27.2.1 Selecting columns with .cols\n\nThe first argument to across(), .cols, selects the columns to transform. This uses the same specifications as select(), ?sec-select, so you can use functions like starts_with() and ends_with() to select columns based on their name.\nThere are two additional selection techniques that are particularly useful for across(): everything() and where(). everything() is straightforward: it selects every (non-grouping) column:\n\ndf <- tibble(\n  grp = sample(2, 10, replace = TRUE),\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf |> \n  group_by(grp) |> \n  summarize(across(everything(), median))\n#> # A tibble: 2 × 5\n#>     grp       a       b     c     d\n#>   <int>   <dbl>   <dbl> <dbl> <dbl>\n#> 1     1 -0.0935 -0.0163 0.363 0.364\n#> 2     2  0.312  -0.0576 0.208 0.565\n\nNote grouping columns (grp here) are not included in across(), because they’re automatically preserved by summarize().\nwhere() allows you to select columns based on their type:\n\n\nwhere(is.numeric) selects all numeric columns.\n\nwhere(is.character) selects all string columns.\n\nwhere(is.Date) selects all date columns.\n\nwhere(is.POSIXct) selects all date-time columns.\n\nwhere(is.logical) selects all logical columns.\n\nJust like other selectors, you can combine these with Boolean algebra. For example, !where(is.numeric) selects all non-numeric columns, and starts_with(\"a\") & where(is.logical) selects all logical columns whose name starts with “a”.\n\n27.2.2 Calling a single function\nThe second argument to across() defines how each column will be transformed. In simple cases, as above, this will be a single existing function. This is a pretty special feature of R: we’re passing one function (median, mean, str_flatten, …) to another function (across). This is one of the features that makes R a functional programming language.\nIt’s important to note that we’re passing this function to across(), so across() can call it; we’re not calling it ourselves. That means the function name should never be followed by (). If you forget, you’ll get an error:\n\ndf |> \n  group_by(grp) |> \n  summarize(across(everything(), median()))\n#> Error in `summarize()`:\n#> ℹ In argument: `across(everything(), median())`.\n#> Caused by error in `is.factor()`:\n#> ! argument \"x\" is missing, with no default\n\nThis error arises because you’re calling the function with no input, e.g.:\n\nmedian()\n#> Error in is.factor(x): argument \"x\" is missing, with no default\n\n\n27.2.3 Calling multiple functions\nIn more complex cases, you might want to supply additional arguments or perform multiple transformations. Let’s motivate this problem with a simple example: what happens if we have some missing values in our data? median() propagates those missing values, giving us a suboptimal output:\n\nrnorm_na <- function(n, n_na, mean = 0, sd = 1) {\n  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))\n}\n\ndf_miss <- tibble(\n  a = rnorm_na(5, 1),\n  b = rnorm_na(5, 1),\n  c = rnorm_na(5, 2),\n  d = rnorm(5)\n)\ndf_miss |> \n  summarize(\n    across(a:d, median),\n    n = n()\n  )\n#> # A tibble: 1 × 5\n#>       a     b     c     d     n\n#>   <dbl> <dbl> <dbl> <dbl> <int>\n#> 1    NA    NA    NA  1.15     5\n\nIt would be nice if we could pass along na.rm = TRUE to median() to remove these missing values. To do so, instead of calling median() directly, we need to create a new function that calls median() with the desired arguments:\n\ndf_miss |> \n  summarize(\n    across(a:d, function(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n#> # A tibble: 1 × 5\n#>       a     b      c     d     n\n#>   <dbl> <dbl>  <dbl> <dbl> <int>\n#> 1 0.139 -1.11 -0.387  1.15     5\n\nThis is a little verbose, so R comes with a handy shortcut: for this sort of throw away, or anonymous1, function you can replace function with \\2:\n\ndf_miss |> \n  summarize(\n    across(a:d, \\(x) median(x, na.rm = TRUE)),\n    n = n()\n  )\n\nIn either case, across() effectively expands to the following code:\n\ndf_miss |> \n  summarize(\n    a = median(a, na.rm = TRUE),\n    b = median(b, na.rm = TRUE),\n    c = median(c, na.rm = TRUE),\n    d = median(d, na.rm = TRUE),\n    n = n()\n  )\n\nWhen we remove the missing values from the median(), it would be nice to know just how many values were removed. We can find that out by supplying two functions to across(): one to compute the median and the other to count the missing values. You supply multiple functions by using a named list to .fns:\n\ndf_miss |> \n  summarize(\n    across(a:d, list(\n      median = \\(x) median(x, na.rm = TRUE),\n      n_miss = \\(x) sum(is.na(x))\n    )),\n    n = n()\n  )\n#> # A tibble: 1 × 9\n#>   a_median a_n_miss b_median b_n_miss c_median c_n_miss d_median d_n_miss\n#>      <dbl>    <int>    <dbl>    <int>    <dbl>    <int>    <dbl>    <int>\n#> 1    0.139        1    -1.11        1   -0.387        2     1.15        0\n#> # ℹ 1 more variable: n <int>\n\nIf you look carefully, you might intuit that the columns are named using a glue specification (Section 15.3.2) like {.col}_{.fn} where .col is the name of the original column and .fn is the name of the function. That’s not a coincidence! As you’ll learn in the next section, you can use .names argument to supply your own glue spec.\n\n27.2.4 Column names\nThe result of across() is named according to the specification provided in the .names argument. We could specify our own if we wanted the name of the function to come first3:\n\ndf_miss |> \n  summarize(\n    across(\n      a:d,\n      list(\n        median = \\(x) median(x, na.rm = TRUE),\n        n_miss = \\(x) sum(is.na(x))\n      ),\n      .names = \"{.fn}_{.col}\"\n    ),\n    n = n(),\n  )\n#> # A tibble: 1 × 9\n#>   median_a n_miss_a median_b n_miss_b median_c n_miss_c median_d n_miss_d\n#>      <dbl>    <int>    <dbl>    <int>    <dbl>    <int>    <dbl>    <int>\n#> 1    0.139        1    -1.11        1   -0.387        2     1.15        0\n#> # ℹ 1 more variable: n <int>\n\nThe .names argument is particularly important when you use across() with mutate(). By default, the output of across() is given the same names as the inputs. This means that across() inside of mutate() will replace existing columns. For example, here we use coalesce() to replace NAs with 0:\n\ndf_miss |> \n  mutate(\n    across(a:d, \\(x) coalesce(x, 0))\n  )\n#> # A tibble: 5 × 4\n#>        a      b      c     d\n#>    <dbl>  <dbl>  <dbl> <dbl>\n#> 1  0.434 -1.25   0     1.60 \n#> 2  0     -1.43  -0.297 0.776\n#> 3 -0.156 -0.980  0     1.15 \n#> 4 -2.61  -0.683 -0.785 2.13 \n#> 5  1.11   0     -0.387 0.704\n\nIf you’d like to instead create new columns, you can use the .names argument to give the output new names:\n\ndf_miss |> \n  mutate(\n    across(a:d, \\(x) abs(x), .names = \"{.col}_abs\")\n  )\n#> # A tibble: 5 × 8\n#>        a      b      c     d  a_abs  b_abs  c_abs d_abs\n#>    <dbl>  <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <dbl>\n#> 1  0.434 -1.25  NA     1.60   0.434  1.25  NA     1.60 \n#> 2 NA     -1.43  -0.297 0.776 NA      1.43   0.297 0.776\n#> 3 -0.156 -0.980 NA     1.15   0.156  0.980 NA     1.15 \n#> 4 -2.61  -0.683 -0.785 2.13   2.61   0.683  0.785 2.13 \n#> 5  1.11  NA     -0.387 0.704  1.11  NA      0.387 0.704\n\n\n27.2.5 Filtering\nacross() is a great match for summarize() and mutate() but it’s more awkward to use with filter(), because you usually combine multiple conditions with either | or &. It’s clear that across() can help to create multiple logical columns, but then what? So dplyr provides two variants of across() called if_any() and if_all():\n\n# same as df_miss |> filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))\ndf_miss |> filter(if_any(a:d, is.na))\n#> # A tibble: 4 × 4\n#>        a      b      c     d\n#>    <dbl>  <dbl>  <dbl> <dbl>\n#> 1  0.434 -1.25  NA     1.60 \n#> 2 NA     -1.43  -0.297 0.776\n#> 3 -0.156 -0.980 NA     1.15 \n#> 4  1.11  NA     -0.387 0.704\n\n# same as df_miss |> filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))\ndf_miss |> filter(if_all(a:d, is.na))\n#> # A tibble: 0 × 4\n#> # ℹ 4 variables: a <dbl>, b <dbl>, c <dbl>, d <dbl>\n\n\n27.2.6 across() in functions\nacross() is particularly useful to program with because it allows you to operate on multiple columns. For example, Jacob Scott uses this little helper which wraps a bunch of lubridate functions to expand all date columns into year, month, and day columns:\n\nexpand_dates <- function(df) {\n  df |> \n    mutate(\n      across(where(is.Date), list(year = year, month = month, day = mday))\n    )\n}\n\ndf_date <- tibble(\n  name = c(\"Amy\", \"Bob\"),\n  date = ymd(c(\"2009-08-03\", \"2010-01-16\"))\n)\n\ndf_date |> \n  expand_dates()\n#> # A tibble: 2 × 5\n#>   name  date       date_year date_month date_day\n#>   <chr> <date>         <dbl>      <dbl>    <int>\n#> 1 Amy   2009-08-03      2009          8        3\n#> 2 Bob   2010-01-16      2010          1       16\n\nacross() also makes it easy to supply multiple columns in a single argument because the first argument uses tidy-select; you just need to remember to embrace that argument, as we discussed in Section 26.3.2. For example, this function will compute the means of numeric columns by default. But by supplying the second argument you can choose to summarize just selected columns:\n\nsummarize_means <- function(df, summary_vars = where(is.numeric)) {\n  df |> \n    summarize(\n      across({{ summary_vars }}, \\(x) mean(x, na.rm = TRUE)),\n      n = n()\n    )\n}\ndiamonds |> \n  group_by(cut) |> \n  summarize_means()\n#> # A tibble: 5 × 9\n#>   cut       carat depth table price     x     y     z     n\n#>   <ord>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>\n#> 1 Fair      1.05   64.0  59.1 4359.  6.25  6.18  3.98  1610\n#> 2 Good      0.849  62.4  58.7 3929.  5.84  5.85  3.64  4906\n#> 3 Very Good 0.806  61.8  58.0 3982.  5.74  5.77  3.56 12082\n#> 4 Premium   0.892  61.3  58.7 4584.  5.97  5.94  3.65 13791\n#> 5 Ideal     0.703  61.7  56.0 3458.  5.51  5.52  3.40 21551\n\ndiamonds |> \n  group_by(cut) |> \n  summarize_means(c(carat, x:z))\n#> # A tibble: 5 × 6\n#>   cut       carat     x     y     z     n\n#>   <ord>     <dbl> <dbl> <dbl> <dbl> <int>\n#> 1 Fair      1.05   6.25  6.18  3.98  1610\n#> 2 Good      0.849  5.84  5.85  3.64  4906\n#> 3 Very Good 0.806  5.74  5.77  3.56 12082\n#> 4 Premium   0.892  5.97  5.94  3.65 13791\n#> 5 Ideal     0.703  5.51  5.52  3.40 21551\n\n\n27.2.7 Vs pivot_longer()\n\nBefore we go on, it’s worth pointing out an interesting connection between across() and pivot_longer() (?sec-pivoting). In many cases, you perform the same calculations by first pivoting the data and then performing the operations by group rather than by column. For example, take this multi-function summary:\n\ndf |> \n  summarize(across(a:d, list(median = median, mean = mean)))\n#> # A tibble: 1 × 8\n#>   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#>      <dbl>  <dbl>    <dbl>  <dbl>    <dbl>  <dbl>    <dbl>  <dbl>\n#> 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\nWe could compute the same values by pivoting longer and then summarizing:\n\nlong <- df |> \n  pivot_longer(a:d) |> \n  group_by(name) |> \n  summarize(\n    median = median(value),\n    mean = mean(value)\n  )\nlong\n#> # A tibble: 4 × 3\n#>   name   median   mean\n#>   <chr>   <dbl>  <dbl>\n#> 1 a      0.0380 0.205 \n#> 2 b     -0.0163 0.0910\n#> 3 c      0.260  0.0716\n#> 4 d      0.540  0.508\n\nAnd if you wanted the same structure as across() you could pivot again:\n\nlong |> \n  pivot_wider(\n    names_from = name,\n    values_from = c(median, mean),\n    names_vary = \"slowest\",\n    names_glue = \"{name}_{.value}\"\n  )\n#> # A tibble: 1 × 8\n#>   a_median a_mean b_median b_mean c_median c_mean d_median d_mean\n#>      <dbl>  <dbl>    <dbl>  <dbl>    <dbl>  <dbl>    <dbl>  <dbl>\n#> 1   0.0380  0.205  -0.0163 0.0910    0.260 0.0716    0.540  0.508\n\nThis is a useful technique to know about because sometimes you’ll hit a problem that’s not currently possible to solve with across(): when you have groups of columns that you want to compute with simultaneously. For example, imagine that our data frame contains both values and weights and we want to compute a weighted mean:\n\ndf_paired <- tibble(\n  a_val = rnorm(10),\n  a_wts = runif(10),\n  b_val = rnorm(10),\n  b_wts = runif(10),\n  c_val = rnorm(10),\n  c_wts = runif(10),\n  d_val = rnorm(10),\n  d_wts = runif(10)\n)\n\nThere’s currently no way to do this with across()4, but it’s relatively straightforward with pivot_longer():\n\ndf_long <- df_paired |> \n  pivot_longer(\n    everything(), \n    names_to = c(\"group\", \".value\"), \n    names_sep = \"_\"\n  )\ndf_long\n#> # A tibble: 40 × 3\n#>   group    val   wts\n#>   <chr>  <dbl> <dbl>\n#> 1 a      0.715 0.518\n#> 2 b     -0.709 0.691\n#> 3 c      0.718 0.216\n#> 4 d     -0.217 0.733\n#> 5 a     -1.09  0.979\n#> 6 b     -0.209 0.675\n#> # ℹ 34 more rows\n\ndf_long |> \n  group_by(group) |> \n  summarize(mean = weighted.mean(val, wts))\n#> # A tibble: 4 × 2\n#>   group    mean\n#>   <chr>   <dbl>\n#> 1 a      0.126 \n#> 2 b     -0.0704\n#> 3 c     -0.360 \n#> 4 d     -0.248\n\nIf needed, you could pivot_wider() this back to the original form.\n\n27.2.8 Exercises\n\n\nPractice your across() skills by:\n\nComputing the number of unique values in each column of palmerpenguins::penguins.\nComputing the mean of every column in mtcars.\nGrouping diamonds by cut, clarity, and color then counting the number of observations and computing the mean of each numeric column.\n\n\nWhat happens if you use a list of functions in across(), but don’t name them? How is the output named?\nAdjust expand_dates() to automatically remove the date columns after they’ve been expanded. Do you need to embrace any arguments?\n\nExplain what each step of the pipeline in this function does. What special feature of where() are we taking advantage of?\n\nshow_missing <- function(df, group_vars, summary_vars = everything()) {\n  df |> \n    group_by(pick({{ group_vars }})) |> \n    summarize(\n      across({{ summary_vars }}, \\(x) sum(is.na(x))),\n      .groups = \"drop\"\n    ) |>\n    select(where(\\(x) any(x > 0)))\n}\nnycflights13::flights |> show_missing(c(year, month, day))"
  },
  {
    "objectID": "iteration.html#reading-multiple-files",
    "href": "iteration.html#reading-multiple-files",
    "title": "27  Iteration",
    "section": "\n27.3 Reading multiple files",
    "text": "27.3 Reading multiple files\nIn the previous section, you learned how to use dplyr::across() to repeat a transformation on multiple columns. In this section, you’ll learn how to use purrr::map() to do something to every file in a directory. Let’s start with a little motivation: imagine you have a directory full of excel spreadsheets5 you want to read. You could do it with copy and paste:\n\ndata2019 <- readxl::read_excel(\"data/y2019.xlsx\")\ndata2020 <- readxl::read_excel(\"data/y2020.xlsx\")\ndata2021 <- readxl::read_excel(\"data/y2021.xlsx\")\ndata2022 <- readxl::read_excel(\"data/y2022.xlsx\")\n\nAnd then use dplyr::bind_rows() to combine them all together:\n\ndata <- bind_rows(data2019, data2020, data2021, data2022)\n\nYou can imagine that this would get tedious quickly, especially if you had hundreds of files, not just four. The following sections show you how to automate this sort of task. There are three basic steps: use list.files() to list all the files in a directory, then use purrr::map() to read each of them into a list, then use purrr::list_rbind() to combine them into a single data frame. We’ll then discuss how you can handle situations of increasing heterogeneity, where you can’t do exactly the same thing to every file.\n\n27.3.1 Listing files in a directory\nAs the name suggests, list.files() lists the files in a directory. You’ll almost always use three arguments:\n\nThe first argument, path, is the directory to look in.\npattern is a regular expression used to filter the file names. The most common pattern is something like [.]xlsx$ or [.]csv$ to find all files with a specified extension.\nfull.names determines whether or not the directory name should be included in the output. You almost always want this to be TRUE.\n\nTo make our motivating example concrete, this book contains a folder with 12 excel spreadsheets containing data from the gapminder package. Each file contains one year’s worth of data for 142 countries. We can list them all with the appropriate call to list.files():\n\npaths <- list.files(\"data/gapminder\", pattern = \"[.]xlsx$\", full.names = TRUE)\npaths\n#>  [1] \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\"\n#>  [3] \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\"\n#>  [5] \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\"\n#>  [7] \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\"\n#>  [9] \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\"\n#> [11] \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\n\n27.3.2 Lists\nNow that we have these 12 paths, we could call read_excel() 12 times to get 12 data frames:\n\ngapminder_1952 <- readxl::read_excel(\"data/gapminder/1952.xlsx\")\ngapminder_1957 <- readxl::read_excel(\"data/gapminder/1957.xlsx\")\ngapminder_1962 <- readxl::read_excel(\"data/gapminder/1962.xlsx\")\n ...,\ngapminder_2007 <- readxl::read_excel(\"data/gapminder/2007.xlsx\")\n\nBut putting each sheet into its own variable is going to make it hard to work with them a few steps down the road. Instead, they’ll be easier to work with if we put them into a single object. A list is the perfect tool for this job:\n\nfiles <- list(\n  readxl::read_excel(\"data/gapminder/1952.xlsx\"),\n  readxl::read_excel(\"data/gapminder/1957.xlsx\"),\n  readxl::read_excel(\"data/gapminder/1962.xlsx\"),\n  ...,\n  readxl::read_excel(\"data/gapminder/2007.xlsx\")\n)\n\nNow that you have these data frames in a list, how do you get one out? You can use files[[i]] to extract the ith element:\n\nfiles[[3]]\n#> # A tibble: 142 × 5\n#>   country     continent lifeExp      pop gdpPercap\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 Afghanistan Asia         32.0 10267083      853.\n#> 2 Albania     Europe       64.8  1728137     2313.\n#> 3 Algeria     Africa       48.3 11000948     2551.\n#> 4 Angola      Africa       34    4826015     4269.\n#> 5 Argentina   Americas     65.1 21283783     7133.\n#> 6 Australia   Oceania      70.9 10794968    12217.\n#> # ℹ 136 more rows\n\nWe’ll come back to [[ in more detail in ?sec-subset-one.\n\n27.3.3 purrr::map() and list_rbind()\n\nThe code to collect those data frames in a list “by hand” is basically just as tedious to type as code that reads the files one-by-one. Happily, we can use purrr::map() to make even better use of our paths vector. map() is similar toacross(), but instead of doing something to each column in a data frame, it does something to each element of a vector.map(x, f) is shorthand for:\n\nlist(\n  f(x[[1]]),\n  f(x[[2]]),\n  ...,\n  f(x[[n]])\n)\n\nSo we can use map() to get a list of 12 data frames:\n\nfiles <- map(paths, readxl::read_excel)\nlength(files)\n#> [1] 12\n\nfiles[[1]]\n#> # A tibble: 142 × 5\n#>   country     continent lifeExp      pop gdpPercap\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 Afghanistan Asia         28.8  8425333      779.\n#> 2 Albania     Europe       55.2  1282697     1601.\n#> 3 Algeria     Africa       43.1  9279525     2449.\n#> 4 Angola      Africa       30.0  4232095     3521.\n#> 5 Argentina   Americas     62.5 17876956     5911.\n#> 6 Australia   Oceania      69.1  8691212    10040.\n#> # ℹ 136 more rows\n\n(This is another data structure that doesn’t display particularly compactly with str() so you might want to load it into RStudio and inspect it with View()).\nNow we can use purrr::list_rbind() to combine that list of data frames into a single data frame:\n\nlist_rbind(files)\n#> # A tibble: 1,704 × 5\n#>   country     continent lifeExp      pop gdpPercap\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 Afghanistan Asia         28.8  8425333      779.\n#> 2 Albania     Europe       55.2  1282697     1601.\n#> 3 Algeria     Africa       43.1  9279525     2449.\n#> 4 Angola      Africa       30.0  4232095     3521.\n#> 5 Argentina   Americas     62.5 17876956     5911.\n#> 6 Australia   Oceania      69.1  8691212    10040.\n#> # ℹ 1,698 more rows\n\nOr we could do both steps at once in a pipeline:\n\npaths |> \n  map(readxl::read_excel) |> \n  list_rbind()\n\nWhat if we want to pass in extra arguments to read_excel()? We use the same technique that we used with across(). For example, it’s often useful to peak at the first few rows of the data with n_max = 1:\n\npaths |> \n  map(\\(path) readxl::read_excel(path, n_max = 1)) |> \n  list_rbind()\n#> # A tibble: 12 × 5\n#>   country     continent lifeExp      pop gdpPercap\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 Afghanistan Asia         28.8  8425333      779.\n#> 2 Afghanistan Asia         30.3  9240934      821.\n#> 3 Afghanistan Asia         32.0 10267083      853.\n#> 4 Afghanistan Asia         34.0 11537966      836.\n#> 5 Afghanistan Asia         36.1 13079460      740.\n#> 6 Afghanistan Asia         38.4 14880372      786.\n#> # ℹ 6 more rows\n\nThis makes it clear that something is missing: there’s no year column because that value is recorded in the path, not in the individual files. We’ll tackle that problem next.\n\n27.3.4 Data in the path\nSometimes the name of the file is data itself. In this example, the file name contains the year, which is not otherwise recorded in the individual files. To get that column into the final data frame, we need to do two things:\nFirst, we name the vector of paths. The easiest way to do this is with the set_names() function, which can take a function. Here we use basename() to extract just the file name from the full path:\n\npaths |> set_names(basename) \n#>                  1952.xlsx                  1957.xlsx \n#> \"data/gapminder/1952.xlsx\" \"data/gapminder/1957.xlsx\" \n#>                  1962.xlsx                  1967.xlsx \n#> \"data/gapminder/1962.xlsx\" \"data/gapminder/1967.xlsx\" \n#>                  1972.xlsx                  1977.xlsx \n#> \"data/gapminder/1972.xlsx\" \"data/gapminder/1977.xlsx\" \n#>                  1982.xlsx                  1987.xlsx \n#> \"data/gapminder/1982.xlsx\" \"data/gapminder/1987.xlsx\" \n#>                  1992.xlsx                  1997.xlsx \n#> \"data/gapminder/1992.xlsx\" \"data/gapminder/1997.xlsx\" \n#>                  2002.xlsx                  2007.xlsx \n#> \"data/gapminder/2002.xlsx\" \"data/gapminder/2007.xlsx\"\n\nThose names are automatically carried along by all the map functions, so the list of data frames will have those same names:\n\nfiles <- paths |> \n  set_names(basename) |> \n  map(readxl::read_excel)\n\nThat makes this call to map() shorthand for:\n\nfiles <- list(\n  \"1952.xlsx\" = readxl::read_excel(\"data/gapminder/1952.xlsx\"),\n  \"1957.xlsx\" = readxl::read_excel(\"data/gapminder/1957.xlsx\"),\n  \"1962.xlsx\" = readxl::read_excel(\"data/gapminder/1962.xlsx\"),\n  ...,\n  \"2007.xlsx\" = readxl::read_excel(\"data/gapminder/2007.xlsx\")\n)\n\nYou can also use [[ to extract elements by name:\n\nfiles[[\"1962.xlsx\"]]\n#> # A tibble: 142 × 5\n#>   country     continent lifeExp      pop gdpPercap\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 Afghanistan Asia         32.0 10267083      853.\n#> 2 Albania     Europe       64.8  1728137     2313.\n#> 3 Algeria     Africa       48.3 11000948     2551.\n#> 4 Angola      Africa       34    4826015     4269.\n#> 5 Argentina   Americas     65.1 21283783     7133.\n#> 6 Australia   Oceania      70.9 10794968    12217.\n#> # ℹ 136 more rows\n\nThen we use the names_to argument to list_rbind() to tell it to save the names into a new column called year then use readr::parse_number() to extract the number from the string.\n\npaths |> \n  set_names(basename) |> \n  map(readxl::read_excel) |> \n  list_rbind(names_to = \"year\") |> \n  mutate(year = parse_number(year))\n#> # A tibble: 1,704 × 6\n#>    year country     continent lifeExp      pop gdpPercap\n#>   <dbl> <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1  1952 Afghanistan Asia         28.8  8425333      779.\n#> 2  1952 Albania     Europe       55.2  1282697     1601.\n#> 3  1952 Algeria     Africa       43.1  9279525     2449.\n#> 4  1952 Angola      Africa       30.0  4232095     3521.\n#> 5  1952 Argentina   Americas     62.5 17876956     5911.\n#> 6  1952 Australia   Oceania      69.1  8691212    10040.\n#> # ℹ 1,698 more rows\n\nIn more complicated cases, there might be other variables stored in the directory name, or maybe the file name contains multiple bits of data. In that case, use set_names() (without any arguments) to record the full path, and then use tidyr::separate_wider_delim() and friends to turn them into useful columns.\n\npaths |> \n  set_names() |> \n  map(readxl::read_excel) |> \n  list_rbind(names_to = \"year\") |> \n  separate_wider_delim(year, delim = \"/\", names = c(NA, \"dir\", \"file\")) |> \n  separate_wider_delim(file, delim = \".\", names = c(\"file\", \"ext\"))\n#> # A tibble: 1,704 × 8\n#>   dir       file  ext   country     continent lifeExp      pop gdpPercap\n#>   <chr>     <chr> <chr> <chr>       <chr>       <dbl>    <dbl>     <dbl>\n#> 1 gapminder 1952  xlsx  Afghanistan Asia         28.8  8425333      779.\n#> 2 gapminder 1952  xlsx  Albania     Europe       55.2  1282697     1601.\n#> 3 gapminder 1952  xlsx  Algeria     Africa       43.1  9279525     2449.\n#> 4 gapminder 1952  xlsx  Angola      Africa       30.0  4232095     3521.\n#> 5 gapminder 1952  xlsx  Argentina   Americas     62.5 17876956     5911.\n#> 6 gapminder 1952  xlsx  Australia   Oceania      69.1  8691212    10040.\n#> # ℹ 1,698 more rows\n\n\n27.3.5 Save your work\nNow that you’ve done all this hard work to get to a nice tidy data frame, it’s a great time to save your work:\n\ngapminder <- paths |> \n  set_names(basename) |> \n  map(readxl::read_excel) |> \n  list_rbind(names_to = \"year\") |> \n  mutate(year = parse_number(year))\n\nwrite_csv(gapminder, \"gapminder.csv\")\n\nNow when you come back to this problem in the future, you can read in a single csv file. For large and richer datasets, using parquet might be a better choice than .csv, as discussed in ?sec-parquet.\nIf you’re working in a project, we suggest calling the file that does this sort of data prep work something like 0-cleanup.R. The 0 in the file name suggests that this should be run before anything else.\nIf your input data files change over time, you might consider learning a tool like targets to set up your data cleaning code to automatically re-run whenever one of the input files is modified.\n\n27.3.6 Many simple iterations\nHere we’ve just loaded the data directly from disk, and were lucky enough to get a tidy dataset. In most cases, you’ll need to do some additional tidying, and you have two basic options: you can do one round of iteration with a complex function, or do multiple rounds of iteration with simple functions. In our experience most folks reach first for one complex iteration, but you’re often better by doing multiple simple iterations.\nFor example, imagine that you want to read in a bunch of files, filter out missing values, pivot, and then combine. One way to approach the problem is to write a function that takes a file and does all those steps then call map() once:\n\nprocess_file <- function(path) {\n  df <- read_csv(path)\n  \n  df |> \n    filter(!is.na(id)) |> \n    mutate(id = tolower(id)) |> \n    pivot_longer(jan:dec, names_to = \"month\")\n}\n\npaths |> \n  map(process_file) |> \n  list_rbind()\n\nAlternatively, you could perform each step of process_file() to every file:\n\npaths |> \n  map(read_csv) |> \n  map(\\(df) df |> filter(!is.na(id))) |> \n  map(\\(df) df |> mutate(id = tolower(id))) |> \n  map(\\(df) df |> pivot_longer(jan:dec, names_to = \"month\")) |> \n  list_rbind()\n\nWe recommend this approach because it stops you getting fixated on getting the first file right before moving on to the rest. By considering all of the data when doing tidying and cleaning, you’re more likely to think holistically and end up with a higher quality result.\nIn this particular example, there’s another optimization you could make, by binding all the data frames together earlier. Then you can rely on regular dplyr behavior:\n\npaths |> \n  map(read_csv) |> \n  list_rbind() |> \n  filter(!is.na(id)) |> \n  mutate(id = tolower(id)) |> \n  pivot_longer(jan:dec, names_to = \"month\")\n\n\n27.3.7 Heterogeneous data\nUnfortunately, sometimes it’s not possible to go from map() straight to list_rbind() because the data frames are so heterogeneous that list_rbind() either fails or yields a data frame that’s not very useful. In that case, it’s still useful to start by loading all of the files:\n\nfiles <- paths |> \n  map(readxl::read_excel) \n\nThen a very useful strategy is to capture the structure of the data frames so that you can explore it using your data science skills. One way to do so is with this handy df_types function6 that returns a tibble with one row for each column:\n\ndf_types <- function(df) {\n  tibble(\n    col_name = names(df), \n    col_type = map_chr(df, vctrs::vec_ptype_full),\n    n_miss = map_int(df, \\(x) sum(is.na(x)))\n  )\n}\n\ndf_types(gapminder)\n#> # A tibble: 6 × 3\n#>   col_name  col_type  n_miss\n#>   <chr>     <chr>      <int>\n#> 1 year      double         0\n#> 2 country   character      0\n#> 3 continent character      0\n#> 4 lifeExp   double         0\n#> 5 pop       double         0\n#> 6 gdpPercap double         0\n\nYou can then apply this function to all of the files, and maybe do some pivoting to make it easier to see where the differences are. For example, this makes it easy to verify that the gapminder spreadsheets that we’ve been working with are all quite homogeneous:\n\nfiles |> \n  map(df_types) |> \n  list_rbind(names_to = \"file_name\") |> \n  select(-n_miss) |> \n  pivot_wider(names_from = col_name, values_from = col_type)\n#> # A tibble: 12 × 6\n#>   file_name country   continent lifeExp pop    gdpPercap\n#>   <chr>     <chr>     <chr>     <chr>   <chr>  <chr>    \n#> 1 1952.xlsx character character double  double double   \n#> 2 1957.xlsx character character double  double double   \n#> 3 1962.xlsx character character double  double double   \n#> 4 1967.xlsx character character double  double double   \n#> 5 1972.xlsx character character double  double double   \n#> 6 1977.xlsx character character double  double double   \n#> # ℹ 6 more rows\n\nIf the files have heterogeneous formats, you might need to do more processing before you can successfully merge them. Unfortunately, we’re now going to leave you to figure that out on your own, but you might want to read about map_if() and map_at(). map_if() allows you to selectively modify elements of a list based on their values; map_at() allows you to selectively modify elements based on their names.\n\n27.3.8 Handling failures\nSometimes the structure of your data might be sufficiently wild that you can’t even read all the files with a single command. And then you’ll encounter one of the downsides of map(): it succeeds or fails as a whole. map() will either successfully read all of the files in a directory or fail with an error, reading zero files. This is annoying: why does one failure prevent you from accessing all the other successes?\nLuckily, purrr comes with a helper to tackle this problem: possibly(). possibly() is what’s known as a function operator: it takes a function and returns a function with modified behavior. In particular, possibly() changes a function from erroring to returning a value that you specify:\n\nfiles <- paths |> \n  map(possibly(\\(path) readxl::read_excel(path), NULL))\n\ndata <- files |> list_rbind()\n\nThis works particularly well here because list_rbind(), like many tidyverse functions, automatically ignores NULLs.\nNow you have all the data that can be read easily, and it’s time to tackle the hard part of figuring out why some files failed to load and what to do about it. Start by getting the paths that failed:\n\nfailed <- map_vec(files, is.null)\npaths[failed]\n#> character(0)\n\nThen call the import function again for each failure and figure out what went wrong."
  },
  {
    "objectID": "iteration.html#saving-multiple-outputs",
    "href": "iteration.html#saving-multiple-outputs",
    "title": "27  Iteration",
    "section": "\n27.4 Saving multiple outputs",
    "text": "27.4 Saving multiple outputs\nIn the last section, you learned about map(), which is useful for reading multiple files into a single object. In this section, we’ll now explore sort of the opposite problem: how can you take one or more R objects and save it to one or more files? We’ll explore this challenge using three examples:\n\nSaving multiple data frames into one database.\nSaving multiple data frames into multiple .csv files.\nSaving multiple plots to multiple .png files.\n\n\n27.4.1 Writing to a database\nSometimes when working with many files at once, it’s not possible to fit all your data into memory at once, and you can’t do map(files, read_csv). One approach to deal with this problem is to load your data into a database so you can access just the bits you need with dbplyr.\nIf you’re lucky, the database package you’re using will provide a handy function that takes a vector of paths and loads them all into the database. This is the case with duckdb’s duckdb_read_csv():\n\ncon <- DBI::dbConnect(duckdb::duckdb())\nduckdb::duckdb_read_csv(con, \"gapminder\", paths)\n\nThis would work well here, but we don’t have csv files, instead we have excel spreadsheets. So we’re going to have to do it “by hand”. Learning to do it by hand will also help you when you have a bunch of csvs and the database that you’re working with doesn’t have one function that will load them all in.\nWe need to start by creating a table that will fill in with data. The easiest way to do this is by creating a template, a dummy data frame that contains all the columns we want, but only a sampling of the data. For the gapminder data, we can make that template by reading a single file and adding the year to it:\n\ntemplate <- readxl::read_excel(paths[[1]])\ntemplate$year <- 1952\ntemplate\n#> # A tibble: 142 × 6\n#>   country     continent lifeExp      pop gdpPercap  year\n#>   <chr>       <chr>       <dbl>    <dbl>     <dbl> <dbl>\n#> 1 Afghanistan Asia         28.8  8425333      779.  1952\n#> 2 Albania     Europe       55.2  1282697     1601.  1952\n#> 3 Algeria     Africa       43.1  9279525     2449.  1952\n#> 4 Angola      Africa       30.0  4232095     3521.  1952\n#> 5 Argentina   Americas     62.5 17876956     5911.  1952\n#> 6 Australia   Oceania      69.1  8691212    10040.  1952\n#> # ℹ 136 more rows\n\nNow we can connect to the database, and use DBI::dbCreateTable() to turn our template into a database table:\n\ncon <- DBI::dbConnect(duckdb::duckdb())\nDBI::dbCreateTable(con, \"gapminder\", template)\n\ndbCreateTable() doesn’t use the data in template, just the variable names and types. So if we inspect the gapminder table now you’ll see that it’s empty but it has the variables we need with the types we expect:\n\ncon |> tbl(\"gapminder\")\n#> # Source:   table<gapminder> [0 x 6]\n#> # Database: DuckDB 0.8.0 [13081@Windows 10 x64:R 4.2.0/:memory:]\n#> # ℹ 6 variables: country <chr>, continent <chr>, lifeExp <dbl>, pop <dbl>,\n#> #   gdpPercap <dbl>, year <dbl>\n\nNext, we need a function that takes a single file path, reads it into R, and adds the result to the gapminder table. We can do that by combining read_excel() with DBI::dbAppendTable():\n\nappend_file <- function(path) {\n  df <- readxl::read_excel(path)\n  df$year <- parse_number(basename(path))\n  \n  DBI::dbAppendTable(con, \"gapminder\", df)\n}\n\nNow we need to call append_file() once for each element of paths. That’s certainly possible with map():\n\npaths |> map(append_file)\n\nBut we don’t care about the output of append_file(), so instead of map() it’s slightly nicer to use walk(). walk() does exactly the same thing as map() but throws the output away:\n\npaths |> walk(append_file)\n\nNow we can see if we have all the data in our table:\n\ncon |> \n  tbl(\"gapminder\") |> \n  count(year)\n#> # Source:   SQL [?? x 2]\n#> # Database: DuckDB 0.8.0 [13081@Windows 10 x64:R 4.2.0/:memory:]\n#>    year     n\n#>   <dbl> <dbl>\n#> 1  1952   142\n#> 2  1957   142\n#> 3  1962   142\n#> 4  1967   142\n#> 5  1972   142\n#> 6  1977   142\n#> # ℹ more rows\n\n\n27.4.2 Writing csv files\nThe same basic principle applies if we want to write multiple csv files, one for each group. Let’s imagine that we want to take the ggplot2::diamonds data and save one csv file for each clarity. First we need to make those individual datasets. There are many ways you could do that, but there’s one way we particularly like: group_nest().\n\nby_clarity <- diamonds |> \n  group_nest(clarity)\n\nby_clarity\n#> # A tibble: 8 × 2\n#>   clarity               data\n#>   <ord>   <list<tibble[,9]>>\n#> 1 I1               [741 × 9]\n#> 2 SI2            [9,194 × 9]\n#> 3 SI1           [13,065 × 9]\n#> 4 VS2           [12,258 × 9]\n#> 5 VS1            [8,171 × 9]\n#> 6 VVS2           [5,066 × 9]\n#> # ℹ 2 more rows\n\nThis gives us a new tibble with eight rows and two columns. clarity is our grouping variable and data is a list-column containing one tibble for each unique value of clarity:\n\nby_clarity$data[[1]]\n#> # A tibble: 741 × 9\n#>   carat cut       color depth table price     x     y     z\n#>   <dbl> <ord>     <ord> <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n#> 1  0.32 Premium   E      60.9    58   345  4.38  4.42  2.68\n#> 2  1.17 Very Good J      60.2    61  2774  6.83  6.9   4.13\n#> 3  1.01 Premium   F      61.8    60  2781  6.39  6.36  3.94\n#> 4  1.01 Fair      E      64.5    58  2788  6.29  6.21  4.03\n#> 5  0.96 Ideal     F      60.7    55  2801  6.37  6.41  3.88\n#> 6  1.04 Premium   G      62.2    58  2801  6.46  6.41  4   \n#> # ℹ 735 more rows\n\nWhile we’re here, let’s create a column that gives the name of output file, using mutate() and str_glue():\n\nby_clarity <- by_clarity |> \n  mutate(path = str_glue(\"diamonds-{clarity}.csv\"))\n\nby_clarity\n#> # A tibble: 8 × 3\n#>   clarity               data path             \n#>   <ord>   <list<tibble[,9]>> <glue>           \n#> 1 I1               [741 × 9] diamonds-I1.csv  \n#> 2 SI2            [9,194 × 9] diamonds-SI2.csv \n#> 3 SI1           [13,065 × 9] diamonds-SI1.csv \n#> 4 VS2           [12,258 × 9] diamonds-VS2.csv \n#> 5 VS1            [8,171 × 9] diamonds-VS1.csv \n#> 6 VVS2           [5,066 × 9] diamonds-VVS2.csv\n#> # ℹ 2 more rows\n\nSo if we were going to save these data frames by hand, we might write something like:\n\nwrite_csv(by_clarity$data[[1]], by_clarity$path[[1]])\nwrite_csv(by_clarity$data[[2]], by_clarity$path[[2]])\nwrite_csv(by_clarity$data[[3]], by_clarity$path[[3]])\n...\nwrite_csv(by_clarity$by_clarity[[8]], by_clarity$path[[8]])\n\nThis is a little different to our previous uses of map() because there are two arguments that are changing, not just one. That means we need a new function: map2(), which varies both the first and second arguments. And because we again don’t care about the output, we want walk2() rather than map2(). That gives us:\n\nwalk2(by_clarity$data, by_clarity$path, write_csv)\n\n\n27.4.3 Saving plots\nWe can take the same basic approach to create many plots. Let’s first make a function that draws the plot we want:\n\ncarat_histogram <- function(df) {\n  ggplot(df, aes(x = carat)) + geom_histogram(binwidth = 0.1)  \n}\n\ncarat_histogram(by_clarity$data[[1]])\n\n\n\n\nNow we can use map() to create a list of many plots7 and their eventual file paths:\n\nby_clarity <- by_clarity |> \n  mutate(\n    plot = map(data, carat_histogram),\n    path = str_glue(\"clarity-{clarity}.png\")\n  )\n\nThen use walk2() with ggsave() to save each plot:\n\nwalk2(\n  by_clarity$path,\n  by_clarity$plot,\n  \\(path, plot) ggsave(path, plot, width = 6, height = 6)\n)\n\nThis is shorthand for:\n\nggsave(by_clarity$path[[1]], by_clarity$plot[[1]], width = 6, height = 6)\nggsave(by_clarity$path[[2]], by_clarity$plot[[2]], width = 6, height = 6)\nggsave(by_clarity$path[[3]], by_clarity$plot[[3]], width = 6, height = 6)\n...\nggsave(by_clarity$path[[8]], by_clarity$plot[[8]], width = 6, height = 6)"
  },
  {
    "objectID": "iteration.html#summary",
    "href": "iteration.html#summary",
    "title": "27  Iteration",
    "section": "\n27.5 Summary",
    "text": "27.5 Summary\nIn this chapter, you’ve seen how to use explicit iteration to solve three problems that come up frequently when doing data science: manipulating multiple columns, reading multiple files, and saving multiple outputs. But in general, iteration is a super power: if you know the right iteration technique, you can easily go from fixing one problem to fixing all the problems. Once you’ve mastered the techniques in this chapter, we highly recommend learning more by reading the Functionals chapter of Advanced R and consulting the purrr website.\nIf you know much about iteration in other languages, you might be surprised that we didn’t discuss the for loop. That’s because R’s orientation towards data analysis changes how we iterate: in most cases you can rely on an existing idiom to do something to each columns or each group. And when you can’t, you can often use a functional programming tool like map() that does something to each element of a list. However, you will see for loops in wild-caught code, so you’ll learn about them in the next chapter where we’ll discuss some important base R tools."
  },
  {
    "objectID": "iteration.html#footnotes",
    "href": "iteration.html#footnotes",
    "title": "27  Iteration",
    "section": "",
    "text": "Anonymous, because we never explicitly gave it a name with &lt;-. Another term programmers use for this is “lambda function”.↩︎\nIn older code you might see syntax that looks like ~ .x + 1. This is another way to write anonymous functions but it only works inside tidyverse functions and always uses the variable name .x. We now recommend the base syntax, \\(x) x + 1.↩︎\nYou can’t currently change the order of the columns, but you could reorder them after the fact using relocate() or similar.↩︎\nMaybe there will be one day, but currently we don’t see how.↩︎\nIf you instead had a directory of csv files with the same format, you can use the technique from ?sec-readr-directory.↩︎\nWe’re not going to explain how it works, but if you look at the docs for the functions used, you should be able to puzzle it out.↩︎\nYou can print by_clarity$plot to get a crude animation — you’ll get one plot for each element of plots. NOTE: this didn’t happen for me.↩︎"
  },
  {
    "objectID": "base-R.html#introduction",
    "href": "base-R.html#introduction",
    "title": "28  A field guide to base R",
    "section": "28.1 Introduction",
    "text": "28.1 Introduction\nTo finish off the programming section, we’re going to give you a quick tour of the most important base R functions that we don’t otherwise discuss in the book. These tools are particularly useful as you do more programming and will help you read code you’ll encounter in the wild.\nThis is a good place to remind you that the tidyverse is not the only way to solve data science problems. We teach the tidyverse in this book because tidyverse packages share a common design philosophy, increasing the consistency across functions, and making each new function or package a little easier to learn and use. It’s not possible to use the tidyverse without using base R, so we’ve actually already taught you a lot of base R functions: from library() to load packages, to sum() and mean() for numeric summaries, to the factor, date, and POSIXct data types, and of course all the basic operators like +, -, /, *, |, &, and !. What we haven’t focused on so far is base R workflows, so we will highlight a few of those in this chapter.\nAfter you read this book, you’ll learn other approaches to the same problems using base R, data.table, and other packages. You’ll undoubtedly encounter these other approaches when you start reading R code written by others, particularly if you’re using StackOverflow. It’s 100% okay to write code that uses a mix of approaches, and don’t let anyone tell you otherwise!\nIn this chapter, we’ll focus on four big topics: subsetting with [, subsetting with [[ and $, the apply family of functions, and for loops. To finish off, we’ll briefly discuss two essential plotting functions.\n\n28.1.1 Prerequisites\nThis package focuses on base R so doesn’t have any real prerequisites, but we’ll load the tidyverse in order to explain some of the differences.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "base-R.html#sec-subset-many",
    "href": "base-R.html#sec-subset-many",
    "title": "28  A field guide to base R",
    "section": "28.2 Selecting multiple elements with [",
    "text": "28.2 Selecting multiple elements with [\n[ is used to extract sub-components from vectors and data frames, and is called like x[i] or x[i, j]. In this section, we’ll introduce you to the power of [, first showing you how you can use it with vectors, then how the same principles extend in a straightforward way to two-dimensional (2d) structures like data frames. We’ll then help you cement that knowledge by showing how various dplyr verbs are special cases of [.\n\n28.2.1 Subsetting vectors\nThere are five main types of things that you can subset a vector with, i.e., that can be the i in x[i]:\n\nA vector of positive integers. Subsetting with positive integers keeps the elements at those positions:\n\nx &lt;- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#&gt; [1] \"three\" \"two\"   \"five\"\n\nBy repeating a position, you can actually make a longer output than input, making the term “subsetting” a bit of a misnomer.\n\nx[c(1, 1, 5, 5, 5, 2)]\n#&gt; [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\n\nA vector of negative integers. Negative values drop the elements at the specified positions:\n\nx[c(-1, -3, -5)]\n#&gt; [1] \"two\"  \"four\"\n\nA logical vector. Subsetting with a logical vector keeps all values corresponding to a TRUE value. This is most often useful in conjunction with the comparison functions.\n\nx &lt;- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n#&gt; [1] 10  3  5  8  1\n\n# All even (or missing!) values of x\nx[x %% 2 == 0]\n#&gt; [1] 10 NA  8 NA\n\nUnlike filter(), NA indices will be included in the output as NAs.\nA character vector. If you have a named vector, you can subset it with a character vector:\n\nx &lt;- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#&gt; xyz def \n#&gt;   5   2\n\nAs with subsetting with positive integers, you can use a character vector to duplicate individual entries.\nNothing. The final type of subsetting is nothing, x[], which returns the complete x. This is not useful for subsetting vectors, but as we’ll see shortly, it is useful when subsetting 2d structures like tibbles.\n\n\n\n28.2.2 Subsetting data frames\nThere are quite a few different ways1 that you can use [ with a data frame, but the most important way is to select rows and columns independently with df[rows, cols]. Here rows and cols are vectors as described above. For example, df[rows, ] and df[, cols] select just rows or just columns, using the empty subset to preserve the other dimension.\nHere are a couple of examples:\n\ndf &lt;- tibble(\n  x = 1:3, \n  y = c(\"a\", \"e\", \"f\"), \n  z = runif(3)\n)\n\n# Select first row and second column\ndf[1, 2]\n#&gt; # A tibble: 1 × 1\n#&gt;   y    \n#&gt;   &lt;chr&gt;\n#&gt; 1 a\n\n# Select all rows and columns x and y\ndf[, c(\"x\" , \"y\")]\n#&gt; # A tibble: 3 × 2\n#&gt;       x y    \n#&gt;   &lt;int&gt; &lt;chr&gt;\n#&gt; 1     1 a    \n#&gt; 2     2 e    \n#&gt; 3     3 f\n\n# Select rows where `x` is greater than 1 and all columns\ndf[df$x &gt; 1, ]\n#&gt; # A tibble: 2 × 3\n#&gt;       x y         z\n#&gt;   &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     2 e     0.834\n#&gt; 2     3 f     0.601\n\nWe’ll come back to $ shortly, but you should be able to guess what df$x does from the context: it extracts the x variable from df. We need to use it here because [ doesn’t use tidy evaluation, so you need to be explicit about the source of the x variable.\nThere’s an important difference between tibbles and data frames when it comes to [. In this book, we’ve mainly used tibbles, which are data frames, but they tweak some behaviors to make your life a little easier. In most places, you can use “tibble” and “data frame” interchangeably, so when we want to draw particular attention to R’s built-in data frame, we’ll write data.frame. If df is a data.frame, then df[, cols] will return a vector if col selects a single column and a data frame if it selects more than one column. If df is a tibble, then [ will always return a tibble.\n\ndf1 &lt;- data.frame(x = 1:3)\ndf1[, \"x\"]\n#&gt; [1] 1 2 3\n\ndf2 &lt;- tibble(x = 1:3)\ndf2[, \"x\"]\n#&gt; # A tibble: 3 × 1\n#&gt;       x\n#&gt;   &lt;int&gt;\n#&gt; 1     1\n#&gt; 2     2\n#&gt; 3     3\n\nOne way to avoid this ambiguity with data.frames is to explicitly specify drop = FALSE:\n\ndf1[, \"x\" , drop = FALSE]\n#&gt;   x\n#&gt; 1 1\n#&gt; 2 2\n#&gt; 3 3\n\n\n\n28.2.3 dplyr equivalents\nSeveral dplyr verbs are special cases of [:\n\nfilter() is equivalent to subsetting the rows with a logical vector, taking care to exclude missing values:\n\ndf &lt;- tibble(\n  x = c(2, 3, 1, 1, NA), \n  y = letters[1:5], \n  z = runif(5)\n)\ndf |&gt; filter(x &gt; 1)\n\n# same as\ndf[!is.na(df$x) & df$x &gt; 1, ]\n\nAnother common technique in the wild is to use which() for its side-effect of dropping missing values: df[which(df$x &gt; 1), ].\narrange() is equivalent to subsetting the rows with an integer vector, usually created with order():\n\ndf |&gt; arrange(x, y)\n\n# same as\ndf[order(df$x, df$y), ]\n\nYou can use order(decreasing = TRUE) to sort all columns in descending order or -rank(col) to sort columns in decreasing order individually.\nBoth select() and relocate() are similar to subsetting the columns with a character vector:\n\ndf |&gt; select(x, z)\n\n# same as\ndf[, c(\"x\", \"z\")]\n\n\nBase R also provides a function that combines the features of filter() and select()2 called subset():\n\ndf |&gt; \n  filter(x &gt; 1) |&gt; \n  select(y, z)\n#&gt; # A tibble: 2 × 2\n#&gt;   y           z\n#&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt; 1 a     0.157  \n#&gt; 2 b     0.00740\n\n\n# same as\ndf |&gt; subset(x &gt; 1, c(y, z))\n\nThis function was the inspiration for much of dplyr’s syntax.\n\n\n28.2.4 Exercises\n\nCreate functions that take a vector as input and return:\n\nThe elements at even-numbered positions.\nEvery element except the last value.\nOnly even values (and no missing values).\n\nWhy is x[-which(x &gt; 0)] not the same as x[x &lt;= 0]? Read the documentation for which() and do some experiments to figure it out."
  },
  {
    "objectID": "base-R.html#sec-subset-one",
    "href": "base-R.html#sec-subset-one",
    "title": "28  A field guide to base R",
    "section": "28.3 Selecting a single element with $ and [[",
    "text": "28.3 Selecting a single element with $ and [[\n[, which selects many elements, is paired with [[ and $, which extract a single element. In this section, we’ll show you how to use [[ and $ to pull columns out of data frames, discuss a couple more differences between data.frames and tibbles, and emphasize some important differences between [ and [[ when used with lists.\n\n28.3.1 Data frames\n[[ and $ can be used to extract columns out of a data frame. [[ can access by position or by name, and $ is specialized for access by name:\n\ntb &lt;- tibble(\n  x = 1:4,\n  y = c(10, 4, 1, 21)\n)\n\n# by position\ntb[[1]]\n#&gt; [1] 1 2 3 4\n\n# by name\ntb[[\"x\"]]\n#&gt; [1] 1 2 3 4\ntb$x\n#&gt; [1] 1 2 3 4\n\nThey can also be used to create new columns, the base R equivalent of mutate():\n\ntb$z &lt;- tb$x + tb$y\ntb\n#&gt; # A tibble: 4 × 3\n#&gt;       x     y     z\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1    10    11\n#&gt; 2     2     4     6\n#&gt; 3     3     1     4\n#&gt; 4     4    21    25\n\nThere are several other base R approaches to creating new columns including with transform(), with(), and within(). Hadley collected a few examples at https://gist.github.com/hadley/1986a273e384fb2d4d752c18ed71bedf.\nUsing $ directly is convenient when performing quick summaries. For example, if you just want to find the size of the biggest diamond or the possible values of cut, there’s no need to use summarize():\n\nmax(diamonds$carat)\n#&gt; [1] 5.01\n\nlevels(diamonds$cut)\n#&gt; [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"\n\ndplyr also provides an equivalent to [[/$ that we didn’t mention in Chapter 4: pull(). pull() takes either a variable name or variable position and returns just that column. That means we could rewrite the above code to use the pipe:\n\ndiamonds |&gt; pull(carat) |&gt; mean()\n#&gt; [1] 0.7979397\n\ndiamonds |&gt; pull(cut) |&gt; levels()\n#&gt; [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"\n\n\n\n28.3.2 Tibbles\nThere are a couple of important differences between tibbles and base data.frames when it comes to $. Data frames match the prefix of any variable names (so-called partial matching) and don’t complain if a column doesn’t exist:\n\ndf &lt;- data.frame(x1 = 1)\ndf$x\n#&gt; [1] 1\ndf$z\n#&gt; NULL\n\nTibbles are more strict: they only ever match variable names exactly and they will generate a warning if the column you are trying to access doesn’t exist:\n\ntb &lt;- tibble(x1 = 1)\n\ntb$x\n#&gt; Warning: Unknown or uninitialised column: `x`.\n#&gt; NULL\ntb$z\n#&gt; Warning: Unknown or uninitialised column: `z`.\n#&gt; NULL\n\nFor this reason we sometimes joke that tibbles are lazy and surly: they do less and complain more.\n\n\n28.3.3 Lists\n[[ and $ are also really important for working with lists, and it’s important to understand how they differ from [. Let’s illustrate the differences with a list named l:\n\nl &lt;- list(\n  a = 1:3, \n  b = \"a string\", \n  c = pi, \n  d = list(-1, -5)\n)\n\n\n[ extracts a sub-list. It doesn’t matter how many elements you extract, the result will always be a list.\n\nstr(l[1:2])\n#&gt; List of 2\n#&gt;  $ a: int [1:3] 1 2 3\n#&gt;  $ b: chr \"a string\"\n\nstr(l[1])\n#&gt; List of 1\n#&gt;  $ a: int [1:3] 1 2 3\n\nstr(l[4])\n#&gt; List of 1\n#&gt;  $ d:List of 2\n#&gt;   ..$ : num -1\n#&gt;   ..$ : num -5\n\nLike with vectors, you can subset with a logical, integer, or character vector.\n[[ and $ extract a single component from a list. They remove a level of hierarchy from the list.\n\nstr(l[[1]])\n#&gt;  int [1:3] 1 2 3\n\nstr(l[[4]])\n#&gt; List of 2\n#&gt;  $ : num -1\n#&gt;  $ : num -5\n\nstr(l$a)\n#&gt;  int [1:3] 1 2 3\n\n\nThe difference between [ and [[ is particularly important for lists because [[ drills down into the list while [ returns a new, smaller list. To help you remember the difference, take a look at the unusual pepper shaker shown in Figure 28.1. If this pepper shaker is your list pepper, then, pepper[1] is a pepper shaker containing a single pepper packet. pepper[2] would look the same, but would contain the second packet. pepper[1:2] would be a pepper shaker containing two pepper packets. pepper[[1]] would extract the pepper packet itself.\n\n\n\n\n\nFigure 28.1: (Left) A pepper shaker that Hadley once found in his hotel room. (Middle) pepper[1]. (Right) pepper[[1]]\n\n\n\n\nThis same principle applies when you use 1d [ with a data frame: df[\"x\"] returns a one-column data frame and df[[\"x\"]] returns a vector.\n\n\n28.3.4 Exercises\n\nWhat happens when you use [[ with a positive integer that’s bigger than the length of the vector? What happens when you subset with a name that doesn’t exist?\nWhat would pepper[[1]][1] be? What about pepper[[1]][[1]]?"
  },
  {
    "objectID": "base-R.html#apply-family",
    "href": "base-R.html#apply-family",
    "title": "28  A field guide to base R",
    "section": "28.4 Apply family",
    "text": "28.4 Apply family\nIn Chapter 27, you learned tidyverse techniques for iteration like dplyr::across() and the map family of functions. In this section, you’ll learn about their base equivalents, the apply family. In this context apply and map are synonyms because another way of saying “map a function over each element of a vector” is “apply a function over each element of a vector”. Here we’ll give you a quick overview of this family so you can recognize them in the wild.\nThe most important member of this family is lapply(), which is very similar to purrr::map()3. In fact, because we haven’t used any of map()’s more advanced features, you can replace every map() call in Chapter 27 with lapply().\nThere’s no exact base R equivalent to across() but you can get close by using [ with lapply(). This works because under the hood, data frames are lists of columns, so calling lapply() on a data frame applies the function to each column.\n\ndf &lt;- tibble(a = 1, b = 2, c = \"a\", d = \"b\", e = 4)\n\n# First find numeric columns\nnum_cols &lt;- sapply(df, is.numeric)\nnum_cols\n#&gt;     a     b     c     d     e \n#&gt;  TRUE  TRUE FALSE FALSE  TRUE\n\n# Then transform each column with lapply() then replace the original values\ndf[, num_cols] &lt;- lapply(df[, num_cols, drop = FALSE], \\(x) x * 2)\ndf\n#&gt; # A tibble: 1 × 5\n#&gt;       a     b c     d         e\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1     2     4 a     b         8\n\nThe code above uses a new function, sapply(). It’s similar to lapply() but it always tries to simplify the result, hence the s in its name, here producing a logical vector instead of a list. We don’t recommend using it for programming, because the simplification can fail and give you an unexpected type, but it’s usually fine for interactive use. purrr has a similar function called map_vec() that we didn’t mention in Chapter 27.\nBase R provides a stricter version of sapply() called vapply(), short for vector apply. It takes an additional argument that specifies the expected type, ensuring that simplification occurs the same way regardless of the input. For example, we could replace the sapply() call above with this vapply() where we specify that we expect is.numeric() to return a logical vector of length 1:\n\nvapply(df, is.numeric, logical(1))\n#&gt;     a     b     c     d     e \n#&gt;  TRUE  TRUE FALSE FALSE  TRUE\n\nThe distinction between sapply() and vapply() is really important when they’re inside a function (because it makes a big difference to the function’s robustness to unusual inputs), but it doesn’t usually matter in data analysis.\nAnother important member of the apply family is tapply() which computes a single grouped summary:\n\ndiamonds |&gt; \n  group_by(cut) |&gt; \n  summarize(price = mean(price))\n#&gt; # A tibble: 5 × 2\n#&gt;   cut       price\n#&gt;   &lt;ord&gt;     &lt;dbl&gt;\n#&gt; 1 Fair      4359.\n#&gt; 2 Good      3929.\n#&gt; 3 Very Good 3982.\n#&gt; 4 Premium   4584.\n#&gt; 5 Ideal     3458.\n\ntapply(diamonds$price, diamonds$cut, mean)\n#&gt;      Fair      Good Very Good   Premium     Ideal \n#&gt;  4358.758  3928.864  3981.760  4584.258  3457.542\n\nUnfortunately tapply() returns its results in a named vector which requires some gymnastics if you want to collect multiple summaries and grouping variables into a data frame (it’s certainly possible to not do this and just work with free floating vectors, but in our experience that just delays the work). If you want to see how you might use tapply() or other base techniques to perform other grouped summaries, Hadley has collected a few techniques in a gist.\nThe final member of the apply family is the titular apply(), which works with matrices and arrays. In particular, watch out for apply(df, 2, something), which is a slow and potentially dangerous way of doing lapply(df, something). This rarely comes up in data science because we usually work with data frames and not matrices."
  },
  {
    "objectID": "base-R.html#for-loops",
    "href": "base-R.html#for-loops",
    "title": "28  A field guide to base R",
    "section": "28.5 for loops",
    "text": "28.5 for loops\nfor loops are the fundamental building block of iteration that both the apply and map families use under the hood. for loops are powerful and general tools that are important to learn as you become a more experienced R programmer. The basic structure of a for loop looks like this:\n\nfor (element in vector) {\n  # do something with element\n}\n\nThe most straightforward use of for loops is to achieve the same effect as walk(): call some function with a side-effect on each element of a list. For example, in Section 27.4.1 instead of using walk():\n\npaths |&gt; walk(append_file)\n\nWe could have used a for loop:\n\nfor (path in paths) {\n  append_file(path)\n}\n\nThings get a little trickier if you want to save the output of the for loop, for example reading all of the excel files in a directory like we did in Chapter 27:\n\npaths &lt;- dir(\"data/gapminder\", pattern = \"\\\\.xlsx$\", full.names = TRUE)\nfiles &lt;- map(paths, readxl::read_excel)\n\nThere are a few different techniques that you can use, but we recommend being explicit about what the output is going to look like upfront. In this case, we’re going to want a list the same length as paths, which we can create with vector():\n\nfiles &lt;- vector(\"list\", length(paths))\n\nThen instead of iterating over the elements of paths, we’ll iterate over their indices, using seq_along() to generate one index for each element of paths:\n\nseq_along(paths)\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\nUsing the indices is important because it allows us to link to each position in the input with the corresponding position in the output:\n\nfor (i in seq_along(paths)) {\n  files[[i]] &lt;- readxl::read_excel(paths[[i]])\n}\n\nTo combine the list of tibbles into a single tibble you can use do.call() + rbind():\n\ndo.call(rbind, files)\n#&gt; # A tibble: 1,704 × 5\n#&gt;   country     continent lifeExp      pop gdpPercap\n#&gt;   &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia         28.8  8425333      779.\n#&gt; 2 Albania     Europe       55.2  1282697     1601.\n#&gt; 3 Algeria     Africa       43.1  9279525     2449.\n#&gt; 4 Angola      Africa       30.0  4232095     3521.\n#&gt; 5 Argentina   Americas     62.5 17876956     5911.\n#&gt; 6 Australia   Oceania      69.1  8691212    10040.\n#&gt; # ℹ 1,698 more rows\n\nRather than making a list and saving the results as we go, a simpler approach is to build up the data frame piece-by-piece:\n\nout &lt;- NULL\nfor (path in paths) {\n  out &lt;- rbind(out, readxl::read_excel(path))\n}\n\nWe recommend avoiding this pattern because it can become very slow when the vector is very long. This is the source of the persistent canard that for loops are slow: they’re not, but iteratively growing a vector is."
  },
  {
    "objectID": "base-R.html#plots",
    "href": "base-R.html#plots",
    "title": "28  A field guide to base R",
    "section": "28.6 Plots",
    "text": "28.6 Plots\nMany R users who don’t otherwise use the tidyverse prefer ggplot2 for plotting due to helpful features like sensible defaults, automatic legends, and a modern look. However, base R plotting functions can still be useful because they’re so concise — it takes very little typing to do a basic exploratory plot.\nThere are two main types of base plot you’ll see in the wild: scatterplots and histograms, produced with plot() and hist() respectively. Here’s a quick example from the diamonds dataset:\n\n# Left\nhist(diamonds$carat)\n\n# Right\nplot(diamonds$carat, diamonds$price)\n\n\n\n\n\n\n\n\n\n\n\nNote that base plotting functions work with vectors, so you need to pull columns out of the data frame using $ or some other technique."
  },
  {
    "objectID": "base-R.html#summary",
    "href": "base-R.html#summary",
    "title": "28  A field guide to base R",
    "section": "28.7 Summary",
    "text": "28.7 Summary\nIn this chapter, we’ve shown you a selection of base R functions useful for subsetting and iteration. Compared to approaches discussed elsewhere in the book, these functions tend to have more of a “vector” flavor than a “data frame” flavor because base R functions tend to take individual vectors, rather than a data frame and some column specification. This often makes life easier for programming and so becomes more important as you write more functions and begin to write your own packages.\nThis chapter concludes the programming section of the book. You’ve made a solid start on your journey to becoming not just a data scientist who uses R, but a data scientist who can program in R. We hope these chapters have sparked your interest in programming and that you’re looking forward to learning more outside of this book."
  },
  {
    "objectID": "base-R.html#footnotes",
    "href": "base-R.html#footnotes",
    "title": "28  A field guide to base R",
    "section": "",
    "text": "Read https://adv-r.hadley.nz/subsetting.html#subset-multiple to see how you can also subset a data frame like it is a 1d object and how you can subset it with a matrix.↩︎\nBut it doesn’t handle grouped data frames differently and it doesn’t support selection helper functions like starts_with().↩︎\nIt just lacks convenient features like progress bars and reporting which element caused the problem if there’s an error.↩︎"
  },
  {
    "objectID": "communicate.html",
    "href": "communicate.html",
    "title": "Communicate",
    "section": "",
    "text": "到目前为止，您已经学习了将数据导入 R，将其整理成便于分析的形式，然后通过转换和可视化来理解您的数据的工具。但 是，除非您能向他人解释您的分析，否则您的分析再好都没有用：您需要传达（communicate）您的结果。\n\n\n\n\nFigure 1: 沟通是数据科学过程的最后一部分；如果您不能将您的结果传达给其他人，那么您分析再好都没有用。\n\n\n\nCommunication 是以下两章的主题：\n\n在 Chapter 29, 您将了解 Quarto，这是一种用于集成文字、代码和结果的工具。 您可以使用 Quarto 进行分析师与分析师（analyst-to-analyst）之间的交流以及分析师与决策者（analyst-to-decision-maker）之间的交流。 由于 Quarto 格式的强大功能，您甚至可以将同一份文档用于两种目的。\n在 Chapter 30, 您将学习您可以使用 Quarto 生成的许多其他类型的输出，包括 dashboards、websites、books。\n\n这些章节主要关注交流的技术机制，而不是与其他人交流思想的真正难题。然 而，还有很多其他关于沟通的好的书籍，我们会在每一章的末尾为您指出。"
  },
  {
    "objectID": "quarto.html#introduction",
    "href": "quarto.html#introduction",
    "title": "29  Quarto",
    "section": "\n29.1 Introduction",
    "text": "29.1 Introduction\n29.1 简介\nQuarto提供了一个统一的数据科学创作框架，将您的代码、其结果和您的文字结合在一起。Q uarto documents 是完全可复制的，并支持数十种输出格式，如 PDF、Word files、presentations 等。\nQuarto files 旨在以三种方式使用：\n\n用于与决策者沟通，他们希望关注结论，而不是分析背后的代码。\n与其他数据科学家（包括未来的您！）合作，他们对您的结论和您如何达到它们（即代码）都感兴趣。\n作为进行数据科学的环境，作为现代实验室笔记本，在其中不仅可以记录您所做的事情，还可以记录您当时的想法。\n\nQuarto 是一个命令行界面工具，而不是 R 包。 这意味着帮助不是通过 ? 获得的。 相反，在您学习本章并在未来使用 Quarto 时，您应该参考 Quarto documentation。\n如果您是 R Markdown 用户，您可能会想 “Quarto 听起来很像 R Markdown”。 您没错！ Quarto 将 R Markdown 生态系统中许多包（rmarkdown、bookdown、distill、xaringan 等）的功能统一到一个一致的系统中，并通过对 Python 和 Julia 等多种编程语言的原生支持扩展它。 在某种意义上，Quarto 反映了在十年中扩展和支持 R Markdown 生态系统所学到的一切。\n\n29.1.1 Prerequisites\n29.1.1 先决条件\n您需要 Quarto command line interface (Quarto CLI)，但您不需要显式安装或加载它，因为 RStudio 在需要时会自动执行这两项操作。"
  },
  {
    "objectID": "quarto.html#quarto-basics",
    "href": "quarto.html#quarto-basics",
    "title": "29  Quarto",
    "section": "\n29.2 Quarto basics",
    "text": "29.2 Quarto basics\n29.2 Quarto 基础\n这是一个 Quarto 文件 – 一个扩展名为 .qmd` 的纯文本文件：\n\n---\ntitle: \"Diamond sizes\"\ndate: 2022-09-12\nformat: html\n---\n\n```{r}\n#| label: setup\n#| include: false\n\nlibrary(tidyverse)\n\nsmaller <- diamonds |> \n  filter(carat <= 2.5)\n```\n\nWe have data about `r nrow(diamonds)` diamonds.\nOnly `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats.\nThe distribution of the remainder is shown below:\n\n```{r}\n#| label: plot-smaller-diamonds\n#| echo: false\n\nsmaller |> \n  ggplot(aes(x = carat)) + \n  geom_freqpoly(binwidth = 0.01)\n```\n\n它包含三种重要类型的内容：\n\n一个（可选的）YAML header 由 ---s 包围。\n\nChunks of R code 由 ``` 包围。\n混合了简单文本格式如 # heading 和 _italics_ 的文本。\n\nFigure 29.1 显示了 RStudio 中的 .qmd 文档，其中代码和输出交错。您 可以通过点击 Run 图标（它看起来像 chunk 顶部的播放按钮），或按 Cmd/Ctrl + Shift + Enter 来运行每个代码块。 RStudio 执行代码并将结果与代码内联显示。\n\n\n\n\nFigure 29.1: RStudio 中的 Quarto 文档。代码和输出交错在文档中，绘图输出显示在代码的正下方。\n\n\n\n\n如果您不喜欢在文档中看到图形和输出结果，而更愿意使用 RStudio 的控制台和绘图窗格，您可以点击 “Render” 旁边的齿轮图标，切换到 “Chunk Output in Console”，如 Figure 29.2 所示。\n\n\n\n\nFigure 29.2: RStudio 中的 Quarto 文档，在绘图窗格中具有绘图输出。\n\n\n\n\n要生成包含所有文本、代码和结果的完整报告，点击 “Render” 或按下 Cmd/Ctrl + Shift + K。 您还可以使用 quarto::quarto_render(\"diamond-sizes.qmd\") 以编程方式执行此操作。 这将在 viewer 窗口中显示报告，如 Figure 29.3 所示，并创建一个 HTML 文件。\n\n\n\n\nFigure 29.3: RStudio 中的 Quarto 文档以及在 Viewer 窗口中呈现的文档。\n\n\n\n\n当您渲染文档时，Quarto 将 .qmd 文件发送给 knitr，https://yihui.org/knitr/，它执行所有的 code chunks 并创建一个包含代码及其输出的新的 markdown (.md) 文档。 然后，由 knitr 生成的 markdown 文件被 pandoc，https://pandoc.org/，处理，pandoc 负责创建最终的文件。 这个过程如 Figure 29.4 所示。 这种两步工作流的优势是您可以创建非常广泛的输出格式，您将在 Chapter 30 中了解到这一点。\n\n\n\n\nFigure 29.4: Diagram of Quarto workflow from qmd, to knitr, to md, to pandoc, to output in PDF, MS Word, or HTML formats.\n\n\n\n\n要开始使用您自己的 .qmd 文件，请在菜单栏中选择 File > New File > Quarto Document…。 RStudio 将启动一个向导，您可以使用它来预填充您的文件，以提醒您 Quarto 的关键特性。\n接下来的几节将更详细地介绍 Quarto 文档的三个组成部分：the markdown text, the code chunks, the YAML header。\n\n29.2.1 Exercises\n29.2.1 练习\n\n使用 File > New File > Quarto Document 创建一个新的 Quarto 文档。 阅读说明。 练习逐个运行代码块。 然后通过点击适当的按钮或使用适当的快捷键来渲染（render）文档。 验证您可以修改代码，重新运行它，并查看修改后的输出。\n创建一个新的 Quarto 文档为三种内置格式：HTML、PDF、Word。 渲染这三个文档。 输出有何不同？ 输入有何不同？ （为了构建 PDF 输出，您可能需要安装 LaTeX — 如果需要，RStudio 将提示您安装。）"
  },
  {
    "objectID": "quarto.html#visual-editor",
    "href": "quarto.html#visual-editor",
    "title": "29  Quarto",
    "section": "\n29.3 Visual editor",
    "text": "29.3 Visual editor\n29.3 可视化编辑器\nRStudio 的可视化编辑器提供了一个 WYSIWYM 界面，用于编写 Quarto 文档。 在底层，Quarto 文档（.qmd 文件）采用了 Markdown 语法，这是一种轻量级的约定，用于格式化纯文本文件。 实际上，Quarto 使用了 Pandoc Markdown（一种稍微扩展的 Markdown 版本，Quarto 能够理解），包括表格、引用、交叉引用、脚注、divs/spans、定义列表、属性、原始 HTML/TeX 等，还支持执行代码块并内联显示其输出。 虽然 Markdown 的设计目的是易于阅读和编写，但仍然需要学习新的语法，您将在 Section 29.4 中看到。 因此，如果您对像 .qmd 文件这样的计算文档还不熟悉，但有使用 Google Docs 或 MS Word 等工具的经验，那么在 RStudio 中使用可视化编辑器是开始使用 Quarto 的最简单方法。\n在可视化编辑器中，您可以使用菜单栏上的按钮插入图片、表格、交叉引用等，也可以使用通用的 ⌘ / 快键插入几乎任何内容。 如果您在行的开头（如 Figure 29.5 所示），还可以输入 / 来调用该快捷方式。\n\n\n\n\nFigure 29.5: Quarto visual editor.\n\n\n\n\n使用可视化编辑器还可以方便地插入图像并自定义其显示方式。 您可以直接将剪贴板中的图像粘贴到可视化编辑器中（RStudio 会将该图像的副本放置在项目目录中并链接到它），或者可以使用可视化编辑器的 Insert > Figure / Image 菜单浏览到要插入的图像或粘贴其 URL。 此外，使用相同的菜单，您还可以调整图像的大小，并添加标题、替代文本和链接。\n可视化编辑器还有许多其他功能，我们在此未列举，您在使用它进行创作时可能会发现它们很有用。\n最重要的是，虽然可视化编辑器以格式化方式显示内容，但在底层，它将内容保存为普通的 Markdown，并且您可以在可视化编辑器和源代码编辑器之间切换，使用任一工具查看和编辑内容。\n\n29.3.1 Exercises\n\n使用可视化编辑器重新创建 Figure 29.5 中的文档。\n使用可视化编辑器，使用 Insert 菜单插入一个代码块，然后插入任何工具。\n使用可视化编辑器，找出如何：\n\n添加脚注（footnote）。\n添加水平线（horizontal rule）。\n添加块引用（block quote）。\n\n\n在可视化编辑器中，转到 Insert > Citation 并使用其 DOI (digital object identifier) 插入一篇名为 Welcome to the Tidyverse 的论文的引用，该论文的 DOI 是 10.21105/joss.01686。 渲染文档并观察引用如何显示在文档中。 您在文档的 YAML 中观察到了哪些变化？"
  },
  {
    "objectID": "quarto.html#sec-source-editor",
    "href": "quarto.html#sec-source-editor",
    "title": "29  Quarto",
    "section": "\n29.4 Source editor",
    "text": "29.4 Source editor\n29.4 源码编辑器\n您还可以使用 RStudio 中的源码编辑器编辑 Quarto 文档，无需使用可视化编辑器的帮助。 虽然可视化编辑器对于那些有在 Google docs 等工具中编写经验的人来说会感觉很熟悉，但源码编辑器对于那些有编写 R 脚本或 R Markdown 文档经验的人来说也会感觉很熟悉。 源码编辑器还可以用于调试任何 Quarto 语法错误，因为在纯文本中更容易捕捉这些错误。\n下面的指南显示了如何在源码编辑器中使用 Pandoc 的 Markdown 编写 Quarto 文档。\n\n## Text formatting\n\n*italic* **bold** ~~strikeout~~ `code`\n\nsuperscript^2^ subscript~2~\n\n[underline]{.underline} [small caps]{.smallcaps}\n\n## Headings\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\n## Lists\n\n-   Bulleted list item 1\n\n-   Item 2\n\n    -   Item 2a\n\n    -   Item 2b\n\n1.  Numbered list item 1\n\n2.  Item 2.\n    The numbers are incremented automatically in the output.\n\n## Links and images\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](quarto.png){fig-alt=\"Quarto logo and the word quarto spelled in small case letters\"}\n\n## Tables\n\n| First Header | Second Header |\n|--------------|---------------|\n| Content Cell | Content Cell  |\n| Content Cell | Content Cell  |\n\n最好的学习方法就是亲自尝试。 可能需要几天的时间，但很快你就会熟悉它们，而且不需要思考就能应用它们。 如果你忘记了某些语法，你可以在 Help > Markdown Quick Reference 下找到一个方便的 Markdown 快速参考表。\n\n29.4.1 Exercises\n\n通过创建一个简短的简历来练习所学内容。 标题应为你的姓名，并至少包括教育或就业方面的标题。 每个部分应包含工作/学位的项目符号列表。 用粗体突出显示年份。\n\n使用源码编辑器和 Markdown 快速参考手册，找出以下内容的写法：\n\n添加脚注（footnote）。\n添加水平分隔线（horizontal rule）。\n添加块引用（block quote）。\n\n\n将 diamond-sizes.qmd 文件的内容从 https://github.com/hadley/r4ds/tree/main/quarto 复制粘贴到本地的 R Quarto 文档中。 检查是否可以运行它，然后在频率多边形之后添加描述其最显著特征的文本。\n在 Google doc 或 MS Word （或找到你之前创建的文档）中创建一个文档，其中包含标题、超链接、格式化文本等内容。 将文档的内容复制粘贴到 Quarto 的可视化编辑器中。 然后，切换到源码编辑器并检查源代码。"
  },
  {
    "objectID": "quarto.html#code-chunks",
    "href": "quarto.html#code-chunks",
    "title": "29  Quarto",
    "section": "\n29.5 Code chunks",
    "text": "29.5 Code chunks\n29.5 代码块\n要在 Quarto 文档中运行代码，您需要插入一个代码块。 有三种方法可以实现：\n\n使用键盘快捷键 Cmd + Option + I / Ctrl + Alt + I。\n在编辑器工具栏中使用 “Insert” 按钮图标。\n手动输入代码块的定界符 ```{r} 和 ```。\n\n我们建议您学习使用键盘快捷键。 这将在长期使用中为您节省很多时间！\n您可以继续使用键盘快捷键来运行代码，希望您现在已经熟悉并喜欢它了：Cmd/Ctrl + Enter。 然而，代码块有一个新的键盘快捷键：Cmd/Ctrl + Shift + Enter，它可以运行代码块中的所有代码。 将代码块视为一个函数。 代码块应该相对独立，并围绕着一个特定的任务展开。\n下面的部分描述了代码块头部，包括 ```{r}，后面是可选的代码块标签和其他各种代码块选项，每个选项占据自己的一行，并以 #| 标记。\n\n29.5.1 Chunk label\n29.5.1 代码块标签\n代码块可以被赋予一个可选的标签，例如\n\n```{r}\n#| label: simple-addition\n\n1 + 1\n```\n#> [1] 2\n\n这具有三个优点：\n\n\n您可以使用脚本编辑器左下方的下拉代码导航器更轻松地导航到特定的代码块位置：\n\n\n\n\n\n\n代码块生成的图形将具有有用的名称，使它们更容易在其他地方使用。 在 Section 29.6 中会有更多介绍。\n您可以设置缓存代码块的网络，以避免在每次运行时重新执行昂贵的计算。 在 Section 29.8 中会有更多介绍。\n\n您的代码块标签应该简短但富有启发性，并且不应包含空格。我 们建议使用破折号（-）来分隔单词（而不是下划线，_），并避免在代码块标签中使用其他特殊字符。\n通常情况下，您可以根据自己的喜好为代码块命名，但有一个具有特殊行为的代码块名字：setup。 在笔记本模式下，名为 setup 的代码块将在任何其他代码之前自动运行一次。\n此外，代码块标签不能重复。 每个代码块标签必须是唯一的。\n\n29.5.2 Chunk options\n29.5.2 代码块选项\n代码块的输出可以通过选项（options）进行自定义，这些选项是提供给代码块头部的字段。 Knitr 提供了近 60 个选项，您可以使用它们来自定义您的代码块。 这里我们将介绍您经常使用的最重要的代码块选项。 您可以在 https://yihui.org/knitr/options 上查看完整列表。\n最重要的一组选项控制代码块是否被执行以及在生成的报告中插入哪些结果：\n\neval: false 防止代码被执行。 （显然，如果代码不运行，则不会生成任何结果）。 这对于显示示例代码或禁用大块代码而不需要在每一行进行注释非常有用。\ninclude: false 运行代码，但不显示代码或结果在最终文档中。 使用此选项可以避免在报告中混乱的设置代码。\necho: false 不显示代码，但结果会出现在最终文件中。 在撰写面向不希望看到底层 R 代码的人的报告时使用此选项。\nmessage: false or warning: false 不显示消息或警告出现在最终文件中。\nresults: hide 隐藏打印输出；fig-show: hide 隐藏图形。\nerror: true 即使代码返回错误，仍然继续渲染。 这在最终版本的报告中很少使用，但如果需要调试 .qmd 文件中的具体情况，它可以非常有用。 如果您正在教学 R 并希望故意引入错误，这也很有用。 默认情况下，error: false 导致在文档中出现一个错误时渲染失败。\n\n这些代码块选项都会添加到代码块的头部，在 #| 之后，例如，在下面的代码块中，由于 eval 设置为 false，结果不会被打印出来。\n\n```{r}\n#| label: simple-multiplication\n#| eval: false\n\n2 * 2\n```\n\n下表总结了每个选项抑制的输出类型：\n\n\n\n\n\n\n\n\n\n\n\nOption\nRun code\nShow code\nOutput\nPlots\nMessages\nWarnings\n\n\n\neval: false\nX\n\nX\nX\nX\nX\n\n\ninclude: false\n\nX\nX\nX\nX\nX\n\n\necho: false\n\nX\n\n\n\n\n\n\nresults: hide\n\n\nX\n\n\n\n\n\nfig-show: hide\n\n\n\nX\n\n\n\n\nmessage: false\n\n\n\n\nX\n\n\n\nwarning: false\n\n\n\n\n\nX\n\n\n\n29.5.3 Global options\n29.5.3 全局选项\n随着您在 knitr 中的工作越来越多，您会发现一些默认的代码块选项不符合您的需求，因此您想要进行更改。\n您可以通过在文档的 YAML 部分的 execute 下添加首选项（preferred options）来实现这一点。 例如，如果您为不需要看到代码只需要查看结果和叙述的受众准备报告，您可以在文档级别设置 echo: false。 这将默认隐藏代码，只显示您选择显示的代码块（使用 echo: true）。 您可以考虑设置 message: false 和 warning: false，但这会使调试问题变得更加困难，因为您在最终文档中看不到任何消息。\ntitle: \"My report\"\nexecute:\n  echo: false\n由于 Quarto 被设计为支持多种语言（包括 R 以及其他语言如 Python、Julia 等），并非所有的 knitr 选项都可以在文档执行级别使用，因为其中一些选项仅适用于 knitr 而不适用于 Quarto 用于在其他语言中运行代码的引擎（例如 Jupyter）。 然而，您仍然可以在文档的 knitr 字段下的 opts_chunk 中设置这些选项作为全局选项。 例如，在撰写书籍和教程时，我们会设置以下选项：\ntitle: \"Tutorial\"\nknitr:\n  opts_chunk:\n    comment: \"#>\"\n    collapse: true\n这使用了我们首选的注释（comment）格式，并确保代码和输出紧密结合。\n\n29.5.4 Inline code\n29.5.4 内联代码\n还有一种将 R 代码嵌入到 Quarto 文档中的方法：直接在文本中使用 `r `。 如果您在文本中提到数据的属性，这种方法非常有用。 例如，本章开始时使用的示例文档中有以下内容：\n\nWe have data about `r nrow(diamonds)` diamonds. Only `r nrow(diamonds) - nrow(smaller)` are larger than 2.5 carats. The distribution of the remainder is shown below:\n\n渲染报告后，这些计算的结果将插入到文本中：\n\nWe have data about 53940 diamonds. Only 126 are larger than 2.5 carats. The distribution of the remainder is shown below:\n\n在将数字插入文本时，format() 函数非常有用。 它允许您设置数字的位数（digits），以避免打印过度精确的结果，并使用 big.mark 参数使数字更易读。 您可以将这些功能组合到一个辅助函数中，如下所示：\n\ncomma <- function(x) format(x, digits = 2, big.mark = \",\")\ncomma(3452345)\n#> [1] \"3,452,345\"\ncomma(.12358124331)\n#> [1] \"0.12\"\n\n\n29.5.5 Exercises\n\n添加一个部分（section），探讨钻石尺寸如何根据切工、颜色和净度而变化。 假设您正在为一个不了解 R 的人撰写报告，您可以设置一个全局选项，而不是在每个代码块上设置 echo: false。\n从 https://github.com/hadley/r4ds/tree/main/quarto 下载 diamond-sizes.qmd。 添加一个部分，描述最大的 20 颗钻石，包括显示它们最重要属性的表格。\n修改 diamonds-sizes.qmd，使用 label_comma() 生成漂亮格式的输出。 同时包括大于 2.5 克拉的钻石的百分比。"
  },
  {
    "objectID": "quarto.html#sec-figures",
    "href": "quarto.html#sec-figures",
    "title": "29  Quarto",
    "section": "\n29.6 Figures",
    "text": "29.6 Figures\n29.6 图像\n在 Quarto 文档中，图像可以以嵌入方式（例如，PNG 或 JPEG 文件）或作为代码块的结果生成。\n要从外部文件中嵌入图像，你可以使用 RStudio 中的可视化编辑器的插入菜单，选择 Figure / Image。 这将弹出一个菜单，你可以浏览到要插入的图像，并添加替代文本或标题，并调整其大小。 在可视化编辑器中，你还可以直接从剪贴板中粘贴图像到文档中，RStudio 会将该图像的副本放置在项目文件夹中。\n如果你包含一个生成图像的代码块（例如，包含 ggplot() 调用），生成的图形将自动包含在 Quarto 文档中。\n\n29.6.1 Figure sizing\n29.6.1 图像大小\n在 Quarto 中，图像的大小和形状是一个较大的挑战。 有五个主要选项用于控制图像的大小：fig-width, fig-height, fig-asp, out-width and out-height。 图像的大小调整是具有挑战性的，因为有两个尺寸（R 创建的图形的尺寸和插入输出文档时的尺寸），以及多种指定尺寸的方法（例如 height, width, and aspect ratio: pick two of three）。\n我们推荐使用五个选项中的三个：\n\n图像通常更具美感如果它们具有一致的宽度。 为了实现这个，可以默认设置 fig-width: 6 (6”)、fig-asp: 0.618 （黄金比例）。 然后在单个代码块中，只需要调整 fig-asp。\n通过 out-width 控制输出图像的大小，并将其设置为输出文档的正文宽度的百分比。 建议设置 out-width: \"70%\"、fig-align: center。 这样可以给图像留出空间，不会占用太多的空间。\n要将多个图像放置在一行中，可以将 layout-ncol 设置为 2（两个图形）、3（三个图形）等。 这样，如果 layout-ncol 为 2，每个图形的 out-width 将自动设置为 “50%”；如果 layout-ncol 为 3，每个图形的 out-width 将自动设置为 “33%”。 根据您想要展示的内容（例如，展示数据或展示不同的图像变化），还可以调整 fig-width，如下所述。\n\n如果您发现图像中文本太小，那么您需要调整 fig-width。 如果 fig-width 大于图像在最终文档中的渲染尺寸，那么文本将会过小；如果 fig-width 较小，文本将会过大。 您通常需要进行一些实验来确定 fig-width 和最终文档中宽度之间的正确比例关系。 为了说明这个原理，以下三个图形的 fig-width 分别为 4、6、8：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n如果您希望确保所有图像的字体大小保持一致，在设置 out-width 时，您还需要调整 fig-width 以保持与默认 out-width 的相同比例关系。 例如，如果默认的 fig-width 为 6，out-width 为 “70%”，当您设置 out-width: \"50%\" 时，您需要将 fig-width 设置为 4.3 (6 * 0.5 / 0.7)。\n图像的大小和缩放是一门既有艺术又有科学的学问，正确调整可能需要反复尝试和调整。 您可以在 taking control of plot scaling blog post 中了解更多关于图形大小调整的内容。\n\n29.6.2 Other important options\n29.6.2 其他重要选项\n在代码和文本交织的情况下，比如在这本书中，您可以设置 fig-show: hold，这样绘图会在代码之后显示。 这样做的一个好处是，它迫使您将大块的代码与解释分开。\n要为绘图添加标题（caption），请使用 fig-cap。 在 Quarto 中，这将使图像从内联变为 “floating”。\n如果您要生成 PDF 输出，那么默认的图形类型是 PDF。 这是一个很好的默认设置，因为 PDF 是高质量的矢量图形。 然而，如果您要显示成千上万个点，它们可能会产生非常大和缓慢的图形。 在这种情况下，设置 fig-format: \"png\" 强制使用 PNG 格式。 它们的质量稍低，但文件大小会更小。\n给生成图像的代码块命名是一个好习惯，即使您不经常给其他代码块加标签。 代码块的标签用于生成磁盘上图像的文件名，因此给代码块命名可以更轻松地选择出图像并在其他情况下重用（例如，如果您想快速将单个图像放入电子邮件中）。\n\n29.6.3 Exercises\n\n在可视化编辑器中打开 diamond-sizes.qmd，找到一张钻石的图片，复制它，然后粘贴到文档中。双 击图片，在其上方添加标题。调 整图片的大小，并渲染您的文档。观 察图片如何保存在当前工作目录中。\n在 diamond-sizes.qmd 中编辑生成绘图的代码块的标签，以 fig- 为前缀，并使用代码块选项 fig-cap 添加一个标题说明。然 后，编辑代码块上方的文本，使用 Insert > Cross Reference 为该图形添加一个交叉引用。\n\n逐个使用以下代码块选项更改图像的大小，渲染您的文档，并描述图像的变化情况。\n\nfig-width: 10\nfig-height: 3\nout-width: \"100%\"\nout-width: \"20%\""
  },
  {
    "objectID": "quarto.html#tables",
    "href": "quarto.html#tables",
    "title": "29  Quarto",
    "section": "\n29.7 Tables",
    "text": "29.7 Tables\n29.7 表格\n类似于图像，您可以在 Quarto 文档中包含两种类型的表格。 它们可以是您直接在 Quarto 文档中创建的 Markdown 表格（使用 “Insert Table” 菜单），或者它们可以是作为代码块计算结果生成的表格。 在本节中，我们将重点介绍后者，通过计算生成的表格。\n默认情况下，Quarto 将数据框和矩阵打印为在控制台中看到的样子：\n\nmtcars[1:5, ]\n#>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n如果您希望以其他格式显示数据，可以使用 knitr::kable() 函数。 下面的代码生成 Table 29.1。\n\nknitr::kable(mtcars[1:5, ], )\n\n\n\nTable 29.1: A knitr kable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n阅读 ?knitr::kable 的文档以了解其他自定义表格的方式。 对于更深层次的自定义，可以考虑使用 gt, huxtable, reactable, kableExtra, xtable, stargazer, pander, tables, and ascii 等包。 每个包都提供了一组工具，用于从 R 代码返回格式化的表格。\n\n29.7.1 Exercises\n\n在可视化编辑器中打开 diamond-sizes.qmd，插入一个代码块，并使用 knitr::kable() 添加一个表格，显示 diamonds 数据框的前 5 行。\n使用 gt::gt() 显示相同的表格。\n为表格添加一个以 tbl- 为前缀的代码块标签，并使用代码块选项 tbl-cap 为表格添加标题。然 后，编辑代码块上方的文本，使用 Insert > Cross Reference 为表格添加一个交叉引用。"
  },
  {
    "objectID": "quarto.html#sec-caching",
    "href": "quarto.html#sec-caching",
    "title": "29  Quarto",
    "section": "\n29.8 Caching",
    "text": "29.8 Caching\n29.8 缓存\n通常情况下，每次渲染文档时都从一个完全干净的状态开始。 这对于可重现性非常有好处，因为它确保您已经在代码中捕捉到了每一个重要的计算。 然而，如果某些计算需要很长时间，这可能会很痛苦。 解决方案是使用 cache: true。\n您可以在文档级别启用 Knitr 缓存，以缓存文档中所有计算的结果，使用标准的 YAML 选项：\n---\ntitle: \"My Document\"\nexecute: \n  cache: true\n---\n您还可以在代码块级别启用缓存以在特定代码块中缓存计算结果：\n\n```{r}\n#| cache: true\n\n# code for lengthy computation...\n```\n\n当设置了缓存时，它将会将代码块的输出保存到一个特定命名的文件中。 在后续运行中，knitr 会检查代码是否发生了变化，如果没有变化，它将重用缓存的结果。\n缓存系统必须谨慎使用，因为默认情况下它仅基于代码本身，而不考虑其依赖关系。 例如，在下面的示例中，processed_data 代码块依赖于 raw-data 代码块：\n``` {{r}}\n#| label: raw-data\n#| cache: true\n\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```\n``` {{r}}\n#| label: processed_data\n#| cache: true\n\nprocessed_data <- rawdata |> \n  filter(!is.na(import_var)) |> \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\n通过对 processed_data 代码块设置缓存，如果更改了 dplyr 管道，它将会被重新运行，但如果 read_csv() 调用发生更改，它将不会重新运行。 你可以通过使用 dependson 代码块选项来避免这个问题：\n``` {{r}}\n#| label: processed-data\n#| cache: true\n#| dependson: \"raw-data\"\n\nprocessed_data <- rawdata |> \n  filter(!is.na(import_var)) |> \n  mutate(new_variable = complicated_transformation(x, y, z))\n```\ndependson 应包含一个字符向量，其中包含所有缓存代码块依赖的代码块。 当检测到其中一个依赖项发生更改时，Knitr 将更新缓存块的结果。\n请注意，如果 a_very_large_file.csv 更改，代码块将不会更新，因为 Knitr 缓存只跟踪 .qmd 文件内的更改。 如果你想跟踪该文件的更改，可以使用 cache.extra 选项。 这是一个任意的 R 表达式，当它发生更改时将使缓存无效。 一个好用的函数是 file.mtime()：它返回上次修改的时间。 然后你可以编写：\n``` {{r}}\n#| label: raw-data\n#| cache: true\n#| cache.extra: !expr file.mtime(\"a_very_large_file.csv\")\n\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```\n我们遵循了 David Robinson 的建议，为这些代码块命名：每个代码块的名称都以它创建的主要对象命名。 这样可以更容易理解 dependson 的规范。\n随着缓存策略的逐渐复杂化，定期使用 knitr::clean_cache() 清除所有缓存是一个好主意。\n\n29.8.1 Exercises\n设置一个网络代码块 d 依赖于 c 和 b，其中 b 和 c 依赖于 a。每 个代码块将使用 lubridate::now() 打印当前时间，并设置 cache: true，然后验证你对缓存的理解。"
  },
  {
    "objectID": "quarto.html#troubleshooting",
    "href": "quarto.html#troubleshooting",
    "title": "29  Quarto",
    "section": "\n29.9 Troubleshooting",
    "text": "29.9 Troubleshooting\n29.9 故障排除\n调试 Quarto 文档可能会很具有挑战性，因为您不再处于交互式 R 环境中，而且您需要学习一些新的技巧。 此外，错误可能是由于 Quarto 文档本身的问题或者 Quarto 文档中的 R 代码引起的。\n在带有代码块的文档中，一个常见的错误是重复的代码块标签，如果您的工作流程涉及复制和粘贴代码块，这种错误尤其普遍。 要解决此问题，您只需更改其中一个重复的标签即可。\n如果错误是由文档中的 R 代码引起的，您应该始终尝试在交互式会话中重新创建问题。 重新启动 R，然后从代码菜单中选择 “Run all chunks”，或者使用键盘快捷键 Ctrl + Alt + R。 如果运气好的话，这将重新创建问题，然后您可以通过交互方式找出问题所在。\n如果这样做没有帮助，那么您的交互式环境和 Quarto 环境之间一定存在一些差异。 您需要系统地探索各种选项。 最常见的差异是工作目录：Quarto 的工作目录是它所在的目录。 通过在代码块中包含 getwd()，检查工作目录是否符合您的期望。\n接下来，列出可能导致错误的所有可能性。 您需要系统地检查在您的 R 会话和 Quarto 会话中它们是否相同。 最简单的方法是在引起问题的代码块上设置 error: true，然后使用 print() 和 str() 来检查设置是否符合您的期望。"
  },
  {
    "objectID": "quarto.html#yaml-header",
    "href": "quarto.html#yaml-header",
    "title": "29  Quarto",
    "section": "\n29.10 YAML header",
    "text": "29.10 YAML header\n29.10 YAML 标头\n您可以通过调整 YAML header 的参数来控制许多其他的 “整个文档” 设置。 也许您想知道 YAML 是什么意思：它代表的是 “YAML Ain’t Markup Language”，它被设计用于以对人类易于阅读和编写的方式表示层次化数据。 Quarto 使用它来控制输出的许多细节。 在这里，我们将讨论三个方面：self-contained documents、document parameters、bibliographies。\n\n29.10.1 Self-contained\n29.10.1 自包含\nHTML 文档通常具有许多外部依赖项（例如图像、CSS样式表、JavaScript等），默认情况下，Quarto 会将这些依赖项放在与您的 .qmd 文件相同目录下的 _files 文件夹中。 如果您将 HTML 文件发布到托管平台（例如 QuartoPub, https://quartopub.com/），该目录中的依赖项将与您的文档一起发布，因此可以在发布的报告中访问它们。 然而，如果您希望将报告通过电子邮件发送给同事，您可能更喜欢拥有一个单独的、自包含的（self-contained）HTML 文档，其中嵌入了所有的依赖项。 您可以通过指定 embed-resources 选项来实现这一点：\nformat:\n  html:\n    embed-resources: true\n生成的文件将是自包含（self-contained）的，这意味着它不需要任何外部文件和互联网访问，就可以通过浏览器正确显示。\n\n29.10.2 Parameters\n29.10.2 参数\nQuarto 文档可以包含一个或多个参数，这些参数的值可以在渲染报告时进行设置。 参数在您希望以不同的值重新呈现同一份报告时非常有用，以满足各种关键输入的要求。 例如，您可能需要按分行生成销售报告，按学生生成考试结果报告，或按国家生成人口统计摘要报告。 要声明一个或多个参数，请使用 params 字段。\n以下示例使用一个 my_class 参数来确定要显示哪个车型类别：\n\n---\nformat: html\nparams:\n  my_class: \"suv\"\n---\n\n```{r}\n#| label: setup\n#| include: false\n\nlibrary(tidyverse)\n\nclass <- mpg |> filter(class == params$my_class)\n```\n\n# Fuel economy for `r params$my_class`s\n\n```{r}\n#| message: false\n\nggplot(class, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n```\n\n正如您所看到的，参数在代码块中作为一个只读列表命名为 params 可供使用。\n您可以直接将原子向量写入 YAML 头文件。 您还可以通过在参数值前加上 !expr 来运行任意的 R 表达式。 这是指定日期/时间参数的好方法。\nparams:\n  start: !expr lubridate::ymd(\"2015-01-01\")\n  snapshot: !expr lubridate::ymd_hms(\"2015-01-01 12:30:00\")\n\n29.10.3 Bibliographies and Citations\n29.10.3 参考文献和引用\nQuarto 可以自动按照多种样式生成引用（citations）和参考文献（bibliography）。 向 Quarto 文档添加引用和参考文献的最简单方法是使用 RStudio 中的可视化编辑器。\n要使用可视化编辑器添加引用，请转到 Insert > Citation。 可以从多种来源插入引用：\n\nDOI (Document Object Identifier) references.\nZotero personal or group libraries.\nSearches of Crossref, DataCite, or PubMed.\nYour document bibliography (a .bib file in the directory of your document)\n\n在底层，可视化模式使用标准的 Pandoc markdown 来表示引用（例如，[@citation]）。\n如果您使用前三种方法之一添加引用，可视化编辑器将自动为您创建一个 bibliography.bib 文件，并将引用添加到其中。 它还会在文档的 YAML 中添加一个 bibliography 字段。 随着您添加更多的引用，该文件将填充其引用。 您还可以使用许多常见的参考文献格式（包括 BibLaTeX、BibTeX、EndNote、Medline）直接编辑此文件。\n要在源码编辑器的 .qmd 文件中创建引用，请使用由引用文件中的引用标识符组成的键，前面加上 ‘@’ 符号。 然后将引用放在方括号中。 以下是一些示例：\nSeparate multiple citations with a `;`: Blah blah [@smith04; @doe99].\n\nYou can add arbitrary comments inside the square brackets: \nBlah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].\n\nRemove the square brackets to create an in-text citation: @smith04 \nsays blah, or @smith04 [p. 33] says blah.\n\nAdd a `-` before the citation to suppress the author's name: \nSmith says blah [-@smith04].\n当 Quarto 渲染您的文件时，它会构建并将参考文献追加到文档的末尾。 参考文献将包含来自您的参考文献文件中的每个引用，但不包含章节标题。 因此，常见的做法是在文件末尾加上一个用于参考文献的章节标题，例如 # References 或 # Bibliography。\n您可以通过在 csl 字段中引用 CSL (citation style language) 文件来更改引用和参考文献的样式：\nbibliography: rmarkdown.bib\ncsl: apa.csl\n与 bibliography 字段一样，您的 CSL 文件应该包含文件的路径。 在这里，我们假设 CSL 文件与 .qmd 文件位于同一目录中。 一个寻找常见参考文献样式的好地方是 https://github.com/citation-style-language/styles。"
  },
  {
    "objectID": "quarto.html#workflow",
    "href": "quarto.html#workflow",
    "title": "29  Quarto",
    "section": "\n29.11 Workflow",
    "text": "29.11 Workflow\n29.11 工作流程\n之前，我们讨论了一种基本的工作流程，即在控制台（console）中进行交互式的 R 代码编写，然后将有效的代码部分捕捉到脚本编辑器（script editor）中。 Quarto 将控制台和脚本编辑器结合在一起，模糊了交互式探索和长期代码捕捉之间的界限。 您可以在代码块内快速迭代，使用 Cmd/Ctrl + Shift + Enter 进行编辑和重新执行。 当您满意时，可以继续并开始一个新的代码块。\nQuarto 之所以重要，还因为它紧密地融合了文字和代码。 这使其成为一个出色的分析笔记本（analysis notebook），因为它可以让您开发代码并记录您的想法。 分析笔记本与物理科学中的经典实验室笔记本有许多相同的目标。 它具有以下特点：\n\n记录您做了什么以及为什么这样做。 无论您的记忆有多好，如果您不记录您的操作，总会有一段时间您会忘记重要的细节。 将它们写下来，以免遗忘！\n支持深入思考。 如果您在进行分析时记录下自己的想法，并持续反思它们，您更有可能进行出色的分析。 当您最终撰写分析报告与他人分享时，这也会为您节省时间。\n帮助他人理解您的工作。 很少有人独自进行数据分析，您通常会作为团队的一员工作。 实验室笔记本不仅可以帮助您与同事或实验室伙伴分享您的工作内容，还可以解释您为何这样做。\n\n许多关于有效使用实验室笔记本的良好建议同样适用于分析笔记本。 我们参考了我们自己的经验以及 Colin Purrington 在实验室笔记本方面的建议（https://colinpurrington.com/tips/lab-notebooks），总结出以下几点提示：\n\n确保每个笔记本都有一个描述性的标题、一个引人注目的文件名，并且第一个段落简要描述了分析的目标。\n\n使用 YAML header date 字段记录您开始使用该笔记本的日期：\ndate: 2016-08-23\n使用 ISO8601 YYYY-MM-DD 格式，以确保没有歧义。 即使您平时不以这种方式书写日期，也请使用该格式。\n\n如果您在一个分析想法上花费了很多时间，结果发现它是一条死胡同，不要将其删除！ 写下一个简要说明，解释为什么它失败了，并将其保留在笔记本中。 这将帮助您在未来回到分析时避免走同样的死胡同。\n通常情况下，最好在 R 之外进行数据录入。 但如果您确实需要记录少量数据片段，请使用 tibble::tribble() 方法清晰地排列出来。\n如果您发现数据文件中存在错误，永远不要直接修改它，而是编写代码来纠正该值。 解释为什么要进行修正。\n在您结束当天的工作之前，请确保能够渲染笔记本。 如果使用了缓存，请确保清除缓存。 这将让您在代码仍然清晰明了的情况下解决任何问题。\n如果您希望您的代码能够长期可重现（即，可以在下个月或明年再次运行），您需要跟踪代码所使用的软件包的版本。 一个严谨的方法是使用 renv（https://rstudio.github.io/renv/index.html），它可以将软件包存储在您的项目目录中。 一个快速而简单的方法是包含一个运行 sessionInfo() 的代码块 — 这不会让您轻松地重新创建当前的软件包，但至少您会知道它们是什么。\n在您的职业生涯中，您将创建非常多的分析笔记本。 您应该如何组织它们，以便在将来能够再次找到它们？ 我们建议将它们存储在单独的项目中，并制定一个良好的命名方案。"
  },
  {
    "objectID": "quarto.html#summary",
    "href": "quarto.html#summary",
    "title": "29  Quarto",
    "section": "\n29.12 Summary",
    "text": "29.12 Summary\n29.12 总结\n在本章中，我们向您介绍了 Quarto，用于在一个地方撰写和发布包含代码和文字的可重现的计算文档。 您学习了如何在 RStudio 中使用可视化或源码编辑器编写 Quarto 文档，了解了代码块的工作原理以及如何自定义其选项，学习了如何在 Quarto 文档中包含图像和表格，以及有关计算缓存的选项。 此外，您还学习了如何调整 YAML header 选项以创建自包含或参数化的文档，以及如何包含引用和参考文献。 我们还为您提供了一些故障排除和工作流程的技巧。\n虽然本章的介绍足以让您开始使用 Quarto，但还有很多内容需要学习。 Quarto 目前仍然比较年轻，正快速发展。 了解最新创新的最佳途径是访问官方的 Quarto 网站：https://quarto.org。\n在这里，我们还没有涵盖到两个重要的主题：协作和准确地向其他人传达您的思想细节。 协作是现代数据科学的重要组成部分，通过使用版本控制工具，如 Git 和 GitHub，您可以使自己的工作更加轻松。 我们推荐阅读《Happy Git with R》，这是 Jenny Bryan 编写的一本用户友好的关于 Git 和 GitHub 的介绍，特别适合 R 用户。 这本书可以免费在线阅读：https://happygitwithr.com。\n我们还没有涉及到为了清晰传达分析结果而应该编写的内容。 为了提高您的写作水平，我们强烈推荐阅读 Joseph M. Williams 和 Joseph Bizup 的 Style: Lessons in Clarity and Grace 或 George Gopen 的 The Sense of Structure: Writing from the Reader’s Perspective。 这两本书将帮助您理解句子和段落的结构，并为您提供工具来使您的写作更加清晰。 （这些书如果全新购买会比较昂贵，但它们在许多英语课程中都被使用，因此有很多便宜的二手副本可供选择）。 George Gopen 在其网站 https://www.georgegopen.com/the-litigation-articles.html 上还有一些关于写作的短文。 尽管这些文章是针对律师写作的，但几乎所有内容也适用于数据科学家。"
  },
  {
    "objectID": "quarto-formats.html#introduction",
    "href": "quarto-formats.html#introduction",
    "title": "30  Quarto formats",
    "section": "\n30.1 Introduction",
    "text": "30.1 Introduction\n到目前为止，您已经看到了使用 Quarto 生成 HTML 文档的示例。 本章将简要概述您可以使用 Quarto 生成的其他多种输出类型。\n有两种设置文档输出的方式：\n\n\n永久地，通过修改 YAML header：\ntitle: \"Diamond sizes\"\nformat: html\n\n\n临时地，通过手动调用 quarto::quarto_render()：\n\nquarto::quarto_render(\"diamond-sizes.qmd\", output_format = \"docx\")\n\n这在您希望以编程方式生成多种类型的输出时非常有用，因为 output_format 参数还可以接受一个值列表。\n\nquarto::quarto_render(\"diamond-sizes.qmd\", output_format = c(\"docx\", \"pdf\"))"
  },
  {
    "objectID": "quarto-formats.html#output-options",
    "href": "quarto-formats.html#output-options",
    "title": "30  Quarto formats",
    "section": "\n30.2 Output options",
    "text": "30.2 Output options\nQuarto 提供了多种输出格式。 您可以在 https://quarto.org/docs/output-formats/all-formats.html 找到完整的列表。 许多格式共享一些输出选项（例如，toc: true 用于包含目录），但其他选项是特定于格式的（例如，code-fold: true 将代码块折叠为 HTML 输出中的 <details> 标签，以便用户可以按需显示它，这在 PDF 或 Word 文档中不适用）。\n要覆盖默认选项，您需要使用一个扩展的 format 字段。 例如，如果您想要以浮动的目录渲染一个 html 文档，您可以使用以下方式：\nformat:\n  html:\n    toc: true\n    toc_float: true\n您甚至可以通过提供一个格式列表来将内容渲染为多个输出格式：\nformat:\n  html:\n    toc: true\n    toc_float: true\n  pdf: default\n  docx: default\n请注意特殊的语法（pdf: default），如果您不想覆盖任何默认选项。\n要将文档渲染为 YAML 中指定的所有格式，可以使用 output_format = \"all\"。\n\nquarto::quarto_render(\"diamond-sizes.qmd\", output_format = \"all\")"
  },
  {
    "objectID": "quarto-formats.html#documents",
    "href": "quarto-formats.html#documents",
    "title": "30  Quarto formats",
    "section": "\n30.3 Documents",
    "text": "30.3 Documents\n前一章重点介绍了默认的 html 输出。 在此基础上有几种基本变体，可以生成不同类型的文档。 例如：\n\npdf 生成使用 LaTeX（一种开源文档排版系统）的 PDF 文件，您需要安装它。 如果您尚未安装，RStudio 将提示您进行安装。\ndocx for Microsoft Word (.docx) documents.\nodt for OpenDocument Text (.odt) documents.\nrtf for Rich Text Format (.rtf) documents.\ngfm for a GitHub Flavored Markdown (.md) document.\nipynb for Jupyter Notebooks (.ipynb).\n\n请记住，当生成要与决策者（decision-makers）共享的文档时，您可以通过在文档的 YAML 中设置全局选项来关闭默认显示代码的功能：\nexecute:\n  echo: false\n对于 html 文档，另一个选项是默认隐藏代码块，但可以通过点击显示出来：\nformat:\n  html:\n    code: true"
  },
  {
    "objectID": "quarto-formats.html#presentations",
    "href": "quarto-formats.html#presentations",
    "title": "30  Quarto formats",
    "section": "\n30.4 Presentations",
    "text": "30.4 Presentations\n您还可以使用 Quarto 制作演示文稿（presentations）。 与 Keynote 或 PowerPoint 等工具相比，您的视觉控制权较少，但自动将 R 代码的结果插入演示文稿可以节省大量时间。 演示文稿通过将内容分成幻灯片来工作，每个幻灯片从第二级（##）标题开始。 此外，第一级（#）标题表示一个新部分的开始，该部分标题幻灯片默认位于中间居中。\nQuarto 支持多种演示文稿格式，包括：\n\nrevealjs - HTML presentation with revealjs\npptx - PowerPoint presentation\nbeamer - PDF presentation with LaTeX Beamer.\n\n您可以在 https://quarto.org/docs/presentations 上阅读更多关于使用 Quarto 创建演示文稿的信息。"
  },
  {
    "objectID": "quarto-formats.html#interactivity",
    "href": "quarto-formats.html#interactivity",
    "title": "30  Quarto formats",
    "section": "\n30.5 Interactivity",
    "text": "30.5 Interactivity\n就像任何 HTML 文档一样，使用 Quarto 创建的 HTML 文档也可以包含交互式组件。 在这里，我们介绍了两种在您的 Quarto 文档中包含交互性的选项：htmlwidgets 和 Shiny。\n\n30.5.1 htmlwidgets\nHTML是一种交互式的格式，您可以通过使用 htmlwidgets 来充分利用这种交互性，它是一组 R 函数，用于生成交互式的 HTML 可视化效果。 例如，下面是一个 leaflet 地图。 如果您在网页上查看此页面，您可以拖动地图、放大和缩小等操作。 当然，在书籍中无法进行这些操作，因此 Quarto 会自动为您插入一个静态截图。\n\nlibrary(leaflet)\nleaflet() |>\n  setView(174.764, -36.877, zoom = 16) |> \n  addTiles() |>\n  addMarkers(174.764, -36.877, popup = \"Maungawhau\") \n\nhtmlwidgets 的好处在于您不需要了解 HTML 或 JavaScript 的任何知识就可以使用它们。 所有的细节都被封装在包内部，所以您无需担心这些。\n有许多提供 htmlwidgets 的包，包括：\n\ndygraphs for interactive time series visualizations.\nDT for interactive tables.\nthreejs for interactive 3d plots.\nDiagrammeR for diagrams (like flow charts and simple node-link diagrams).\n\n要了解更多关于 htmlwidgets 的信息，并查看提供 htmlwidgets 的完整包列表，请访问 https://www.htmlwidgets.org。\n\n30.5.2 Shiny\nhtmlwidgets 提供了客户端（client-side）交互性 — 所有的交互性都在浏览器中进行，与 R 无关。 一方面，这非常好，因为您可以分发 HTML 文件而无需连接到 R。 然而，这从根本上限制了您所能做的事情，只能限于已经在 HTML 和 JavaScript 中实现的功能。 另一种选择是使用 shiny，这是一个允许您使用 R 代码而不是 JavaScript 来创建交互性的包。\n要从 Quarto 文档中调用 Shiny 代码，请在 YAML header 添加 server: shiny：\ntitle: \"Shiny Web App\"\nformat: html\nserver: shiny\n接下来，您可以使用 “input” 函数将交互式组件添加到文档中：\n\nlibrary(shiny)\n\ntextInput(\"name\", \"What is your name?\")\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)\n\n\n\n\n\n\n然后，您还需要一个具有 context: server 代码块选项的代码块，其中包含需要在 Shiny 服务器上运行的代码。\n然后，您可以使用 input$name 和 input$age 引用这些值，并且每当它们发生变化时，使用它们的代码将自动重新运行。\n在这里，我们无法向您展示一个实时的 shiny 应用程序，因为 shiny 的交互性发生在服务器端（server-side）。 这意味着您可以编写交互式应用程序而无需了解 JavaScript，但您需要一个服务器来运行它们。 这引入了一个逻辑问题：Shiny 应用程序需要一个 Shiny 服务器才能在线运行。 当您在自己的计算机上运行 Shiny 应用程序时，Shiny 会自动为您设置一个 Shiny 服务器，但如果您想在线发布这种交互性，您需要一个面向公众的 Shiny 服务器。 这就是 shiny 的根本取舍：您可以在 shiny 文档中做任何您在 R 中可以做的事情，但需要有人在运行 R。\n要了解更多关于 Shiny 的内容，我们推荐阅读 Hadley Wickham 的《Mastering Shiny》，https://mastering-shiny.org。"
  },
  {
    "objectID": "quarto-formats.html#websites-and-books",
    "href": "quarto-formats.html#websites-and-books",
    "title": "30  Quarto formats",
    "section": "\n30.6 Websites and books",
    "text": "30.6 Websites and books\n通过一些额外的基础设施，您可以使用 Quarto 生成一个完整的网站或书籍：\n\n将您的 .qmd 文件放置在一个目录中。 index.qmd 将成为首页。\n\n添加一个名为 _quarto.yml 的 YAML 文件，用于为站点提供导航。 在此文件中，将 project 类型设置为 book 或 website，例如：\nproject:\n  type: book\n\n\n例如，下面的 _quarto.yml 文件从三个源文件创建一个网站：index.qmd（首页），viridis-colors.qmd和terrain-colors.qmd。\n\nproject:\n  type: website\n\nwebsite:\n  title: \"A website on color scales\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: viridis-colors.qmd\n        text: Viridis colors\n      - href: terrain-colors.qmd\n        text: Terrain colors\n\n您需要用于书籍的 _quarto.yml 文件的结构非常相似。 以下示例展示了如何创建一个包含四个章节的书籍，并将其渲染为三种不同的输出格式（html、pdf、epub）。 同样，源文件是 .qmd 文件。\n\nproject:\n  type: book\n\nbook:\n  title: \"A book on color scales\"\n  author: \"Jane Coloriste\"\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - viridis-colors.qmd\n    - terrain-colors.qmd\n\nformat:\n  html:\n    theme: cosmo\n  pdf: default\n  epub: default\n\n我们建议您在创建网站和书籍时使用 RStudio 项目。 基于 _quarto.yml 文件，RStudio 将识别您正在使用的项目类型，并在 IDE 中添加一个构建选项卡，您可以使用该选项卡来渲染和预览您的网站和书籍。 网站和书籍都可以使用 quarto::render() 进行渲染。\n请访问 https://quarto.org/docs/websites 了解有关 Quarto websites 的更多信息，以及 https://quarto.org/docs/books 了解有关 books 的更多信息。"
  },
  {
    "objectID": "quarto-formats.html#other-formats",
    "href": "quarto-formats.html#other-formats",
    "title": "30  Quarto formats",
    "section": "\n30.7 Other formats",
    "text": "30.7 Other formats\nQuarto 还提供了更多的输出格式选项：\n\n您可以使用 Quarto Journal 模板撰写期刊文章：https://quarto.org/docs/journals/templates.html。\n您可以将 Quarto 文档输出为 Jupyter Notebooks，使用 format: ipynb: https://quarto.org/docs/reference/formats/ipynb.html。\n\n请访问 https://quarto.org/docs/output-formats/all-formats.html 以获取更多输出格式的列表。"
  },
  {
    "objectID": "quarto-formats.html#summary",
    "href": "quarto-formats.html#summary",
    "title": "30  Quarto formats",
    "section": "\n30.8 Summary",
    "text": "30.8 Summary\n在本章中，我们向您介绍了使用 Quarto 进行结果展示的各种选项，包括静态和交互式文档、演示文稿、网站和书籍。\n要了解更多关于在这些不同格式中进行有效沟通的信息，我们推荐以下资源：\n\n要提升您的演讲技巧，可以尝试阅读 Neal Ford、Matthew McCollough 和 Nathaniel Schutta 合著的Presentation Patterns。 该书提供了一系列有效的模式（包括低级和高级模式），可以应用于改进您的演讲。\n如果您需要进行学术演讲，您可能会喜欢 Leek group guide to giving talks。\n虽然我们没有参加过，但我们听说过 Matt McGarrity 关于公共演讲的在线课程的好评：https://www.coursera.org/learn/public-speaking。\n如果您正在创建许多仪表板（dashboards），请务必阅读 Stephen Few 的 Information Dashboard Design: The Effective Visual Communication of Data。 该书将帮助您创建真正有用而不仅仅漂亮的仪表板。\n有效地传达您的想法通常需要一些关于平面设计的知识。 Robin Williams的 The Non-Designer’s Design Book 是一个很好的入门资料。"
  },
  {
    "objectID": "preface-2e.html",
    "href": "preface-2e.html",
    "title": "Preface to the second edition",
    "section": "",
    "text": "第二版前言\n欢迎来到 “R for Data Science” 第二版！ 这是对第一版的重大改进，删除了我们认为不再有用的内容，增加了我们希望在第一版中包含的内容，并且通常更新了文本和代码，以反映最佳实践的变化。 我们还非常高兴迎来一位新的合著者：Mine Çetinkaya-Rundel，一位知名的数据科学教育家，也是我们在 Posit（以前被称为 RStudio 的公司）的同事之一。\n以下是最重大改变的简要总结：\n\n书的第一部分更名为 “Whole game”。 该部分的目标是在我们深入细节之前，给您提供关于数据科学”整个游戏”的大致细节。\n书的第二部分是 “Visualize”。 与第一版相比，这部分更全面地介绍了数据可视化工具和最佳实践。 获取所有细节的最佳方式仍然是ggplot2 book，但现在 R4DS 也涵盖了更多最重要的技术。\n书的第三部分现在称为 “Transform”，并新增了关于数字、逻辑向量和缺失值的章节。 这些内容以前是数据转换章节的一部分，但需要更多空间来详细介绍所有细节。\n书的第四部分称为 “Import”。 这是一组新的章节，不仅介绍如何读取平面文本文件，还介绍了如何处理电子表格、从数据库获取数据、处理大数据、转换分层数据以及从网站抓取数据。\n“Program” 部分保留了下来，但进行了全面重写，重点放在了函数编写和迭代的最重要部分。 函数编写现在包括有关如何包装 tidyverse 函数（处理整洁评估的挑战）的详细信息，因为在过去几年中，这变得更加简单和重要。 我们新增了一章，介绍了在现有的 R 代码中可能遇到的重要基本 R 函数。\n“modeling” 部分已被移除。 我们从来没有足够的空间来充分展示建模，而且现在有更好的资源可用。 我们通常建议使用 tidymodels 软件包，并阅读 Max Kuhn 和 Julia Silge 的 Tidy Modeling with R。\n“Communicate” 部分保留下来，但已经全面更新，使用 Quarto 取代了 R Markdown。 本书的这个版本是使用 Quarto 编写的，它显然是未来的工具。"
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "1  Introduction",
    "section": "\n1.1 What you will learn",
    "text": "1.1 What you will learn\n1.1 你将学到什么\n数据科学是一个广阔的领域，你不可能通过阅读一本书来掌握它。 本书旨在为您在最重要的工具和足够的知识方面打下坚实的基础，以便在必要时找到资源以了解更多信息。 我们的典型数据科学项目步骤模型类似于 Figure 1.1。\n\n\n\n\nFigure 1.1: 在我们的数据科学过程模型中，您从数据导入（import）和整理（tidy）开始。接下来，您通过转换（transform）、可视化（visualize）和建模（model）的迭代循环来了解您的数据。您完成这个过程是通过向其他人传达（communicate）您的结果。\n\n\n\n\n首先，您必须导入（import）你的数据到 R。 这通常意味着您将存储在文件、数据库或 Web 应用程序编程接口（API）中的数据加载到 R 中的 data frame 中。 如果您无法将数据导入 R，则无法对其进行数据科学处理！\n一旦您导入了数据，整理（tidy）它是一个好主意。 整理数据意味着以与数据集语义匹配的方式存储它，使其形式一致。 简而言之，当您的数据整洁时，每一列都是一个变量（variable），每一行都是一个观察值（observation）。 整洁的数据很重要，因为一致的结构让您可以专注于回答有关数据的问题，而不必纠结于让数据适应不同的函数。\n一旦您拥有了整洁的数据，下一个常见的步骤就是转换（transform）它。 转换包括缩小感兴趣的观察范围（例如一个城市中的所有人或过去一年中的所有数据）、创建现有变量的函数（例如根据距离和时间计算速度）和计算一组摘要统计量（例如计数或平均值）。 整理（tidying）和转换（transforming）一起被称为梳理（wrangling），因为让您的数据呈现出自然易于处理的形式通常感觉像一场战斗！\n一旦您拥有了您需要的整洁数据，就有两个主要的知识生成引擎：可视化（visualization）和建模（modeling）。这 两者具有互补的优缺点，因此任何真正的数据分析都会在它们之间多次迭代。\n可视化（Visualization）是一种基本的人类活动。 好的可视化会向您展示您没有预料到的东西或提出关于数据的新问题。 好的可视化也可能暗示您提出了错误的问题或需要收集不同的数据。 可视化可以让您感到惊讶，但它们并不特别具有可扩展性，因为它们需要人类来解释。\n模型（Models）是可视化的补充工具。 一旦您已经使问题足够精确，就可以使用模型来回答它们。 模型本质上是数学或计算工具，因此它们通常具有很好的可扩展性。 即使它们没有，购买更多计算机通常也比购买更多大脑便宜！ 但是每个模型都有假设，并且根据其本质，模型不能质疑自己的假设。 这意味着模型不能从根本上让你感到惊讶。\n数据科学的最后一步是沟通（communication），这是任何数据分析项目中绝对关键的部分。 除非您也能将结果传达给他人，否则无论您的模型和可视化做得多好都没用。\n所有这些工具都围绕着编程（programming）。 编程是一个跨领域的工具，在几乎每个数据科学项目中都会使用到。 您不需要成为专家程序员才能成为成功的数据科学家，但学习更多编程会得到回报，因为成为更好的程序员可以让您自动化常见任务并更容易解决新问题。\n在每个数据科学项目中，您都会使用这些工具，但它们对大多数项目来说还不够。 这里有一个粗略的 80/20 规则：使用本书中将学习到的工具，您可以解决每个项目约 80％ 左右的问题，但需要其他工具来解决剩余 20％ 左右。 在本书中，我们将指引您了解更多资源。"
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "1  Introduction",
    "section": "\n1.2 How this book is organized",
    "text": "1.2 How this book is organized\n1.2 本书的组织方式\n数据科学工具的前面描述大致按照您在分析中使用它们的顺序组织（当然，您会多次重复它们）。 然而，根据我们的经验，首先学习数据导入（importing）和整理（tidying）是次优的，因为 80％ 的时间它是常规和无聊的，而另外 20％ 的时间它是奇怪和令人沮丧的。 这是开始学习新主题的糟糕地方！ 相反，我们将从已经导入和整理过的数据的可视化（visualization）和转换（transformation）开始。 这样，当您摄取并整理自己的数据时，您的动力将保持高涨，因为您知道痛苦是值得的。\n在每一章中，我们都尽量遵循一致的模式：从一些激励性的例子开始，以便您可以看到更大的图景，然后深入细节。 本书的每个部分都配有练习题，帮助您练习所学内容。 尽管跳过练习可能很诱人，但没有比在真实问题上练习更好的学习方法了。"
  },
  {
    "objectID": "intro.html#what-you-wont-learn",
    "href": "intro.html#what-you-wont-learn",
    "title": "1  Introduction",
    "section": "\n1.3 What you won’t learn",
    "text": "1.3 What you won’t learn\n1.3 你不会学习什么\n本书没有涵盖几个重要主题。 我们认为，始终专注于基本要素非常重要，这样您才能尽快启动并运行。 这意味着本书无法涵盖所有重要主题。\n\n1.3.1 Modeling\n1.3.1 建模\n建模对于数据科学非常重要，但这是一个很大的主题，不幸的是，我们没有足够的空间来给予它应有的覆盖。 要了解更多关于建模的信息，我们强烈推荐我们的同事 Max Kuhn 和 Julia Silge 撰写的 Tidy Modeling with R。 这本书将教您 tidymodels 系列包，正如您从名称中猜到的那样，它们与我们在本书中使用的 tidyverse 包共享许多约定。\n\n1.3.2 Big data\n1.3.2 大数据\n这本书主要关注小型、内存中的数据集。 这是一个正确的起点，因为除非您拥有小数据的经验，否则您无法处理大数据。 您将在本书的大部分内容中学习到的工具可以轻松处理数百兆字节的数据，并且只需一点注意，您通常可以使用它们来处理几千兆字节的数据。 我们还将向您展示如何从数据库和 parquet 文件中获取数据，这两者都经常用于存储大数据。 您不一定能够处理整个数据集，但这不是问题，因为您只需要一个子集或子样本来回答您感兴趣的问题。\n如果您经常处理较大的数据（例如 10-100 GB），我们建议您了解更多关于 data.table 的信息。 我们在这里不详细讲解是因为它使用与 tidyverse 不同的接口，并且需要您学习一些不同的约定。 然而，它非常快速，并且性能回报值得投入一些时间来学习它，如果您正在处理大量数据。\n\n1.3.3 Python, Julia, and friends\n在这本书中，你不会学到任何关于 Python、Julia 或任何其他对数据科学有用的编程语言。 这并不是因为我们认为这些工具不好。 它们不是！而 且实际上，大多数数据科学团队使用多种语言混合，通常至少使用 R 和 Python。 但是我们坚信最好一次掌握一个工具，而 R 是一个很好的起点。"
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "\n1.4 Prerequisites",
    "text": "1.4 Prerequisites\n1.4 先决条件\n我们假设您已经知道一些东西，以便您能够从这本书中获得最大的收益。 您应该具备一般的数字素养，并且如果您已经具备一些基础编程经验，那将会很有帮助。 如果您以前从未编程过，您可能会发现 Garrett 的 Hands on Programming with R 是本书的一个有价值的补充。\n要运行本书中的代码，您需要四样东西：R、RStudio、一个名为 tidyverse 的 R 包集合、少量其他包。 包（Packages）是可复制 R 代码的基本单元。 它们包括可重用函数（functions）、描述如何使用它们的文档（documentation）和示例数据。\n\n1.4.1 R\n要下载 R，请访问 CRAN（comprehensive R archive network），https://cloud.r-project.org。 每年都会发布一个新的 R 主要版本，每年还会有 2-3 个次要版本。 定期更新是个好主意。 升级可能有点麻烦，特别是对于需要重新安装所有包的主要版本，但推迟只会使情况变得更糟。 我们建议使用 R 4.2.0 或更高版本来阅读本书。\n\n1.4.2 RStudio\nRStudio 是一个用于 R 编程的集成开发环境（IDE），您可以从 https://posit.co/download/rstudio-desktop/ 下载。 RStudio 每年更新几次，它会自动通知您新版本何时发布，因此无需再次检查。 定期升级以利用最新和最好的功能是个好主意。 对于本书，请确保您至少拥有 RStudio 2022.02.0。\n当您启动 RStudio 时，Figure 1.2，您将在界面中看到两个关键区域：控制台窗格（console）和输出窗格（output）。 现在，您需要知道的是，在控制台窗格中输入 R 代码并按回车键运行它。 我们将一路学习！1\n\n\n\n\nFigure 1.2: RStudio IDE 有两个关键区域：在左侧的控制台窗格中键入 R 代码，并在右侧的输出窗格中查找绘图。\n\n\n\n\n\n1.4.3 The tidyverse\n您还需要安装一些 R 包。 R 包（package）是一个函数、数据和文档的集合，它扩展了基础 R 的功能。 使用包是成功使用 R 的关键。 您将在本书中学习的大多数包都是所谓的 tidyverse 的一部分。 tidyverse 中的所有包都共享一种数据和 R 编程的共同哲学，并且设计为协同工作。\n您可以使用一行代码安装完整的 tidyverse：\n\ninstall.packages(\"tidyverse\")\n\n在您的计算机上，在控制台（console）中输入该行代码，然后按回车键运行它。 R 将从 CRAN 下载包并将它们安装到您的计算机上。\n在您使用 library() 加载包之前，您将无法使用包中的函数、对象或帮助文件。 安装包后，您可以使用 library() 函数加载它：\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\n#> ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n这告诉您 tidyverse 加载了九个包：dplyr、forcats、ggplot2、lubridate、purrr、readr、stringr、tibble、tidyr。 这些包被认为是 tidyverse 的核心（core），因为您将在几乎所有分析中使用它们。\ntidyverse 中的包变化相当频繁。 您可以通过运行 tidyverse_update() 查看更新是否可用。\n\n1.4.4 Other packages\n1.4.4 其他包\n有许多其他出色的软件包不属于 tidyverse，因为它们解决了不同领域的问题，或者是基于一套不同底层原则设计的。 这并不意味着它们更好或更差，只是使它们不同。 换句话说，与 tidyverse 相补充的不是混乱的宇宙（messyverse），而是许多其他相关软件包的宇宙。 随着您在 R 中处理更多的数据科学项目，您将学习新的软件包和关于数据的新思维方式。\n在本书中，我们将使用许多 tidyverse 之外的软件包。 例如，我们将使用以下软件包，因为它们为我们提供了有趣的数据集，以便我们在学习 R 的过程中进行实践：\n\ninstall.packages(\n  c(\"arrow\", \"babynames\", \"curl\", \"duckdb\", \"gapminder\", \n    \"ggrepel\", \"ggridges\", \"ggthemes\", \"hexbin\", \"janitor\", \"Lahman\", \n    \"leaflet\", \"maps\", \"nycflights13\", \"openxlsx\", \"palmerpenguins\", \n    \"repurrrsive\", \"tidymodels\", \"writexl\")\n  )\n\n我们还将使用一系列其他包作为一次性示例。 你现在不需要安装它们，只要记住每当你看到这样的报错时：\n\nlibrary(ggrepel)\n#> Error in library(ggrepel) : there is no package called ‘ggrepel’\n\n您需要运行 install.packages(\"ggrepel\") 来安装包。"
  },
  {
    "objectID": "intro.html#running-r-code",
    "href": "intro.html#running-r-code",
    "title": "1  Introduction",
    "section": "\n1.5 Running R code",
    "text": "1.5 Running R code\n1.5 运行 R 代码\n上一节向您展示了几个运行 R 代码的示例。 书中的代码如下所示：\n\n1 + 2\n#> [1] 3\n\n如果您在本地控制台（console）中运行相同的代码，它将如下所示：\n> 1 + 2\n[1] 3\n有两个主要的区别。 在您的控制台中，您在 > 之后输入代码，称为提示符（*prompt**）；我们在书中没有显示提示符。 在书中，输出用 #> 进行注释；而在您的控制台中，它直接出现在您的代码之后。 这两个区别意味着如果您正在使用电子版的书，您可以轻松地将代码从书中复制并粘贴到控制台中。\n在整本书中，我们使用一套一致的约定来引用代码：\n\n函数（Functions）以代码字体显示，并跟随圆括号，例如 sum() 或 mean()。\n其他 R objects（如数据或函数参数）以代码字体显示，不带圆括号，例如 flights 或 x。\n有时，为了清楚地表明一个对象来自哪个软件包，我们会使用软件包名称后跟两个冒号，例如 dplyr::mutate() 或 nycflights13::flights。 这也是有效的 R 代码。"
  },
  {
    "objectID": "intro.html#acknowledgments",
    "href": "intro.html#acknowledgments",
    "title": "1  Introduction",
    "section": "\n1.6 Acknowledgments",
    "text": "1.6 Acknowledgments\n1.6 致谢\n这本书不仅仅是 Hadley、Mine、Garrett 的成果，而是与 R 社区中许多人进行的许多对话（面对面和在线）的结果。 我们非常感谢与大家进行的所有对话；非常感谢你们！\n\n\n\nThis book was written in the open, and many people contributed via pull requests. A special thanks to all 259 of you who contributed improvements via GitHub pull requests (in alphabetical order by username): @a-rosenberg, Tim Becker (@a2800276), Abinash Satapathy (@Abinashbunty), Adam Gruer (@adam-gruer), adi pradhan (@adidoit), A. s. (@Adrianzo), Aep Hidyatuloh (@aephidayatuloh), Andrea Gilardi (@agila5), Ajay Deonarine (@ajay-d), @AlanFeder, Daihe Sui (@alansuidaihe), @alberto-agudo, @AlbertRapp, @aleloi, pete (@alonzi), Alex (@ALShum), Andrew M. (@amacfarland), Andrew Landgraf (@andland), @andyhuynh92, Angela Li (@angela-li), Antti Rask (@AnttiRask), LOU Xun (@aquarhead), @ariespirgel, @august-18, Michael Henry (@aviast), Azza Ahmed (@azzaea), Steven Moran (@bambooforest), Brian G. Barkley (@BarkleyBG), Mara Averick (@batpigandme), Oluwafemi OYEDELE (@BB1464), Brent Brewington (@bbrewington), Bill Behrman (@behrman), Ben Herbertson (@benherbertson), Ben Marwick (@benmarwick), Ben Steinberg (@bensteinberg), Benjamin Yeh (@bentyeh), Betul Turkoglu (@betulturkoglu), Brandon Greenwell (@bgreenwell), Bianca Peterson (@BinxiePeterson), Birger Niklas (@BirgerNi), Brett Klamer (@bklamer), @boardtc, Christian (@c-hoh), Caddy (@caddycarine), Camille V Leonard (@camillevleonard), @canovasjm, Cedric Batailler (@cedricbatailler), Christina Wei (@christina-wei), Christian Mongeau (@chrMongeau), Cooper Morris (@coopermor), Colin Gillespie (@csgillespie), Rademeyer Vermaak (@csrvermaak), Chloe Thierstein (@cthierst), Chris Saunders (@ctsa), Abhinav Singh (@curious-abhinav), Curtis Alexander (@curtisalexander), Christian G. Warden (@cwarden), Charlotte Wickham (@cwickham), Kenny Darrell (@darrkj), David Kane (@davidkane9), David (@davidrsch), David Rubinger (@davidrubinger), David Clark (@DDClark), Derwin McGeary (@derwinmcgeary), Daniel Gromer (@dgromer), @Divider85, @djbirke, Danielle Navarro (@djnavarro), Russell Shean (@DOH-RPS1303), Zhuoer Dong (@dongzhuoer), Devin Pastoor (@dpastoor), @DSGeoff, Devarshi Thakkar (@dthakkar09), Julian During (@duju211), Dylan Cashman (@dylancashman), Dirk Eddelbuettel (@eddelbuettel), Edwin Thoen (@EdwinTh), Ahmed El-Gabbas (@elgabbas), Henry Webel (@enryH), Ercan Karadas (@ercan7), Eric Kitaif (@EricKit), Eric Watt (@ericwatt), Erik Erhardt (@erikerhardt), Etienne B. Racine (@etiennebr), Everett Robinson (@evjrob), @fellennert, Flemming Miguel (@flemmingmiguel), Floris Vanderhaeghe (@florisvdh), @funkybluehen, @gabrivera, Garrick Aden-Buie (@gadenbuie), Peter Ganong (@ganong123), Gerome Meyer (@GeroVanMi), Gleb Ebert (@gl-eb), Josh Goldberg (@GoldbergData), bahadir cankardes (@gridgrad), Gustav W Delius (@gustavdelius), Hao Chen (@hao-trivago), Harris McGehee (@harrismcgehee), @hendrikweisser, Hengni Cai (@hengnicai), Iain (@Iain-S), Ian Sealy (@iansealy), Ian Lyttle (@ijlyttle), Ivan Krukov (@ivan-krukov), Jacob Kaplan (@jacobkap), Jazz Weisman (@jazzlw), John Blischak (@jdblischak), John D. Storey (@jdstorey), Gregory Jefferis (@jefferis), Jeffrey Stevens (@JeffreyRStevens), 蒋雨蒙 (@JeldorPKU), Jennifer (Jenny) Bryan (@jennybc), Jen Ren (@jenren), Jeroen Janssens (@jeroenjanssens), @jeromecholewa, Janet Wesner (@jilmun), Jim Hester (@jimhester), JJ Chen (@jjchern), Jacek Kolacz (@jkolacz), Joanne Jang (@joannejang), @johannes4998, John Sears (@johnsears), @jonathanflint, Jon Calder (@jonmcalder), Jonathan Page (@jonpage), Jon Harmon (@jonthegeek), JooYoung Seo (@jooyoungseo), Justinas Petuchovas (@jpetuchovas), Jordan (@jrdnbradford), Jeffrey Arnold (@jrnold), Jose Roberto Ayala Solares (@jroberayalas), Joyce Robbins (@jtr13), @juandering, Julia Stewart Lowndes (@jules32), Sonja (@kaetschap), Kara Woo (@karawoo), Katrin Leinweber (@katrinleinweber), Karandeep Singh (@kdpsingh), Kevin Perese (@kevinxperese), Kevin Ferris (@kferris10), Kirill Sevastyanenko (@kirillseva), Jonathan Kitt (@KittJonathan), @koalabearski, Kirill Müller (@krlmlr), Rafał Kucharski (@kucharsky), Kevin Wright (@kwstat), Noah Landesberg (@landesbergn), Lawrence Wu (@lawwu), @lindbrook, Luke W Johnston (@lwjohnst86), Kara de la Marck (@MarckK), Kunal Marwaha (@marwahaha), Matan Hakim (@matanhakim), Matthias Liew (@MatthiasLiew), Matt Wittbrodt (@MattWittbrodt), Mauro Lepore (@maurolepore), Mark Beveridge (@mbeveridge), @mcewenkhundi, mcsnowface, PhD (@mcsnowface), Matt Herman (@mfherman), Michael Boerman (@michaelboerman), Mitsuo Shiota (@mitsuoxv), Matthew Hendrickson (@mjhendrickson), @MJMarshall, Misty Knight-Finley (@mkfin7), Mohammed Hamdy (@mmhamdy), Maxim Nazarov (@mnazarov), Maria Paula Caldas (@mpaulacaldas), Mustafa Ascha (@mustafaascha), Nelson Areal (@nareal), Nate Olson (@nate-d-olson), Nathanael (@nateaff), @nattalides, Ned Western (@NedJWestern), Nick Clark (@nickclark1000), @nickelas, Nirmal Patel (@nirmalpatel), Nischal Shrestha (@nischalshrestha), Nicholas Tierney (@njtierney), Jakub Nowosad (@Nowosad), Nick Pullen (@nstjhp), @olivier6088, Olivier Cailloux (@oliviercailloux), Robin Penfold (@p0bs), Pablo E. Garcia (@pabloedug), Paul Adamson (@padamson), Penelope Y (@penelopeysm), Peter Hurford (@peterhurford), Peter Baumgartner (@petzi53), Patrick Kennedy (@pkq), Pooya Taherkhani (@pooyataher), Y. Yu (@PursuitOfDataScience), Radu Grosu (@radugrosu), Ranae Dietzel (@Ranae), Ralph Straumann (@rastrau), Rayna M Harris (@raynamharris), @ReeceGoding, Robin Gertenbach (@rgertenbach), Jajo (@RIngyao), Riva Quiroga (@rivaquiroga), Richard Knight (@RJHKnight), Richard Zijdeman (@rlzijdeman), @robertchu03, Robin Kohrs (@RobinKohrs), Robin (@Robinlovelace), Emily Robinson (@robinsones), Rob Tenorio (@robtenorio), Rod Mazloomi (@RodAli), Rohan Alexander (@RohanAlexander), Romero Morais (@RomeroBarata), Albert Y. Kim (@rudeboybert), Saghir (@saghirb), Hojjat Salmasian (@salmasian), Jonas (@sauercrowd), Vebash Naidoo (@sciencificity), Seamus McKinsey (@seamus-mckinsey), @seanpwilliams, Luke Smith (@seasmith), Matthew Sedaghatfar (@sedaghatfar), Sebastian Kraus (@sekR4), Sam Firke (@sfirke), Shannon Ellis (@ShanEllis), @shoili, Christian Heinrich (@Shurakai), S’busiso Mkhondwane (@sibusiso16), SM Raiyyan (@sm-raiyyan), Jakob Krigovsky (@sonicdoe), Stephan Koenig (@stephan-koenig), Stephen Balogun (@stephenbalogun), Steven M. Mortimer (@StevenMMortimer), Stéphane Guillou (@stragu), Sulgi Kim (@sulgik), Sergiusz Bleja (@svenski), Tal Galili (@talgalili), Alec Fisher (@Taurenamo), Todd Gerarden (@tgerarden), Tom Godfrey (@thomasggodfrey), Tim Broderick (@timbroderick), Tim Waterhouse (@timwaterhouse), TJ Mahr (@tjmahr), Thomas Klebel (@tklebel), Tom Prior (@tomjamesprior), Terence Teo (@tteo), @twgardner2, Ulrik Lyngs (@ulyngs), Shinya Uryu (@uribo), Martin Van der Linden (@vanderlindenma), Walter Somerville (@waltersom), @werkstattcodes, Will Beasley (@wibeasley), Yihui Xie (@yihui), Yiming (Paul) Li (@yimingli), @yingxingwu, Hiroaki Yutani (@yutannihilation), Yu Yu Aung (@yuyu-aung), Zach Bogart (@zachbogart), @zeal626, Zeki Akyol (@zekiakyol)."
  },
  {
    "objectID": "intro.html#colophon",
    "href": "intro.html#colophon",
    "title": "1  Introduction",
    "section": "\n1.7 Colophon",
    "text": "1.7 Colophon\n1.7 版权所有\n本书的在线版本可在 https://r4ds.hadley.nz 获得。 它将在实体书重印之间继续发展。 本书的源代码可在 https://github.com/hadley/r4ds 获取。 这本书由 Quarto 提供支持，这使得编写结合文本和可执行代码的书籍变得容易。"
  },
  {
    "objectID": "whole-game.html",
    "href": "whole-game.html",
    "title": "Whole game",
    "section": "",
    "text": "在本书的这一部分，我们的目标是快速为您概述数据科学的主要工具：importing, tidying, transforming, visualizing data，如 Figure 1 所示。 我们希望向您展示数据科学的”整个游戏”，只提供足够的主要要素，让您能够处理真实的、简单的数据集。 本书的后续部分将更深入地涵盖每个主题，扩大您能够处理的数据科学挑战的范围。\n\n\n\n\nFigure 1: 在本书的这一部分，您将学习如何导入（import）、整理（tidy）、转换（transform）、可视化（visualize）数据。\n\n\n\n\n四个章节着重介绍数据科学的工具：\n\n可视化（Visualization）是使用 R 编程的一个很好的起点，因为其收益非常明显：您可以创建优雅且信息丰富的图形，帮助您理解数据。 在 ?sec-data-visualization 中，您将深入学习可视化，了解 ggplot2 plot 的基本结构，以及将数据转化为图形的强大技术。\n仅仅进行可视化通常是不够的，因此在 ?sec-data-transform 中，您将学习关键的动词，这些动词可以帮助您选择重要的变量、过滤关键观察结果、创建新变量和计算总结。\n在 ?sec-data-tidy 中，您将学习关于整洁数据（tidy data）的知识，这是一种一致的存储数据的方式，使转换、可视化和建模变得更加容易。 您将学习其基本原则，以及如何将数据整理成整洁的形式。\n在您进行数据转换和可视化之前，首先需要将数据导入 R 中。 在 ?sec-data-import 中，您将学习如何将 .csv 文件导入 R 中的基础知识。\n除了这些章节之外，还有四个章节着重介绍您的 R 工作流程。 在 Chapter 3, Chapter 5, Chapter 7 中，您将学习编写和组织 R 代码的良好工作流程实践。 这些将为您的长远成功奠定基础，因为它们将为您在处理实际项目时保持组织提供工具。 最后，Chapter 9 将教您如何获取帮助和持续学习。"
  },
  {
    "objectID": "workflow-basics.html#coding-basics",
    "href": "workflow-basics.html#coding-basics",
    "title": "3  Workflow: basics",
    "section": "\n3.1 Coding basics",
    "text": "3.1 Coding basics\n让我们回顾一些基础知识，为了尽快让您开始绘图，我们在之前省略了一些内容。 您可以使用 R 进行基本的数学计算：\n\n1 / 200 * 30\n#> [1] 0.15\n(59 + 73 + 2) / 3\n#> [1] 44.66667\nsin(pi / 2)\n#> [1] 1\n\n您可以使用赋值运算符 <- 创建新的对象：\n\nx <- 3 * 4\n\n请注意，变量 x 的值不会被打印出来，它只是被存储起来了。 如果您想查看该值，请在控制台中输入 x。\n您可以使用 c() 将多个元素组合（combine）成一个向量：\n\nprimes <- c(2, 3, 5, 7, 11, 13)\n\n对向量进行基本的算术运算会应用到向量的每个元素：\n\nprimes * 2\n#> [1]  4  6 10 14 22 26\nprimes - 1\n#> [1]  1  2  4  6 10 12\n\n所有在 R 中创建对象的语句，也就是赋值（assignment）语句，都具有相同的形式：\n\nobject_name <- value\n\n在阅读这段代码时，在脑海中可以说 “object name gets value”。\n您将会进行很多赋值操作，而 <- 的输入可能会很麻烦。 您可以使用 RStudio 的快捷键来节省时间：Alt + - （减号）。 请注意，RStudio 会自动在 <- 周围添加空格，这是一个很好的代码格式化习惯。 代码有时可能很难阅读，因此请给你的眼睛一些休息，并使用空格来提高可读性。"
  },
  {
    "objectID": "workflow-basics.html#comments",
    "href": "workflow-basics.html#comments",
    "title": "3  Workflow: basics",
    "section": "\n3.2 Comments",
    "text": "3.2 Comments\nR 会忽略 # 后面的任何文本。 这允许您编写注释（comments），即 R 忽略但供其他人阅读的文本。 我们有时会在示例中包含注释，以解释代码的运行过程。\n注释可以用于简要描述以下代码的作用。\n\n# create vector of primes\nprimes <- c(2, 3, 5, 7, 11, 13)\n\n# multiply primes by 2\nprimes * 2\n#> [1]  4  6 10 14 22 26\n\n像这样的短小代码片段，可能不需要为每一行代码都留下注释。 但是当你编写的代码变得更加复杂时，注释可以节省你（和你的合作者）很多时间，帮助你理解代码的操作。\n使用注释来解释代码的 why，而不是 how 或 what。 代码的 what 和 how 总是可以通过仔细阅读来弄清楚，即使可能会有些繁琐。 如果你在注释中描述了每个步骤，然后更改了代码，你就必须记住同时更新注释，否则当你将来返回到代码时会感到困惑。\n弄清楚 why 某些事情被做是更加困难的，甚至是不可能的。 例如，geom_smooth() 函数有一个名为 span 的参数，用于控制曲线的平滑程度，较大的值会产生更平滑的曲线。 假设你决定将 span 的值从默认的 0.75 更改为 0.9：未来的读者很容易理解正在发生的是 what，但除非你在注释中记录你的思考过程，否则没有人会明白你 why 改变了默认值。\n对于数据分析代码，使用注释来解释你的整体攻击计划，并在遇到重要发现时记录下来。 从代码本身无法重新获取这些知识。"
  },
  {
    "objectID": "workflow-basics.html#sec-whats-in-a-name",
    "href": "workflow-basics.html#sec-whats-in-a-name",
    "title": "3  Workflow: basics",
    "section": "\n3.3 What’s in a name?",
    "text": "3.3 What’s in a name?\n对象名称必须以字母开头，只能包含字母、数字、_ 和 .。 你希望你的对象名称具有描述性，因此你需要采用一种适用于多个单词的约定。 我们推荐使用蛇形命名法（snake_case），其中你用下划线（_）分隔小写单词。\n\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\nAnd_aFew.People_RENOUNCEconvention\n\n当我们在 Chapter 5 中讨论代码风格时，我们将再次回到命名问题。\n你可以通过输入对象的名称来检查它：\n\nx\n#> [1] 12\n\n提供另一个赋值：\n\nthis_is_a_really_long_name <- 2.5\n\n要检查这个对象，可以尝试使用 RStudio 的自动补全功能：输入 “this”，按下 TAB 键，在唯一前缀之前添加字符，然后按回车键。\n假设你犯了一个错误，this_is_a_really_long_name 的值应该是 3.5，而不是 2.5。 你可以使用另一个键盘快捷键来帮助你进行修正。 例如，你可以按 ↑ 键来调出你最后输入的命令并进行编辑。 或者，输入 “this”，然后按下 Cmd/Ctrl + ↑ 键，以列出以这些字母开头的所有命令。 使用箭头键导航，然后按回车键重新输入命令。 将 2.5 更改为 3.5 并重新运行。\n提供另一个赋值：\n\nr_rocks <- 2^3\n\n让我们尝试检查它：\n\nr_rock\n#> Error: object 'r_rock' not found\nR_rocks\n#> Error: object 'R_rocks' not found\n\n这说明了你和 R 之间的默契协议：R 会为你完成繁琐的计算，但作为交换，你必须在指令上完全准确。 如果不准确，你可能会收到一个错误，提示找不到你要查找的对象。 拼写错误是有影响的；R 无法读懂你的心思，说：“哦，他们在输入 r_rock 时可能是想表达 r_rocks。” 大小写也很重要；同样，R 无法读懂你的心思，说：“哦，他们在输入 R_rocks 时可能是想表达 r_rocks。”"
  },
  {
    "objectID": "workflow-basics.html#calling-functions",
    "href": "workflow-basics.html#calling-functions",
    "title": "3  Workflow: basics",
    "section": "\n3.4 Calling functions",
    "text": "3.4 Calling functions\nR 有一个庞大的内置函数集合，调用方式如下所示：\n\nfunction_name(argument1 = value1, argument2 = value2, ...)\n\n让我们尝试使用 seq() 函数，它可以创建序列（sequences）的数字。同 时，我们也可以了解一些 RStudio 的其他有用功能。 输入 se，然后按下 TAB 键。 一个弹出窗口将显示可能的自动补全选项。 通过输入更多字符（例如，q）来指定 seq()，以消除歧义，或者使用 ↑/↓ 箭头选择。 注意弹出的浮动工具提示，提醒你函数的参数和用途。 如果需要更多帮助，按下 F1 键可以在右下方的帮助选项卡中获取所有细节。\n当你选择了你想要的函数后，再次按下 TAB 键。 RStudio 将会为你添加匹配的左括号（(）和右括号（)）。 输入第一个参数的名称 from，并将其设置为 1。 然后，输入第二个参数的名称 to，并将其设置为 10。 最后，按下回车键。\n\nseq(from = 1, to = 10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n在函数调用中，我们经常省略前几个参数的名称，因此我们可以将其重写如下：\n\nseq(1, 10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n输入以下代码，并注意 RStudio 提供了类似的帮助来配对引号：\n\nx <- \"hello world\"\n\n引号和括号必须总是成对出现。 RStudio 尽力帮助你，但仍然有可能出错，导致不匹配。 如果出现这种情况，R 会显示续行字符 “+”：\n> x <- \"hello\n+\n+ 符号告诉你 R 正在等待更多的输入；它认为你还没有完成。 通常，这意味着你可能忘记了一个 \" 或者一个 )。要么添加缺失的配对，要么按下 ESCAPE 键中断表达式，然后重试。\n请注意，右上方的环境选项卡（Environment tab）显示了你创建的所有对象："
  },
  {
    "objectID": "workflow-basics.html#exercises",
    "href": "workflow-basics.html#exercises",
    "title": "3  Workflow: basics",
    "section": "\n3.5 Exercises",
    "text": "3.5 Exercises\n\n\n为什么这段代码不工作？\n\nmy_variable <- 10\nmy_varıable\n#> Error in eval(expr, envir, enclos): object 'my_varıable' not found\n\n仔细看！ （这可能看起来毫无意义，但当你编程时，训练你的大脑注意到即使是微小的差异也会有所回报。）\n\n\n调整以下每个 R 命令，使其能够正确运行：\n\nlibary(todyverse)\n\nggplot(dTA = mpg) + \n  geom_point(maping = aes(x = displ y = hwy)) +\n  geom_smooth(method = \"lm)\n\n\n按下 Option + Shift + K / Alt + Shift + K。 会发生什么？ 如何通过菜单到达相同的位置？\n\n让我们重新审视 ?sec-ggsave 中的一个练习。 运行以下代码行。 两个图中的哪一个会保存为 mpg-plot.png？ 为什么？\n\nmy_bar_plot <- ggplot(mpg, aes(x = class)) +\n  geom_bar()\nmy_scatter_plot <- ggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\nggsave(filename = \"mpg-plot.png\", plot = my_bar_plot)"
  },
  {
    "objectID": "workflow-basics.html#summary",
    "href": "workflow-basics.html#summary",
    "title": "3  Workflow: basics",
    "section": "\n3.6 Summary",
    "text": "3.6 Summary\n现在你对 R 代码的工作原理有了更多了解，还学到了一些提示，可以帮助你在将来回顾代码时更好地理解它。\n在下一章中，我们将继续你的数据科学之旅，教你如何使用 dplyr 这个 tidyverse 包来转换数据，无论是选择重要变量、筛选感兴趣的行，还是计算摘要统计信息。"
  },
  {
    "objectID": "workflow-style.html#names",
    "href": "workflow-style.html#names",
    "title": "5  Workflow: code style",
    "section": "\n5.1 Names",
    "text": "5.1 Names\n我们在 Section 3.3 中简要讨论了 names。 请记住，变量名（由 <- 创建的和由 mutate() 创建的）应仅使用小写字母、数字和 _。 使用 _ 分隔名称中的单词。\n\n# Strive for:\nshort_flights <- flights |> filter(air_time < 60)\n\n# Avoid:\nSHORTFLIGHTS <- flights |> filter(air_time < 60)\n\n作为一般经验法则，最好选择易于理解的长描述性名称，而不是快速键入的简洁名称。 短名称在编写代码时节省的时间相对较少（尤其是因为自动完成功能会帮助您完成输入），但当您返回旧代码并被迫拼出一个神秘的缩写时，它可能会很耗时。\n如果您有一堆相关事物的名称，请尽量保持一致。 当您忘记以前的约定时很容易出现不一致，所以如果您不得不返回并重命名事物，请不要难过。 一般来说，如果你有一堆变量是同一个主题的变体，你最好给它们一个共同的前缀而不是一个共同的后缀，因为自动完成在变量的开头效果最好。"
  },
  {
    "objectID": "workflow-style.html#spaces",
    "href": "workflow-style.html#spaces",
    "title": "5  Workflow: code style",
    "section": "\n5.2 Spaces",
    "text": "5.2 Spaces\n除了 ^（即 +、-、==、<、…）和赋值运算符 (<-) 外，在数学运算符的两边放置空格。\n\n# Strive for\nz <- (a + b)^2 / d\n\n# Avoid\nz<-( a + b ) ^ 2/d\n\n不要在常规函数调用的括号内或括号外放置空格。 总是在逗号后加一个空格，就像标准英语一样。\n\n# Strive for\nmean(x, na.rm = TRUE)\n\n# Avoid\nmean (x ,na.rm=TRUE)\n\n如果可以改善对齐方式，可以添加额外的空格。 例如，如果您在 mutate() 中创建多个变量，您可能需要添加空格以便所有 = 排成一行。1 这样可以更轻松地浏览代码。\n\nflights |> \n  mutate(\n    speed      = distance / air_time,\n    dep_hour   = dep_time %/% 100,\n    dep_minute = dep_time %%  100\n  )"
  },
  {
    "objectID": "workflow-style.html#sec-pipes",
    "href": "workflow-style.html#sec-pipes",
    "title": "5  Workflow: code style",
    "section": "\n5.3 Pipes",
    "text": "5.3 Pipes\n|> 前面应该始终有一个空格，并且通常应该放在一行的最后。 这使得添加新步骤、重新排列现有步骤、修改步骤中的元素以及通过浏览左侧的 verbs 获得 10,000 英尺的视图变得更加容易。\n\n# Strive for \nflights |>  \n  filter(!is.na(arr_delay), !is.na(tailnum)) |> \n  count(dest)\n\n# Avoid\nflights|>filter(!is.na(arr_delay), !is.na(tailnum))|>count(dest)\n\n如果您要输入的函数具有命名参数（如 mutate() 或 summarize()），请将每个参数放在一个新行中。 如果函数没有命名参数（如 select() 或 filter()），请将所有内容放在一行中，除非它不适合，在这种情况下，您应该将每个参数放在自己的行中。\n\n# Strive for\nflights |>  \n  group_by(tailnum) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights |>\n  group_by(\n    tailnum\n  ) |> \n  summarize(delay = mean(arr_delay, na.rm = TRUE), n = n())\n\n在流水线的第一步之后，每行缩进两个空格。 RStudio 会在 |> 后的换行符后自动为您添加空格。 如果您将每个参数放在自己的行中，请额外缩进两个空格。 确保 ) 独占一行，并且未缩进以匹配函数名称的水平位置。\n\n# Strive for \nflights |>  \n  group_by(tailnum) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE),\n    n = n()\n  )\n\n# Avoid\nflights|>\n  group_by(tailnum) |> \n  summarize(\n             delay = mean(arr_delay, na.rm = TRUE), \n             n = n()\n           )\n\n# Avoid\nflights|>\n  group_by(tailnum) |> \n  summarize(\n  delay = mean(arr_delay, na.rm = TRUE), \n  n = n()\n  )\n\n如果您的管道很容易放在一条线上，那么可以避开其中一些规则。 但根据我们的集体经验，短片段变长是很常见的，因此从长远来看，从所需的所有垂直空间开始通常会节省时间。\n\n# This fits compactly on one line\ndf |> mutate(y = x + 1)\n\n# While this takes up 4x as many lines, it's easily extended to \n# more variables and more steps in the future\ndf |> \n  mutate(\n    y = x + 1\n  )\n\n最后，要小心写很长的管道，比如超过 10-15 行。 试着把它们分解成更小的子任务，给每个任务一个信息丰富的名称。 这些名称将有助于提示读者了解正在发生的事情，并更容易检查中间结果是否符合预期。 每当你可以给一些东西一个信息性名称时，你应该给它一个信息性名称，例如当你从根本上改变数据的结构时，例如，在旋转或总结之后。 不要指望第一次就做对！ 这意味着如果存在可以获得好名字的中间状态，则打破长管道。"
  },
  {
    "objectID": "workflow-style.html#ggplot2",
    "href": "workflow-style.html#ggplot2",
    "title": "5  Workflow: code style",
    "section": "\n5.4 ggplot2",
    "text": "5.4 ggplot2\n适用于管道的相同基本规则也适用于 ggplot2；只需将 + 视为与 |> 相同的方式。\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = month, y = delay)) +\n  geom_point() + \n  geom_line()\n\n同样，如果您不能将函数的所有参数放在一行中，请将每个参数放在其自己的行中：\n\nflights |> \n  group_by(dest) |> \n  summarize(\n    distance = mean(distance),\n    speed = mean(distance / air_time, na.rm = TRUE)\n  ) |> \n  ggplot(aes(x = distance, y = speed)) +\n  geom_smooth(\n    method = \"loess\",\n    span = 0.5,\n    se = FALSE, \n    color = \"white\", \n    linewidth = 4\n  ) +\n  geom_point()\n\n注意从 |> 到 + 的过渡。 我们希望这种转换不是必需的，但不幸的是，ggplot2 是在发现管道之前编写的。"
  },
  {
    "objectID": "workflow-style.html#sectioning-comments",
    "href": "workflow-style.html#sectioning-comments",
    "title": "5  Workflow: code style",
    "section": "\n5.5 Sectioning comments",
    "text": "5.5 Sectioning comments\n随着您的脚本变长，您可以使用 sectioning 注释将您的文件分解成可管理的部分：\n\n# Load data --------------------------------------\n\n# Plot data --------------------------------------\n\nRStudio 提供了一个键盘快捷键来创建这些标题（Cmd/Ctrl + Shift + R），并将它们显示在编辑器左下角的代码导航下拉列表中，如 Figure 5.2 所示。\n\n\n\n\nFigure 5.2: After adding sectioning comments to your script, you can easily navigate to them using the code navigation tool in the bottom-left of the script editor."
  },
  {
    "objectID": "workflow-style.html#exercises",
    "href": "workflow-style.html#exercises",
    "title": "5  Workflow: code style",
    "section": "\n5.6 Exercises",
    "text": "5.6 Exercises\n\n\n按照上述指南重新设计以下管道。\n\nflights|>filter(dest==\"IAH\")|>group_by(year,month,day)|>summarize(n=n(),\ndelay=mean(arr_delay,na.rm=TRUE))|>filter(n>10)\n\nflights|>filter(carrier==\"UA\",dest%in%c(\"IAH\",\"HOU\"),sched_dep_time>\n0900,sched_arr_time<2000)|>group_by(flight)|>summarize(delay=mean(\narr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())|>filter(n>10)"
  },
  {
    "objectID": "workflow-style.html#summary",
    "href": "workflow-style.html#summary",
    "title": "5  Workflow: code style",
    "section": "\n5.7 Summary",
    "text": "5.7 Summary\n在本章中，您学习了代码风格的最重要原则。 从这些开始可能感觉像是一组任意规则（因为它们确实如此！），但随着时间的推移，随着您编写更多代码并与更多人共享代码，您会发现一致的风格是多么重要。 并且不要忘记 styler 包：它是快速提高样式不佳代码质量的好方法。\n在下一章中，我们将切换回数据科学工具，学习整理数据。 Tidy data 是组织数据框的一致方式，在整个 tidyverse 中使用。 这种一致性让您的生活更轻松，因为一旦您拥有整洁的数据，它就可以与绝大多数 tidyverse 函数一起使用。 当然，生活从来都不是一帆风顺的，您在野外遇到的大多数数据集都不是整洁的。 因此，我们还将教您如何使用 tidyr 包来整理您不整洁的数据。"
  },
  {
    "objectID": "workflow-scripts.html#scripts",
    "href": "workflow-scripts.html#scripts",
    "title": "7  Workflow: scripts and projects",
    "section": "\n7.1 Scripts",
    "text": "7.1 Scripts\n到目前为止，您已经使用控制台（console）来运行代码。 这是一个很好的起点，但是当您创建更复杂的 ggplot2 图形和更长的 dplyr 管道时，您会发现它很快就会变得拥挤。 为了给自己更多的工作空间，请使用脚本编辑器。 单击 File 菜单，选择 New File，然后选择 R script，或使用键盘快捷键 Cmd/Ctrl + Shift + N 将其打开。 现在您将看到四个窗格，如 Figure 7.1 所示。 脚本编辑器是试验代码的好地方。 当您想要更改某些内容时，不必重新输入整个内容，只需编辑脚本并重新运行即可。 一旦您编写了可以运行并执行您想要的操作的代码，您就可以将其保存为脚本文件，以便稍后轻松返回。\n\n\n\n\nFigure 7.1: 打开脚本编辑器会在 IDE 的左上角添加一个新窗格。\n\n\n\n\n\n7.1.1 Running code\n脚本编辑器是构建复杂 ggplot2 绘图或长序列 dplyr 操作的绝佳场所。 有效使用脚本编辑器的关键是记住最重要的键盘快捷键之一：Cmd/Ctrl + Enter。 这将在控制台中执行当前的 R 表达式。 例如，采用下面的代码。\n\nlibrary(dplyr)\nlibrary(nycflights13)\n\nnot_cancelled <- flights |> \n  filter(!is.na(dep_delay)█, !is.na(arr_delay))\n\nnot_cancelled |> \n  group_by(year, month, day) |> \n  summarize(mean = mean(dep_delay))\n\n如果光标位于 █ 处，按 Cmd/Ctrl + Enter 将运行生成 not_cancelled 的完整命令。 它还会将光标移动到以下语句（以 not_cancelled |> 开头）。 这样可以通过重复按 Cmd/Ctrl + Enter 轻松地逐步执行完整的脚本。\n您还可以使用 Cmd/Ctrl + Shift + S 一步执行完整的脚本，而不是逐个表达式地运行代码。 定期执行此操作是确保您捕获脚本中代码的所有重要部分的好方法。\n我们建议您始终使用所需的包来启动脚本。 这样，如果您与其他人共享代码，他们可以轻松查看需要安装哪些软件包。 但请注意，您绝对不应该在共享的脚本中包含 install.packages()。 如果他们不小心的话，交出一个会改变他们计算机上某些内容的脚本是不体贴的！\n在学习后续章节时，我们强烈建议从脚本编辑器开始并练习键盘快捷键。 随着时间的推移，以这种方式向控制台发送代码将变得如此自然，您甚至不会想到它。\n\n7.1.2 RStudio diagnostics\n在脚本编辑器中，RStudio 将使用红色波浪线和侧边栏中的十字来突出显示语法错误：\n\n\n\n\n\n将鼠标悬停在十字上即可查看问题所在：\n\n\n\n\n\nRStudio 还会让您了解潜在的问题：\n\n\n\n\n\n\n7.1.3 Saving and naming\n当您退出时，RStudio 会自动保存脚本编辑器的内容，并在您重新打开时自动重新加载。 尽管如此，最好避免使用 Untitled1、Untitled2、Untitled3 等，而是保存脚本并为它们提供信息丰富的名称。\n将文件命名为 code.R 或 myscript.R 可能很诱人，但在为文件选择名称之前应该仔细考虑一下。 文件命名的三个重要原则如下：\n\n文件名应该是机器可读的（machine readable）：避免空格、符号和特殊字符。不要依靠大小写来区分文件。\n文件名应该是人类可读的（human readable）：使用文件名来描述文件中的内容。\n文件名应该与默认顺序配合良好：以数字开头的文件名，以便按字母顺序排序将它们按照使用的顺序排列。\n\n例如，假设项目文件夹中有以下文件。\nalternative model.R\ncode for exploratory analysis.r\nfinalreport.qmd\nFinalReport.qmd\nfig 1.png\nFigure_02.png\nmodel_first_try.R\nrun-first.r\ntemp.txt\n这里存在各种各样的问题：很难找到先运行哪个文件、文件名包含空格、有两个同名但大小写不同的文件（finalreport vs. FinalReport1）、有些名称没有描述其内容（run-first and temp）。\n这是命名和组织同一组文件的更好方法：\n01-load-data.R\n02-exploratory-analysis.R\n03-model-approach-1.R\n04-model-approach-2.R\nfig-01.png\nfig-02.png\nreport-2022-03-20.qmd\nreport-2022-04-02.qmd\nreport-draft-notes.txt\n对关键脚本进行编号可以清楚地显示运行它们的顺序，并且一致的命名方案可以更容易地看到差异。 此外，数字的标签类似，报告通过文件名中包含的日期进行区分，并且 temp 被重命名为 report-draft-notes 以更好地描述其内容。 如果一个目录中有很多文件，建议进一步组织，将不同类型的文件（scripts, figures, etc.）放在不同的目录中。"
  },
  {
    "objectID": "workflow-scripts.html#projects",
    "href": "workflow-scripts.html#projects",
    "title": "7  Workflow: scripts and projects",
    "section": "\n7.2 Projects",
    "text": "7.2 Projects\n有一天，您需要退出 R，去做其他事情，然后再返回分析。 有一天，您将同时进行多项分析，并且希望将它们分开。 有一天，您需要将外部世界的数据引入 R，并将 R 中的数值结果和数字发送回外部世界。\n为了处理这些现实生活中的情况，您需要做出两个决定：\n\n真实来源是什么？ 您将保存什么作为所发生事件的永久记录？\n您的分析在哪里？\n\n\n7.2.1 What is the source of truth?\n作为初学者，可以依赖当前的环境来包含您在分析过程中创建的所有对象。 但是，为了更轻松地处理大型项目或与其他人协作，您的真实来源应该是 R scripts。 使用 R scripts（和数据文件），您可以重新创建环境。 仅在您的环境中，重新创建 R scripts 要困难得多：您要么必须从内存中重新输入大量代码（一路上不可避免地会犯错误），要么必须仔细挖掘您的 R history。\n为了帮助将 R scripts 保留为分析的真实来源，我们强烈建议您指示 RStudio 不要在会话之间保留工作区。 您可以通过运行 usethis::use_blank_slate()2 或 Figure 7.2 中所示的选项来完成此操作。 这会给您带来一些短期的痛苦，因为现在当您重新启动 RStudio 时，它将不再记住您上次运行的代码，也不会再使用您创建的对象或读取的数据集。 但这种短期的痛苦可以避免长期的痛苦，因为它迫使您捕获代码中的所有重要过程。 没有什么比三个月后发现您只将重要计算的结果存储在环境中，而不是计算本身存储在代码中更糟糕的了。\n\n\n\n\nFigure 7.2: 将这些选项复制到你的 RStudio 选项中，以便始终从头开始启动 RStudio 会话。\n\n\n\n\n有一对很棒的键盘快捷键可以协同工作，以确保您在编辑器中捕获了代码的重要部分：\n\n按 Cmd/Ctrl + Shift + 0/F10 重新启动 R。\n按 Cmd/Ctrl + Shift + S 重新运行当前脚本。\n\n我们每周都会使用这种模式数百次。\n或者，如果您不使用键盘快捷键，则可以转到 Session > Restart R，然后突出显示并重新运行当前脚本。\n\n\n\n\n\n\nRStudio server\n\n\n\n如果您使用 RStudio server，默认情况下您的 R 会话永远不会重新启动。 当您关闭 RStudio server 选项卡时，可能感觉您正在关闭 R，但服务器实际上让它在后台运行。 下次您返回时，您将位于与离开时完全相同的位置。 这使得定期重新启动 R 变得更加重要，这样您才能从头开始。\n\n\n\n7.2.2 Where does your analysis live?\nR 有一个强大的工作目录（working directory）概念。 这是 R 查找您要求其加载的文件的位置，也是放置您要求其保存的任何文件的位置。 RStudio 在控制台顶部显示您当前的工作目录：\n\n\n\n\n\n您可以通过运行 getwd() 在 R 代码中打印出来：\n\ngetwd()\n#> [1] \"/Users/hadley/Documents/r4ds\"\n\n在此 R 会话中，当前工作目录（将其视为”home”）位于 hadley 的 Documents 文件夹中名为 r4ds 的子文件夹中。 当您运行此代码时，它会返回不同的结果，因为您的计算机的目录结构与 Hadley 的不同！\n作为 R 初学者，可以将工作目录设置为主目录、文档目录或计算机上任何其他奇怪的目录。 但你已经读了这本书的七章，并且你不再是初学者了。 很快您就应该将项目组织到目录中，并且在处理项目时将 R 的工作目录设置为关联的目录。\n您可以在 R 中设置工作目录，但我们不推荐这样做：\n\nsetwd(\"/path/to/my/CoolProject\")\n\n有一个更好的方法；这种方式还可以让您像专家一样管理您的 R 工作。 就是 RStudio project。\n\n7.2.3 RStudio projects\n将与给定项目关联的所有文件（input data, R scripts, analytical results, and figures）保存在一个目录中是一种明智且常见的做法，RStudio 通过项目（projects）对此提供了内置支持。 让我们创建一个 project 供您在学习本书其余部分时使用。 单击 File > New Project，然后按照 Figure 7.3 中所示的步骤操作。\n\n\n\n\nFigure 7.3: 要创建 new project：（top）首先单击 New Directory，然后（middle）单击 Create Project，然后（bottom）填写目录（project）名称，选择一个好的子目录作为其主目录，然后单击 Create Project。\n\n\n\n\n将您的项目命名为 r4ds，并仔细考虑将项目放在哪个子目录中。 如果您不将其存储在合理的地方，将来将很难找到它！\n此过程完成后，您将获得一个专门用于本书的新 RStudio 项目。 检查项目的 “home” 是否是当前工作目录：\n\ngetwd()\n#> [1] /Users/hadley/Documents/r4ds\n\n现在在脚本编辑器中输入以下命令，然后保存文件，将其命名为 “diamonds.R”。 然后，创建一个名为 “data” 的新文件夹。 您可以通过单击 RStudio Files 窗格中的 “New Folder” 按钮来完成此操作。 最后，运行完整的脚本，将 PNG 和 CSV 文件保存到您的项目目录中。 不用担心细节，你会在本书后面学到它们。\n\nlibrary(tidyverse)\n\nggplot(diamonds, aes(x = carat, y = price)) + \n  geom_hex()\nggsave(\"diamonds.png\")\n\nwrite_csv(diamonds, \"data/diamonds.csv\")\n\n退出 RStudio。 检查与您的项目关联的文件夹 — 注意 .Rproj 文件。 双击该文件以重新打开该项目。 请注意，您回到了上次离开的位置：它是相同的工作目录和命令历史记录，并且您正在处理的所有文件仍然打开。 然而，由于您遵循了我们上面的说明，因此您将拥有一个全新的环境，保证您从头开始。\n以您最喜欢的特定于操作系统的方式，在计算机中搜索 diamonds.png，您会找到 PNG（毫不奇怪），而且还会找到创建它的脚本（diamonds.R）。 这是一个巨大的胜利！ 有一天，您会想要重新制作一个人物，或者只是想了解它的来源。 如果您使用 R 代码严格地将图形保存到文件中，而不是使用鼠标或剪贴板，那么您将能够轻松地重现旧作品！\n\n7.2.4 Relative and absolute paths\n一旦进入项目，您应该只使用相对路径而不是绝对路径。 有什么不同？ 相对路径是相对于工作目录的，即项目的主目录。 当 Hadley 在上面写入 data/diamonds.csv 时，它是 /Users/hadley/Documents/r4ds/data/diamonds.csv 的快捷方式。 但重要的是，如果 Mine 在她的计算机上运行此代码，它将指向 /Users/Mine/Documents/r4ds/data/diamonds.csv。 这就是为什么相对路径很重要：无论 R 项目文件夹位于何处，它们都会起作用。\n无论您的工作目录如何，绝对路径都指向同一位置。 根据您的操作系统，它们看起来略有不同。 在 Windows 上，它们以驱动器号（例如 C:）或两个反斜杠（例如 \\\\servername）开头，在 Mac/Linux 上，它们以斜杠”/“开头（例如 /users/hadley）。 您永远不应该在脚本中使用绝对路径，因为它们会妨碍共享：其他人不会拥有与您完全相同的目录配置。\n操作系统之间还有另一个重要的区别：如何分离路径的组成部分。 Mac 和 Linux 使用斜杠（例如 data/diamonds.csv），Windows 使用反斜杠（例如 data\\diamonds.csv）。 R 可以使用任何一种类型（无论您当前使用什么平台），但不幸的是，反斜杠对 R 来说意味着一些特殊的东西，并且要在路径中获得单个反斜杠，您需要键入两个反斜杠！ 这让生活变得令人沮丧，因此我们建议始终使用带有正斜杠的 Linux/Mac 风格。"
  },
  {
    "objectID": "workflow-scripts.html#exercises",
    "href": "workflow-scripts.html#exercises",
    "title": "7  Workflow: scripts and projects",
    "section": "\n7.3 Exercises",
    "text": "7.3 Exercises\n\n转到 RStudio Tips Twitter account，https://twitter.com/rstudiotips 并找到一个看起来有趣的提示。 练习使用它！\nRStudio 诊断还会报告哪些其他常见错误？ 阅读 https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics 找出。"
  },
  {
    "objectID": "workflow-scripts.html#summary",
    "href": "workflow-scripts.html#summary",
    "title": "7  Workflow: scripts and projects",
    "section": "\n7.4 Summary",
    "text": "7.4 Summary\n在本章中，您学习了如何在 scripts (files) 和 projects (directories) 中组织 R 代码。 就像代码风格一样，一开始这可能感觉像是在忙活。 但是，当您在多个项目中积累更多代码时，您将学会欣赏一点预先的组织如何可以为您节省大量时间。\n总之，scripts 和 projects 为您提供了可靠的工作流程，将在未来为您提供良好的服务：\n\n为每个数据分析项目创建一个 RStudio project。\n将脚本（具有信息丰富的名称）保存在项目中，对其进行编辑，部分或整体运行它们。经常重新启动 R 以确保您已捕获脚本中的所有内容。\n只使用相对路径，而不是绝对路径。\n\n然后，您需要的所有内容都集中在一处，并与您正在处理的所有其他项目完全分开。\n到目前为止，我们已经使用了 R 包中捆绑的数据集。 这样可以更轻松地对预先准备的数据进行一些练习，但显然您的数据无法以这种方式获得。 因此，在下一章中，您将学习如何使用 readr 包将数据从磁盘加载到 R 会话中。"
  },
  {
    "objectID": "workflow-help.html#google-is-your-friend",
    "href": "workflow-help.html#google-is-your-friend",
    "title": "9  Workflow: getting help",
    "section": "\n9.1 Google is your friend",
    "text": "9.1 Google is your friend\n如果您遇到困难，请从 Google 开始。 通常，在查询中添加 “R” 足以将其限制为相关结果：如果搜索没有用，通常意味着没有任何特定于 R 的结果可用。 此外，添加 “tidyverse” 或 “ggplot2” 之类的包名称将有助于将结果范围缩小到您感觉更熟悉的代码，例如 “how to make a boxplot in R” 对比 “how to make a boxplot in R with ggplot2”。 Google 对于错误消息特别有用。 如果您收到错误消息并且不知道其含义，请尝试使用谷歌搜索！ 很可能其他人过去曾对此感到困惑，并且网络上的某个地方会提供帮助。 （如果错误消息不是英文，请运行 Sys.setenv(LANGUAGE = \"en\") 并重新运行代码；您更有可能找到英文错误消息的帮助。）\n如果 Google 没有帮助，请尝试 Stack Overflow。 首先花一点时间搜索现有答案（包括 [R]），将搜索限制为使用 R 的问题和答案。"
  },
  {
    "objectID": "workflow-help.html#making-a-reprex",
    "href": "workflow-help.html#making-a-reprex",
    "title": "9  Workflow: getting help",
    "section": "\n9.2 Making a reprex",
    "text": "9.2 Making a reprex\n如果你的谷歌搜索没有找到任何有用的东西，那么准备一个 reprex 是一个非常好的主意，它是最小可重现示例（reproducible example）的缩写。 良好的 reprex 可以让其他人更容易地帮助你，而且通常你会在解决问题的过程中自己解决问题。 创建 reprex 有两个部分：\n\n首先，您需要使代码可重现。 这意味着您需要捕获所有内容，即包含任何 library() 调用并创建所有必要的对象。 确保完成此操作的最简单方法是使用 reprex 包。\n其次，你需要将其最小化。 去掉与你的问题不直接相关的所有内容。 这通常涉及创建一个比现实生活中所面临的对象小得多且简单得多的 R 对象，甚至使用内置数据。\n\n听起来工作量很大！ 确实可以，但它会带来巨大的回报：\n\n80% 的情况下，创建一个出色的 reprex 可以揭示问题的根源。 令人惊奇的是，编写一个独立且最小的示例的过程经常可以让您回答自己的问题。\n另外 20% 的时间里，你会以一种其他人容易理解的方式抓住问题的本质。 这大大提高了您获得帮助的机会！\n\n手动创建 reprex 时，很容易意外错过某些内容，这意味着您的代码无法在其他人的计算机上运行。 通过使用 reprex 包来避免此问题，该包作为 tidyverse 的一部分安装。 假设您将此代码复制到剪贴板（或者在 RStudio Server 或 Cloud 上选择它）：\n\ny <- 1:4\nmean(y)\n\n然后调用 reprex()，其中默认输出针对 GitHub 进行格式化：\nreprex::reprex()\n渲染良好的 HTML preview 将显示在 RStudio Viewer（如果您使用的是 RStudio）或默认浏览器中。 该 reprex 会自动复制到您的剪贴板（在 RStudio Server 或 Cloud 上，您需要自己复制）：\n``` r\ny <- 1:4\nmean(y)\n#> [1] 2.5\n```\n该文本以一种称为 Markdown 的特殊方式进行格式化，可以将其粘贴到 StackOverflow 或 Github 等网站，它们会自动将其呈现为代码。 以下是 Markdown 在 GitHub 上呈现的样子：\n\ny <- 1:4\nmean(y)\n#> [1] 2.5\n\n其他任何人都可以复制、粘贴并立即运行它。\n为了使示例可重现，您需要包含三件事：required packages, data, and code。\n\nPackages 应该在脚本的顶部加载，这样就可以很容易地看到示例需要哪些包。 现在是检查您是否使用每个软件包的最新版本的好时机；您可能发现了一个错误，该错误自您安装或上次更新该软件包以来已得到修复。 对于 tidyverse 中的包，最简单的检查方法是运行 tidyverse_update()。\n\n包含 data 的最简单方法是使用 dput() 生成重新创建数据所需的 R 代码。 例如，要在 R 中重新创建 mtcars 数据集，请执行以下步骤：\n\nRun dput(mtcars) in R\nCopy the output\nIn reprex, type mtcars <-, then paste.\n\n尝试使用仍能揭示问题的最小数据子集。\n\n\n花一点时间确保您的代码易于其他人阅读：\n\n确保您使用了空格，并且变量名称简洁且信息丰富。\n使用注释指出您的问题所在。\n尽最大努力删除与问题无关的所有内容。\n\n代码越短，就越容易理解并且更容易修复。\n\n\n最后，通过启动新的 R 会话并复制和粘贴脚本来检查您是否确实创建了一个可重现的示例。\n创建 reprex 并不简单，需要一些练习才能学习创建良好的、真正最小的 reprex。 然而，随着您学习和掌握 R，学习提出包含代码的问题并投入时间使其可重现将继续获得回报。"
  },
  {
    "objectID": "workflow-help.html#investing-in-yourself",
    "href": "workflow-help.html#investing-in-yourself",
    "title": "9  Workflow: getting help",
    "section": "\n9.3 Investing in yourself",
    "text": "9.3 Investing in yourself\n您还应该花一些时间做好准备，在问题发生之前解决问题。 从长远来看，每天投入一点时间学习 R 将获得丰厚的回报。 一种方法是在 tidyverse blog 上关注 tidyverse 团队正在做的事情。 为了更广泛地了解 R 社区，我们建议阅读 R Weekly：这是社区努力每周汇总 R 社区中最有趣的新闻。"
  },
  {
    "objectID": "workflow-help.html#summary",
    "href": "workflow-help.html#summary",
    "title": "9  Workflow: getting help",
    "section": "\n9.4 Summary",
    "text": "9.4 Summary\n本章总结了本书的整个游戏部分。 您现在已经了解了数据科学过程中最重要的部分：可视化（visualization）、转换（transformation）、整理（tidying）、导入（importing）。 现在你已经对整个过程有了一个整体的了解，我们开始深入研究小部分的细节。\n本书的下一部分 “Visualize” 深入探讨了图形语法并使用 ggplot2 创建数据可视化，展示了如何使用您迄今为止学到的工具来进行探索性数据分析，并介绍了创建绘图的良好实践用于沟通。"
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "You are reading the work-in-progress second edition of R for Data Science. This chapter is largely complete and just needs final proof reading. You can find the complete first edition at https://r4ds.had.co.nz.\n\n\n\n\n阅读本书的第一部分后，您了解了（至少表面上）进行数据科学的最重要的工具。 现在是时候开始深入研究细节了。 在本书的这一部分中，您将进一步深入了解数据可视化。\n\n\n\n\nFigure 1: Data visualization is often the first step in data exploration.\n\n\n\n\n每一章都会讨论创建数据可视化的一到几个方面。\n\n在 Chapter 10 中，您将了解图形的分层语法。\n在 ?sec-exploratory-data-analysis 中，您将把可视化与好奇心和怀疑精神结合起来，提出并回答有关数据的有趣问题。\n最后，在 ?sec-communication 中，您将学习如何获取探索性图形，提升它们，并将它们转化为说明性图形，这些图形可以帮助分析新手尽可能快速、轻松地了解正在发生的事情。\n\n这三章让您开始进入可视化世界，但还有很多东西需要学习。 了解更多信息的绝对最佳地点是 ggplot2 book：ggplot2: Elegant graphics for data analysis。 它更深入地介绍了基本理论，并提供了更多关于如何组合各个部分来解决实际问题的示例。 另一个很棒的资源是 ggplot2 扩展库 https://exts.ggplot2.tidyverse.org/gallery/。 该站点列出了许多使用新的几何和比例扩展 ggplot2 的软件包。 如果您尝试用 ggplot2 做一些看似困难的事情，那么这是一个很好的起点。"
  },
  {
    "objectID": "layers.html#introduction",
    "href": "layers.html#introduction",
    "title": "10  Layers",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\n在 ?sec-data-visualization 中，您学到的不仅仅是如何制作 scatterplots、bar charts、boxplots。 您学习了一个基础知识，可用于使用 ggplot2 绘制任何类型的绘图。\n在本章中，当您学习图形的分层语法时，您将在此基础上进行扩展。 我们将从更深入地研究美学映射（aesthetic mappings）、几何对象（geometric objects）和分面（facets）开始。 然后，您将了解 ggplot2 在创建绘图时在幕后进行的统计转换。 这些转换用于计算要绘制的新值，例如条形图中的条形高度或箱形图中的中位数。 您还将了解位置调整，这会修改几何图形在绘图中的显示方式。 最后，我们将简要介绍一下坐标系。\n我们不会涵盖每个层的每个功能和选项，但我们将引导您了解 ggplot2 提供的最重要和最常用的功能，并向您介绍扩展 ggplot2 的包。\n\n10.1.1 Prerequisites\n本章重点介绍 ggplot2。 要访问本章中使用的数据集、帮助页面和函数，请通过运行以下代码加载 tidyverse：\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "layers.html#aesthetic-mappings",
    "href": "layers.html#aesthetic-mappings",
    "title": "10  Layers",
    "section": "\n10.2 Aesthetic mappings",
    "text": "10.2 Aesthetic mappings\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\n请记住，与 ggplot2 包捆绑在一起的 mpg 数据框包含 38 种汽车模型的 234 个观察值。\n\nmpg\n#> # A tibble: 234 × 11\n#>   manufacturer model displ  year   cyl trans      drv     cty   hwy fl   \n#>   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr>\n#> 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p    \n#> 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p    \n#> 3 audi         a4      2    2008     4 manual(m6) f        20    31 p    \n#> 4 audi         a4      2    2008     4 auto(av)   f        21    30 p    \n#> 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p    \n#> 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p    \n#> # ℹ 228 more rows\n#> # ℹ 1 more variable: class <chr>\n\nmpg 中的变量包括：\n\ndispl：汽车的发动机尺寸，以升为单位。 数值变量。\nhwy：汽车在高速公路上的燃油效率，以英里/加仑 (mpg) 为单位。 行驶相同距离时，燃油效率低的汽车比燃油效率高的汽车消耗更多的燃油。 数值变量。\nclass：汽车类型。 一个分类变量。\n\n让我们首先可视化各类汽车的 displ 和 hwy 之间的关系。 我们可以使用散点图来做到这一点，其中数值变量 mapped 到 x 和 y aesthetics，分类变量 mapped 到 color 或 shape 等 aesthetic。\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, shape = class)) +\n  geom_point()\n#> Warning: The shape palette can deal with a maximum of 6 discrete values\n#> because more than 6 becomes difficult to discriminate; you have 7.\n#> Consider specifying shapes manually if you must have them.\n#> Warning: Removed 62 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n当 class mapped 到 shape 时，我们收到两个 warnings：\n\n1: The shape palette can deal with a maximum of 6 discrete values because more than 6 becomes difficult to discriminate; you have 7. Consider specifying shapes manually if you must have them.\n2: Removed 62 rows containing missing values (geom_point()).\n\n由于 ggplot2 同时只能使用六个 shapes，因此默认情况下，当您使用 shape aesthetic 时，其他组将不会绘制。 第二个 warning 是相关的 – 数据集中有 62 辆 SUVs，但它们没有被绘制出来。\n类似地，我们也可以将 class map 到 size 或 alpha aesthetics，它们分别控制 points 的 大小和透明度。\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, size = class)) +\n  geom_point()\n#> Warning: Using size for a discrete variable is not advised.\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, alpha = class)) +\n  geom_point()\n#> Warning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\n这两者也会产生 warnings：\n\nUsing alpha for a discrete variable is not advised.\n\n将无序离散（分类）变量（class）Mapping 到有序 aesthetic（size or alpha）通常不是一个好主意，因为它意味着实际上不存在的排名。\n一旦你 map 一个 aesthetic，ggplot2 就会处理剩下的事情。 它选择合理的尺度来符合 aesthetic，并构建一个图例（legend）来解释 levels 和 values 之间的映射。 对于 x 和 y aesthetics，ggplot2 不会创建 legend，但会创建带有刻度线（tick marks）和标签（label）的轴线（axis line）。 轴线（axis line）提供与图例（legend）相同的信息；它解释了位置（locations）和值（values）之间的映射（mapping）。\n您还可以手动将 geom 的视觉属性设置为 geom 函数的参数（outside of aes()），而不是依赖变量映射来确定外观。 例如，我们可以将图中的所有 points 设为蓝色：\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(color = \"blue\")\n\n\n\n\n在这里，颜色并不传达有关变量的信息，而仅改变绘图的外观。 您需要选择一个对该 aesthetic 有意义的值：\n\n字符串形式的颜色名称，例如 color = \"blue\"\n\npoint 的大小（以毫米为单位），例如 size = 1\n\npoint 的 shape 作为数字，例如，shape = 1，如 Figure 10.1 所示。\n\n\n\n\n\nFigure 10.1: R 有 25 个由数字标识的内置形状。有些看似重复：例如，0、15、22 都是正方形。差异来自于 color 和 fill aesthetics 的相互作用。空心形状 (0–14) 的边框由 color 决定；实心形状（15–20）通过 color 填充；填充形状 (21–24) 具有 color 边框并通过 fill 填充。形状的排列使相似的形状彼此相邻。\n\n\n\n\n到目前为止，我们已经讨论了使用 point geom 时可以在散点图中映射或设置的 aesthetics。 您可以在 aesthetic specifications vignette 中了解有关所有可能的 aesthetic mappings 的更多信息：https://ggplot2.tidyverse.org/articles/ggplot2-specs.html。\n您可以用于绘图的具体美观效果取决于您用来表示数据的几何图形。 在下一节中，我们将更深入地研究 geoms。\n\n10.2.1 Exercises\n\n创建 hwy 与 displ 的散点图，其中点用粉红色填充三角形。\n\n为什么以下代码没有生成带有蓝点的图？\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy, color = \"blue\"))\n\n\nstroke aesthetic 有什么作用？ 它适用于什么形状？ （提示：使用 ?geom_point）\n如果将 aesthetic map 到变量名称以外的其他内容，例如 aes(color = displ < 5) 会发生什么？ 请注意，您还需要指定 x 和 y。"
  },
  {
    "objectID": "layers.html#sec-geometric-objects",
    "href": "layers.html#sec-geometric-objects",
    "title": "10  Layers",
    "section": "\n10.3 Geometric objects",
    "text": "10.3 Geometric objects\n这两个图有何相似之处？\n\n\n\n\n\n\n\n\n\n\n两个图都包含相同的 x 变量、相同的 y 变量，并且都描述相同的数据。 但图并不相同。 每个图使用不同的几何对象（geometric object），geom，来表示数据。 左侧的图使用 point geom，右侧的图使用 smooth geom，一条平滑线拟合数据。\n要更改绘图中的 geom，请更改添加到 ggplot() 的 geom 函数。 例如，要绘制上面的图，您可以使用以下代码：\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\nggplot2 中的每个 geom 函数都带有一个 mapping 参数，该参数可以在 geom layer 中本地定义，也可以在 ggplot() layer 中全局定义。 然而，并非每种美学（aesthetic）都适用于每种几何（geom）。 您可以设置点（point）的形状（shape），但无法设置线（line）的形状（shape）。 如果您尝试，ggplot2 将默默地忽略该美学映射（aesthetic mapping）。 另一方面，您可以设置线条（line）的线型（linetype）。 geom_smooth() 将为映射（map）到线型（linetype）的变量的每个唯一值绘制一条具有不同线型（linetype）的不同线（line）。\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy, shape = drv)) + \n  geom_smooth()\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy, linetype = drv)) + \n  geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n在这里，geom_smooth() 将汽车分成三条线（lines）根据它们的 drv 值，该值描述了汽车的传动系统。 一条线描述具有 4 值的所有点，一条线描述具有 f 值的所有点，一条线描述具有 r 值的所有点。 这里，4 代表四轮驱动，f 代表前轮驱动，r 代表后轮驱动。\n如果这听起来很奇怪，我们可以通过将线条叠加在原始数据上，然后根据 drv 对所有内容进行着色来使其更清晰。\n\nggplot(mpg, aes(x = displ, y = hwy, color = drv)) + \n  geom_point() +\n  geom_smooth(aes(linetype = drv))\n\n\n\n\n请注意，该图在同一图中包含两个 geoms。\n许多 geoms，像 geom_smooth()，使用单个几何对象（geometric object）来显示多行数据。 对于这些 geoms，您可以将 group aesthetic 设置为分类变量以绘制多个对象。 ggplot2 将为分组变量的每个唯一值绘制一个单独的对象。 实际上，每当您将 aesthetic 映射到离散变量（如 linetype 示例中）时，ggplot2 都会自动对这些 geoms 的数据进行分组。 依赖此功能很方便，因为 group aesthetic 本身不会向 geoms 添加图例或显着特征。\n\n# Left\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth()\n\n# Middle\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv))\n\n# Right\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n如果将 mappings 放置在 geom 函数中，ggplot2 会将它们视为图层的本地映射（local mappings）。 它将使用这些 mappings 来扩展或覆盖仅该层的全局映射（global mappings）。 这使得在不同图层上展现不同的 aesthetics 成为可能。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(aes(color = class)) + \n  geom_smooth()\n\n\n\n\n您可以使用相同的想法为每一图层指定不同的 data 。 在这里，我们使用红点和空心圆圈来突出显示 two-seater cars。 geom_point() 中的本地数据参数仅覆盖该图层的 ggplot() 中的全局数据参数。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_point(\n    data = mpg |> filter(class == \"2seater\"), \n    color = \"red\"\n  ) +\n  geom_point(\n    data = mpg |> filter(class == \"2seater\"), \n    shape = \"circle open\", size = 3, color = \"red\"\n  )\n\n\n\n\nGeoms 是 ggplot2 的基本构建块。 您可以通过更改绘图的 geom 来彻底改变绘图的外观，并且不同的 geoms 可以揭示数据的不同特征。 例如，下面的直方图和密度图显示高速公路里程的分布是双峰且右偏的，而箱线图则显示两个潜在的异常值。\n\n# Left\nggplot(mpg, aes(x = hwy)) +\n  geom_histogram(binwidth = 2)\n\n# Middle\nggplot(mpg, aes(x = hwy)) +\n  geom_density()\n\n# Right\nggplot(mpg, aes(x = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2 提供了 40 多种几何图形（geoms），但这些并没有涵盖人们可以绘制的所有可能的绘图。 如果您需要不同的 geom，我们建议首先查看扩展包，看看其他人是否已经实现了它（请参阅 https://exts.ggplot2.tidyverse.org/gallery/）。 例如，ggridges 包 (https://wilkelab.org/ggridges) 对于制作山脊线图（ridgeline plots）非常有用，这对于可视化不同级别的分类变量的数值变量的密度非常有用。 在下图中，我们不仅使用了新的 geom（geom_density_ridges()），而且还将相同的变量映射到多种 aesthetics（drv to y, fill, and color）并设置 aesthetic（alpha = 0.5）使密度曲线透明。\n\nlibrary(ggridges)\n#> Warning: package 'ggridges' was built under R version 4.2.1\n\nggplot(mpg, aes(x = hwy, y = drv, fill = drv, color = drv)) +\n  geom_density_ridges(alpha = 0.5, show.legend = FALSE)\n#> Picking joint bandwidth of 1.28\n\n\n\n\n全面概述 ggplot2 提供的所有 geoms 以及包中所有功能的最佳位置是参考页面：https://ggplot2.tidyverse.org/reference。 要了解有关任何单个 geom 的更多信息，请使用帮助（例如?geom_smooth）。\n\n10.3.1 Exercises\n\n您会使用什么 geom 来绘制 line chart？b oxplot？h istogram？a rea chart？\n\n在本章前面我们使用了 show.legend，但没有解释它：\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_smooth(aes(color = drv), show.legend = FALSE)\n\nshow.legend = FALSE 在这里做什么？ 如果删除它会发生什么？ 您认为我们为什么之前使用它？\n\ngeom_smooth() 的 se 参数有什么作用？\n\n重新创建生成以下图表所需的 R 代码。 请注意，图中只要使用分类变量，它就是 drv。"
  },
  {
    "objectID": "layers.html#facets",
    "href": "layers.html#facets",
    "title": "10  Layers",
    "section": "\n10.4 Facets",
    "text": "10.4 Facets\n在 ?sec-data-visualization 中，您了解了如何使用 facet_wrap() 进行分面（faceting），它将图分割成子图，每个子图基于分类变量显示数据的一个子集。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_wrap(~cyl)\n\n\n\n\n要使用两个变量的组合对图进行分面（facet），请从 facet_wrap() 切换到 facet_grid()。 facet_grid() 的第一个参数也是一个公式，但现在它是一个双面公式：rows ~ cols。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl)\n\n\n\n\n默认情况下，每个 facets 的 x 轴和 y 轴共享相同的比例和范围。 当您想要跨 facets 比较数据时，这很有用，但当您想要更好地可视化每个 facet 内的关系时，它可能会受到限制。 将 faceting 函数中的 scales 参数设置为 \"free\" 将允许跨行和列使用不同的轴比例， \"free_x\" 将允许跨行使用不同的比例，\"free_y\" 将允许跨列使用不同的比例。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point() + \n  facet_grid(drv ~ cyl, scales = \"free_y\")\n\n\n\n\n\n10.4.1 Exercises\n\n如果对连续变量进行分面（facet）会发生什么？\n\nfacet_grid(drv ~ cyl) 图中的空单元格是什么意思？ 运行以下代码。 它们与最终的绘图有什么关系？\n\nggplot(mpg) + \n  geom_point(aes(x = drv, y = cyl))\n\n\n\n下面的代码会绘制什么图？ . 的作用是什么？\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n以本节中的第一个 faceted plot 为例：\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n使用 faceting 代替 color aesthetic 有什么优点？ 有什么缺点？ 如果您有更大的数据集，平衡会如何变化？\n\n阅读 ?facet_wrap。 nrow 的作用是什么？ ncol 的作用是什么？ 还有哪些其他选项控制各个面板的布局？ 为什么 facet_grid() 没有 nrow 和 ncol 参数？\n\n以下哪幅图可以更轻松地比较具有不同传动系统的汽车的发动机尺寸 (displ)？ 这对于何时跨行或列放置 faceting 变量意味着什么？\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() + \n  facet_grid(drv ~ .)\n\nggplot(mpg, aes(x = displ)) + \n  geom_histogram() +\n  facet_grid(. ~ drv)\n\n\n\n使用 facet_wrap() 而不是 facet_grid() 重新创建以下图。 facet 标签的位置如何变化？\n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)"
  },
  {
    "objectID": "layers.html#statistical-transformations",
    "href": "layers.html#statistical-transformations",
    "title": "10  Layers",
    "section": "\n10.5 Statistical transformations",
    "text": "10.5 Statistical transformations\n考虑使用 geom_bar() 或 geom_col() 绘制的基本条形图（bar chart）。 下图显示了 diamonds 数据集中的钻石（diamonds）总数，按 cut 分组。 diamonds 数据集位于 ggplot2 包中，包含 ~54,000 颗钻石的信息，包括每颗钻石的 price, carat, color, clarity, and cut。 该图表显示，高质量 cuts 的钻石数量多于低质量 cuts 的钻石。\n\nggplot(diamonds, aes(x = cut)) + \n  geom_bar()\n\n\n\n\n在 x-axis，图表显示 cut，这是 diamonds 的一个变量。 在 y-axis，它显示 count，但 count 不是 diamonds 中的变量！ count 从何而来？ 许多图表，例如 scatterplots，都会绘制数据集的原始值。 其他图表，例如 bar charts，会计算要绘制的新值：\n\nBar charts, histograms, and frequency polygons 对数据进行分 bin，然后绘制 bin counts，即每个 bin 中的点数。\nSmoothers 将模型拟合到您的数据，然后根据模型绘制预测。\nBoxplots 计算分布的 five-number summary，然后将该 summary 显示为特殊格式的 box。\n\n用于计算图形新值的算法称为 stat，是统计变换（statistical transformation）的缩写。 Figure 10.2 显示了此过程如何与 geom_bar() 配合使用。\n\n\n\n\nFigure 10.2: 创建条形图时，我们首先从原始数据开始，然后将其聚合以计算每个条形中的观察值数量，最后映射这些计算变量以绘制美观的图。\n\n\n\n\n您可以通过检查 stat 参数的默认值来了解 geom 使用哪个 stat。 例如，?geom_bar 显示 stat 的默认值为 “count”，这意味着 geom_bar() 使用 stat_count()。 stat_count() 与 geom_bar() 记录在同一页面上。 如果向下滚动，名为 “Computed variables” 的部分会解释它计算两个新变量：count 和 prop。\n每个 geom 都有一个默认的 stat；每个 stat 都有一个默认的 geom。 这意味着您通常可以使用 geoms，而不必担心底层的统计转换。 但是，您可能需要显式使用 stat 的三个原因如下：\n\n\n您可能想要覆盖默认 stat。 在下面的代码中，我们将 geom_bar() 的 stat 从 count （默认）更改为 identity。 这让我们可以将条形的高度映射到 y 变量的原始值。\n\ndiamonds |>\n  count(cut) |>\n  ggplot(aes(x = cut, y = n)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n您可能想要覆盖从转换变量到美学的默认映射。 例如，您可能想要显示比例条形图，而不是计数：\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop), group = 1)) + \n  geom_bar()\n\n\n\n\n要查找可由 stat 计算的可能变量，请在 geom_bar() 的帮助中查找标题为 “computed variables” 的部分。\n\n\n您可能希望更加关注代码中的统计转换。 例如，您可以使用 stat_summary()，它总结每个唯一 x 值的 y 值，以引起人们对您正在计算的 summary 的注意：\n\nggplot(diamonds) + \n  stat_summary(\n    aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\n\n\n\n\n\n\nggplot2 提供了 20 多种 stats 供您使用。 每个 stat 都是一个函数，因此您可以通过通常的方式获得帮助，例如 ?stat_bin。\n\n10.5.1 Exercises\n\n与 stat_summary() 关联的默认 geom 是什么？ 如何重写前面的绘图以使用 geom 函数而不是 stat 函数？\ngeom_col() 的作用是什么？ 它与 geom_bar() 有什么不同？\n大多数 geoms 和 stats 都是成对出现的，几乎总是一起使用。 列出所有配对的列表。 他们有什么共同点？ （提示：通读文档。）\nstat_smooth() 计算哪些变量？ 什么参数控制其行为？\n\n在我们的比例条形图中，我们需要设置 group = 1。 为什么呢？换 句话说，这两张图有什么问题呢？\n\nggplot(diamonds, aes(x = cut, y = after_stat(prop))) + \n  geom_bar()\nggplot(diamonds, aes(x = cut, fill = color, y = after_stat(prop))) + \n  geom_bar()"
  },
  {
    "objectID": "layers.html#position-adjustments",
    "href": "layers.html#position-adjustments",
    "title": "10  Layers",
    "section": "\n10.6 Position adjustments",
    "text": "10.6 Position adjustments\n条形图还有一个神奇之处。 您可以使用 color aesthetic 或更有用的 fill aesthetic 来为条形图着色：\n\n# Left\nggplot(mpg, aes(x = drv, color = drv)) + \n  geom_bar()\n\n# Right\nggplot(mpg, aes(x = drv, fill = drv)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n请注意，如果将 fill aesthetic 映射到另一个变量（例如 class），会发生什么：条形图会自动堆叠（stacked）。 每个彩色矩形代表 drv 和 class 的组合。\n\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar()\n\n\n\n\n使用 position 参数指定的位置调整（position adjustment）自动执行堆叠。 如果您不需要堆叠条形图，可以使用其他三个选项之一：\"identity\", \"dodge\" or \"fill\"。\n\n\nposition = \"identity\" 将把每个对象准确地放置在图表上下文中的位置。 这对于条形图来说不是很有用，因为它会导致重叠。 要看到重叠，我们需要通过将 alpha 设置为较小的值来使条形稍微透明，或者通过设置 fill = NA 使条形完全透明。\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\n\n# Right\nggplot(mpg, aes(x = drv, color = class)) + \n  geom_bar(fill = NA, position = \"identity\")\n\n\n\n\n\n\n\n\n\n\n\nidentity position 调整对于 2d geoms 更有用，例如 points，它是默认值。\n\nposition = \"fill\" 的工作方式类似于堆叠（stacking），但使每组堆叠的条形高度相同。 这使得比较不同组之间的比例变得更加容易。\n\nposition = \"dodge\" 将重叠的对象直接放在一起。 这使得比较各个值变得更容易。\n\n# Left\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"fill\")\n\n# Right\nggplot(mpg, aes(x = drv, fill = class)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n还有另一种类型的调整对于条形图没有用，但对于散点图非常有用。 回想一下我们的第一个散点图。 您是否注意到该图仅显示 126 个点，即使数据集中有 234 个观测值？\n\n\n\n\n\nhwy 和 displ 的基础值是四舍五入的，因此这些点出现在网格上，并且许多点彼此重叠。 此问题称为过度绘图（overplotting）。这 种安排使得很难看到数据的分布。 数据点是否均匀分布在整个图表中，或者是否存在包含 109 个值的 hwy 和 displ 的一种特殊组合？\n您可以通过将位置调整设置为 “jitter” 来避免这种网格化。 position = \"jitter\" 为每个点添加少量随机噪声。 这会将点分散开，因为没有两个点可能接收到相同数量的随机噪声。\n\nggplot(mpg, aes(x = displ, y = hwy)) + \n  geom_point(position = \"jitter\")\n\n\n\n\n添加随机性似乎是改善绘图的一种奇怪方法，但虽然它会使您的图表在小尺度上不太准确，但它会使您的图表在大尺度上更具启发性。 因为这是一个非常有用的操作，所以 ggplot2 附带了 geom_point(position = \"jitter\") 的简写形式：geom_jitter()。\n要了解有关 position adjustment 的更多信息，请查找与每个调整相关的帮助页面：?position_dodge, ?position_fill, ?position_identity, ?position_jitter, and ?position_stack。\n\n10.6.1 Exercises\n\n\n下面的绘图有什么问题吗？ 你可以如何改进它？\n\nggplot(mpg, aes(x = cty, y = hwy)) + \n  geom_point()\n\n\n\n两个图之间有什么区别（如果有的话）？为 什么？\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(position = \"identity\")\n\n\ngeom_jitter() 的哪些参数控制抖动量？\n将 geom_jitter() 与 geom_count() 进行比较和对比。\ngeom_boxplot() 的默认位置调整是多少？ 创建 mpg 数据集的可视化来演示它。"
  },
  {
    "objectID": "layers.html#coordinate-systems",
    "href": "layers.html#coordinate-systems",
    "title": "10  Layers",
    "section": "\n10.7 Coordinate systems",
    "text": "10.7 Coordinate systems\n坐标系统（Coordinate systems）可能是 ggplot2 中最复杂的部分。 默认坐标系是笛卡尔坐标系，其中 x 和 y 位置独立作用以确定每个点的位置。 还有另外两个坐标系偶尔会有帮助。\n\n\ncoord_quickmap() 正确设置地理地图的纵横比。 如果您使用 ggplot2 绘制空间数据，这一点非常重要。 本书没有足够的篇幅来讨论地图，但您可以在 ggplot2: Elegant graphics for data analysis 的 Maps chapter 了解更多信息。\n\nnz <- map_data(\"nz\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\")\n\nggplot(nz, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", color = \"black\") +\n  coord_quickmap()\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoord_polar() 使用极坐标。 极坐标揭示了 bar chart 和 Coxcomb chart 之间的有趣联系。\n\nbar <- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = clarity, fill = clarity), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1)\n\nbar + coord_flip()\nbar + coord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.7.1 Exercises\n\n使用 coord_polar() 将 stacked bar chart 转换为 pie chart。\ncoord_quickmap() 和 coord_map() 有什么区别？\n\n下图告诉您什么关于城市和高速公路 mpg 之间的关系？ 为什么 coord_fixed() 很重要？ geom_abline() 的作用是什么？\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()"
  },
  {
    "objectID": "layers.html#the-layered-grammar-of-graphics",
    "href": "layers.html#the-layered-grammar-of-graphics",
    "title": "10  Layers",
    "section": "\n10.8 The layered grammar of graphics",
    "text": "10.8 The layered grammar of graphics\n我们可以通过添加 position adjustments, stats, coordinate systems, and faceting 来扩展您在 ?sec-ggplot2-calls 中学到的图形模板：\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(\n     mapping = aes(<MAPPINGS>),\n     stat = <STAT>, \n     position = <POSITION>\n  ) +\n  <COORDINATE_FUNCTION> +\n  <FACET_FUNCTION>\n我们的新模板采用七个参数，即模板中出现的括号内的单词。 在实践中，您很少需要提供所有七个参数来制作图表，因为 ggplot2 将为除 data、mappings、 、geom 函数之外的所有内容提供有用的默认值。\n模板中的七个参数组成了图形语法，这是一个用于构建绘图的正式系统。 图形语法基于这样的见解：您可以将任何绘图独特地描述为 a dataset、a geom、a set of mappings、a stat、a position adjustment、a coordinate system、a faceting scheme、a theme 的组合。\n要了解其工作原理，请考虑如何从头开始构建基本图：您可以从 dataset 开始，然后将其转换为您想要显示的信息（with a stat）。 接下来，您可以选择一个 geometric object 来表示转换数据中的每个观测值。 然后，您可以使用 geoms 的 aesthetic properties 来表示数据中的变量。 您可以将每个变量的值 map 到 aesthetic 水平。 这些步骤如 Figure 10.3 所示。 然后，您可以选择一个 coordinate system 来放置 geoms，使用对象的位置（其本身就是一种美学属性）来显示 x 和 y 变量的值。\n\n\n\n\nFigure 10.3: 从原始数据到频率表再到条形图的步骤，其中条形的高度代表频率。\n\n\n\n\n此时，您将拥有一个完整的图形，但您可以进一步调整坐标系内 geoms 的位置（a position adjustment）或将图形拆分为子图（faceting）。 您还可以通过添加一个或多个附加图层来扩展绘图，其中每个附加图层都使用 a dataset, a geom, a set of mappings, a stat, and a position adjustment。\n您可以使用此方法来构建您想象的任何绘图。 换句话说，您可以使用本章中学到的代码模板来构建数十万个独特的绘图。\n如果您想了解更多关于 ggplot2 的理论基础，您可能会喜欢阅读 “The Layered Grammar of Graphics”，这是一篇详细描述 ggplot2 理论的科学论文。"
  },
  {
    "objectID": "layers.html#summary",
    "href": "layers.html#summary",
    "title": "10  Layers",
    "section": "\n10.9 Summary",
    "text": "10.9 Summary\n在本章中，您学习了图形的分层语法，从 aesthetics 和 geometries 开始构建简单的绘图，将绘图 facets 成子集，了解如何计算 geoms 的 statistics，在 geoms 可能发生变化时控制位置细节以避免重叠的 position adjustments。 coordinate systems 允许您从根本上改变 x 和 y 的含义。 我们还没有触及的一层是 theme，我们将在 ?sec-themes 中介绍它。\nggplot2 cheatsheet (which you can find at https://posit.co/resources/cheatsheets) 和 ggplot2 package website (https://ggplot2.tidyverse.org) 是了解完整 ggplot2 功能的两个非常有用的资源。\n从本章中你应该学到的一个重要教训是，当你觉得需要 ggplot2 未提供的 geom 时，最好看看其他人是否已经通过创建一个 ggplot2 扩展包来解决你的问题，该扩展包提供了那个 geom。"
  },
  {
    "objectID": "transform.html",
    "href": "transform.html",
    "title": "Transform",
    "section": "",
    "text": "The second part of the book was a deep dive into data visualization. In this part of the book, you’ll learn about the most important types of variables that you’ll encounter inside a data frame and learn the tools you can use to work with them.\n\n\n\n\nFigure 1: The options for data transformation depends heavily on the type of data involved, the subject of this part of the book.\n\n\n\n\nYou can read these chapters as you need them; they’re designed to be largely standalone so that they can be read out of order.\n\nChapter 13 teaches you about logical vectors. These are the simplest types of vectors, but are extremely powerful. You’ll learn how to create them with numeric comparisons, how to combine them with Boolean algebra, how to use them in summaries, and how to use them for condition transformations.\nChapter 14 dives into tools for vectors of numbers, the powerhouse of data science. You’ll learn more about counting and a bunch of important transformation and summary functions.\nChapter 15 will give you the tools to work with strings: you’ll slice them, you’ll dice them, and you’ll stick them back together again. This chapter mostly focuses on the stringr package, but you’ll also learn some more tidyr functions devoted to extracting data from character strings.\nChapter 16 introduces you to regular expressions, a powerful tool for manipulating strings. This chapter will take you from thinking that a cat walked over your keyboard to reading and writing complex string patterns.\nChapter 17 introduces factors: the data type that R uses to store categorical data. You use a factor when variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string.\n?sec-dates-and-times will give you the key tools for working with dates and date-times. Unfortunately, the more you learn about date-times, the more complicated they seem to get, but with the help of the lubridate package, you’ll learn to how to overcome the most common challenges.\nChapter 19 discusses missing values in depth. We’ve discussed them a couple of times in isolation, but now it’s time to discuss them holistically, helping you come to grips with the difference between implicit and explicit missing values, and how and why you might convert between them.\nChapter 20 finishes up this part of the book by giving you tools to join two (or more) data frames together. Learning about joins will force you to grapple with the idea of keys, and think about how you identify each row in a dataset."
  },
  {
    "objectID": "logicals.html#introduction",
    "href": "logicals.html#introduction",
    "title": "13  Logical vectors",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nIn this chapter, you’ll learn tools for working with logical vectors. Logical vectors are the simplest type of vector because each element can only be one of three possible values: TRUE, FALSE, and NA. It’s relatively rare to find logical vectors in your raw data, but you’ll create and manipulate them in the course of almost every analysis.\nWe’ll begin by discussing the most common way of creating logical vectors: with numeric comparisons. Then you’ll learn about how you can use Boolean algebra to combine different logical vectors, as well as some useful summaries. We’ll finish off with if_else() and case_when(), two useful functions for making conditional changes powered by logical vectors.\n\n13.1.1 Prerequisites\nMost of the functions you’ll learn about in this chapter are provided by base R, so we don’t need the tidyverse, but we’ll still load it so we can use mutate(), filter(), and friends to work with data frames. We’ll also continue to draw examples from the nycflights13::flights dataset.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3\n\nHowever, as we start to cover more tools, there won’t always be a perfect real example. So we’ll start making up some dummy data with c():\n\nx <- c(1, 2, 3, 5, 7, 11, 13)\nx * 2\n#> [1]  2  4  6 10 14 22 26\n\nThis makes it easier to explain individual functions at the cost of making it harder to see how it might apply to your data problems. Just remember that any manipulation we do to a free-floating vector, you can do to a variable inside a data frame with mutate() and friends.\n\ndf <- tibble(x)\ndf |> \n  mutate(y = x *  2)\n#> # A tibble: 7 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     2\n#> 2     2     4\n#> 3     3     6\n#> 4     5    10\n#> 5     7    14\n#> 6    11    22\n#> # ℹ 1 more row"
  },
  {
    "objectID": "logicals.html#comparisons",
    "href": "logicals.html#comparisons",
    "title": "13  Logical vectors",
    "section": "\n13.2 Comparisons",
    "text": "13.2 Comparisons\nA very common way to create a logical vector is via a numeric comparison with <, <=, >, >=, !=, and ==. So far, we’ve mostly created logical variables transiently within filter() — they are computed, used, and then thrown away. For example, the following filter finds all daytime departures that arrive roughly on time:\n\nflights |> \n  filter(dep_time > 600 & dep_time < 2000 & abs(arr_delay) < 20)\n#> # A tibble: 172,286 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      601            600         1      844            850\n#> 2  2013     1     1      602            610        -8      812            820\n#> 3  2013     1     1      602            605        -3      821            805\n#> 4  2013     1     1      606            610        -4      858            910\n#> 5  2013     1     1      606            610        -4      837            845\n#> 6  2013     1     1      607            607         0      858            915\n#> # ℹ 172,280 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nIt’s useful to know that this is a shortcut and you can explicitly create the underlying logical variables with mutate():\n\nflights |> \n  mutate(\n    daytime = dep_time > 600 & dep_time < 2000,\n    approx_ontime = abs(arr_delay) < 20,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 4\n#>   dep_time arr_delay daytime approx_ontime\n#>      <int>     <dbl> <lgl>   <lgl>        \n#> 1      517        11 FALSE   TRUE         \n#> 2      533        20 FALSE   FALSE        \n#> 3      542        33 FALSE   FALSE        \n#> 4      544       -18 FALSE   TRUE         \n#> 5      554       -25 FALSE   FALSE        \n#> 6      554        12 FALSE   TRUE         \n#> # ℹ 336,770 more rows\n\nThis is particularly useful for more complicated logic because naming the intermediate steps makes it easier to both read your code and check that each step has been computed correctly.\nAll up, the initial filter is equivalent to:\n\nflights |> \n  mutate(\n    daytime = dep_time > 600 & dep_time < 2000,\n    approx_ontime = abs(arr_delay) < 20,\n  ) |> \n  filter(daytime & approx_ontime)\n\n\n13.2.1 Floating point comparison\nBeware of using == with numbers. For example, it looks like this vector contains the numbers 1 and 2:\n\nx <- c(1 / 49 * 49, sqrt(2) ^ 2)\nx\n#> [1] 1 2\n\nBut if you test them for equality, you get FALSE:\n\nx == c(1, 2)\n#> [1] FALSE FALSE\n\nWhat’s going on? Computers store numbers with a fixed number of decimal places so there’s no way to exactly represent 1/49 or sqrt(2) and subsequent computations will be very slightly off. We can see the exact values by calling print() with the digits1 argument:\n\nprint(x, digits = 16)\n#> [1] 0.9999999999999999 2.0000000000000004\n\nYou can see why R defaults to rounding these numbers; they really are very close to what you expect.\nNow that you’ve seen why == is failing, what can you do about it? One option is to use dplyr::near() which ignores small differences:\n\nnear(x, c(1, 2))\n#> [1] TRUE TRUE\n\n\n13.2.2 Missing values\nMissing values represent the unknown so they are “contagious”: almost any operation involving an unknown value will also be unknown:\n\nNA > 5\n#> [1] NA\n10 == NA\n#> [1] NA\n\nThe most confusing result is this one:\n\nNA == NA\n#> [1] NA\n\nIt’s easiest to understand why this is true if we artificially supply a little more context:\n\n# We don't know how old Mary is\nage_mary <- NA\n\n# We don't know how old John is\nage_john <- NA\n\n# Are Mary and John the same age?\nage_mary == age_john\n#> [1] NA\n# We don't know!\n\nSo if you want to find all flights where dep_time is missing, the following code doesn’t work because dep_time == NA will yield NA for every single row, and filter() automatically drops missing values:\n\nflights |> \n  filter(dep_time == NA)\n#> # A tibble: 0 × 19\n#> # ℹ 19 variables: year <int>, month <int>, day <int>, dep_time <int>,\n#> #   sched_dep_time <int>, dep_delay <dbl>, arr_time <int>, …\n\nInstead we’ll need a new tool: is.na().\n\n13.2.3 is.na()\n\nis.na(x) works with any type of vector and returns TRUE for missing values and FALSE for everything else:\n\nis.na(c(TRUE, NA, FALSE))\n#> [1] FALSE  TRUE FALSE\nis.na(c(1, NA, 3))\n#> [1] FALSE  TRUE FALSE\nis.na(c(\"a\", NA, \"b\"))\n#> [1] FALSE  TRUE FALSE\n\nWe can use is.na() to find all the rows with a missing dep_time:\n\nflights |> \n  filter(is.na(dep_time))\n#> # A tibble: 8,255 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1       NA           1630        NA       NA           1815\n#> 2  2013     1     1       NA           1935        NA       NA           2240\n#> 3  2013     1     1       NA           1500        NA       NA           1825\n#> 4  2013     1     1       NA            600        NA       NA            901\n#> 5  2013     1     2       NA           1540        NA       NA           1747\n#> 6  2013     1     2       NA           1620        NA       NA           1746\n#> # ℹ 8,249 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nis.na() can also be useful in arrange(). arrange() usually puts all the missing values at the end but you can override this default by first sorting by is.na():\n\nflights |> \n  filter(month == 1, day == 1) |> \n  arrange(dep_time)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 836 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nflights |> \n  filter(month == 1, day == 1) |> \n  arrange(desc(is.na(dep_time)), dep_time)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1       NA           1630        NA       NA           1815\n#> 2  2013     1     1       NA           1935        NA       NA           2240\n#> 3  2013     1     1       NA           1500        NA       NA           1825\n#> 4  2013     1     1       NA            600        NA       NA            901\n#> 5  2013     1     1      517            515         2      830            819\n#> 6  2013     1     1      533            529         4      850            830\n#> # ℹ 836 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nWe’ll come back to cover missing values in more depth in Chapter 19.\n\n13.2.4 Exercises\n\nHow does dplyr::near() work? Type near to see the source code. Is sqrt(2)^2 near 2?\nUse mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected."
  },
  {
    "objectID": "logicals.html#boolean-algebra",
    "href": "logicals.html#boolean-algebra",
    "title": "13  Logical vectors",
    "section": "\n13.3 Boolean algebra",
    "text": "13.3 Boolean algebra\nOnce you have multiple logical vectors, you can combine them together using Boolean algebra. In R, & is “and”, | is “or”, ! is “not”, and xor() is exclusive or2. For example, df |> filter(!is.na(x)) finds all rows where x is not missing and df |> filter(x < -10 | x > 0) finds all rows where x is smaller than -10 or bigger than 0. Figure 13.1 shows the complete set of Boolean operations and how they work.\n\n\n\n\nFigure 13.1: The complete set of Boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects.\n\n\n\n\nAs well as & and |, R also has && and ||. Don’t use them in dplyr functions! These are called short-circuiting operators and only ever return a single TRUE or FALSE. They’re important for programming, not data science.\n\n13.3.1 Missing values\nThe rules for missing values in Boolean algebra are a little tricky to explain because they seem inconsistent at first glance:\n\ndf <- tibble(x = c(TRUE, FALSE, NA))\n\ndf |> \n  mutate(\n    and = x & NA,\n    or = x | NA\n  )\n#> # A tibble: 3 × 3\n#>   x     and   or   \n#>   <lgl> <lgl> <lgl>\n#> 1 TRUE  NA    TRUE \n#> 2 FALSE FALSE NA   \n#> 3 NA    NA    NA\n\nTo understand what’s going on, think about NA | TRUE (NA or TRUE). A missing value in a logical vector means that the value could either be TRUE or FALSE. TRUE | TRUE and FALSE | TRUE are both TRUE because at least one of them is TRUE. NA | TRUE must also be TRUE because NA can either be TRUE or FALSE. However, NA | FALSE is NA because we don’t know if NA is TRUE or FALSE. Similar reasoning applies with NA & FALSE.\n\n13.3.2 Order of operations\nNote that the order of operations doesn’t work like English. Take the following code that finds all flights that departed in November or December:\n\nflights |> \n   filter(month == 11 | month == 12)\n\nYou might be tempted to write it like you’d say in English: “Find all flights that departed in November or December.”:\n\nflights |> \n   filter(month == 11 | 12)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nThis code doesn’t error but it also doesn’t seem to have worked. What’s going on? Here, R first evaluates month == 11 creating a logical vector, which we call nov. It computes nov | 12. When you use a number with a logical operator it converts everything apart from 0 to TRUE, so this is equivalent to nov | TRUE which will always be TRUE, so every row will be selected:\n\nflights |> \n  mutate(\n    nov = month == 11,\n    final = nov | 12,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 3\n#>   month nov   final\n#>   <int> <lgl> <lgl>\n#> 1     1 FALSE TRUE \n#> 2     1 FALSE TRUE \n#> 3     1 FALSE TRUE \n#> 4     1 FALSE TRUE \n#> 5     1 FALSE TRUE \n#> 6     1 FALSE TRUE \n#> # ℹ 336,770 more rows\n\n\n13.3.3 %in%\n\nAn easy way to avoid the problem of getting your ==s and |s in the right order is to use %in%. x %in% y returns a logical vector the same length as x that is TRUE whenever a value in x is anywhere in y .\n\n1:12 %in% c(1, 5, 11)\n#>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\nletters[1:10] %in% c(\"a\", \"e\", \"i\", \"o\", \"u\")\n#>  [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n\nSo to find all flights in November and December we could write:\n\nflights |> \n  filter(month %in% c(11, 12))\n\nNote that %in% obeys different rules for NA to ==, as NA %in% NA is TRUE.\n\nc(1, 2, NA) == NA\n#> [1] NA NA NA\nc(1, 2, NA) %in% NA\n#> [1] FALSE FALSE  TRUE\n\nThis can make for a useful shortcut:\n\nflights |> \n  filter(dep_time %in% c(NA, 0800))\n#> # A tibble: 8,803 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      800            800         0     1022           1014\n#> 2  2013     1     1      800            810       -10      949            955\n#> 3  2013     1     1       NA           1630        NA       NA           1815\n#> 4  2013     1     1       NA           1935        NA       NA           2240\n#> 5  2013     1     1       NA           1500        NA       NA           1825\n#> 6  2013     1     1       NA            600        NA       NA            901\n#> # ℹ 8,797 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n\n13.3.4 Exercises\n\nFind all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.\nHow many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?\nAssuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?"
  },
  {
    "objectID": "logicals.html#sec-logical-summaries",
    "href": "logicals.html#sec-logical-summaries",
    "title": "13  Logical vectors",
    "section": "\n13.4 Summaries",
    "text": "13.4 Summaries\nThe following sections describe some useful techniques for summarizing logical vectors. As well as functions that only work specifically with logical vectors, you can also use functions that work with numeric vectors.\n\n13.4.1 Logical summaries\nThere are two main logical summaries: any() and all(). any(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x. all(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s. Like all summary functions, they’ll return NA if there are any missing values present, and as usual you can make the missing values go away with na.rm = TRUE.\nFor example, we could use all() and any() to find out if every flight was delayed on departure by at most an hour or if any flights were delayed on arrival by five hours or more. And using group_by() allows us to do that by day:\n\nflights |> \n  group_by(year, month, day) |> \n  summarize(\n    all_delayed = all(dep_delay <= 60, na.rm = TRUE),\n    any_long_delay = any(arr_delay >= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day all_delayed any_long_delay\n#>   <int> <int> <int> <lgl>       <lgl>         \n#> 1  2013     1     1 FALSE       TRUE          \n#> 2  2013     1     2 FALSE       TRUE          \n#> 3  2013     1     3 FALSE       FALSE         \n#> 4  2013     1     4 FALSE       FALSE         \n#> 5  2013     1     5 FALSE       TRUE          \n#> 6  2013     1     6 FALSE       FALSE         \n#> # ℹ 359 more rows\n\nIn most cases, however, any() and all() are a little too crude, and it would be nice to be able to get a little more detail about how many values are TRUE or FALSE. That leads us to the numeric summaries.\n\n13.4.2 Numeric summaries of logical vectors\nWhen you use a logical vector in a numeric context, TRUE becomes 1 and FALSE becomes 0. This makes sum() and mean() very useful with logical vectors because sum(x) gives the number of TRUEs and mean(x) gives the proportion of TRUEs (because mean() is just sum() divided by length().\nThat, for example, allows us to see the proportion of flights that were delayed on departure by at most an hour and the number of flights that were delayed on arrival by five hours or more:\n\nflights |> \n  group_by(year, month, day) |> \n  summarize(\n    all_delayed = mean(dep_delay <= 60, na.rm = TRUE),\n    any_long_delay = sum(arr_delay >= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day all_delayed any_long_delay\n#>   <int> <int> <int>       <dbl>          <int>\n#> 1  2013     1     1       0.939              3\n#> 2  2013     1     2       0.914              3\n#> 3  2013     1     3       0.941              0\n#> 4  2013     1     4       0.953              0\n#> 5  2013     1     5       0.964              1\n#> 6  2013     1     6       0.959              0\n#> # ℹ 359 more rows\n\n\n13.4.3 Logical subsetting\nThere’s one final use for logical vectors in summaries: you can use a logical vector to filter a single variable to a subset of interest. This makes use of the base [ (pronounced subset) operator, which you’ll learn more about in ?sec-subset-many.\nImagine we wanted to look at the average delay just for flights that were actually delayed. One way to do so would be to first filter the flights and then calculate the average delay:\n\nflights |> \n  filter(arr_delay > 0) |> \n  group_by(year, month, day) |> \n  summarize(\n    behind = mean(arr_delay),\n    n = n(),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day behind     n\n#>   <int> <int> <int>  <dbl> <int>\n#> 1  2013     1     1   32.5   461\n#> 2  2013     1     2   32.0   535\n#> 3  2013     1     3   27.7   460\n#> 4  2013     1     4   28.3   297\n#> 5  2013     1     5   22.6   238\n#> 6  2013     1     6   24.4   381\n#> # ℹ 359 more rows\n\nThis works, but what if we wanted to also compute the average delay for flights that arrived early? We’d need to perform a separate filter step, and then figure out how to combine the two data frames together3. Instead you could use [ to perform an inline filtering: arr_delay[arr_delay > 0] will yield only the positive arrival delays.\nThis leads to:\n\nflights |> \n  group_by(year, month, day) |> \n  summarize(\n    behind = mean(arr_delay[arr_delay > 0], na.rm = TRUE),\n    ahead = mean(arr_delay[arr_delay < 0], na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 6\n#>    year month   day behind ahead     n\n#>   <int> <int> <int>  <dbl> <dbl> <int>\n#> 1  2013     1     1   32.5 -12.5   842\n#> 2  2013     1     2   32.0 -14.3   943\n#> 3  2013     1     3   27.7 -18.2   914\n#> 4  2013     1     4   28.3 -17.0   915\n#> 5  2013     1     5   22.6 -14.0   720\n#> 6  2013     1     6   24.4 -13.6   832\n#> # ℹ 359 more rows\n\nAlso note the difference in the group size: in the first chunk n() gives the number of delayed flights per day; in the second, n() gives the total number of flights.\n\n13.4.4 Exercises\n\nWhat will sum(is.na(x)) tell you? How about mean(is.na(x))?\nWhat does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return when applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments."
  },
  {
    "objectID": "logicals.html#conditional-transformations",
    "href": "logicals.html#conditional-transformations",
    "title": "13  Logical vectors",
    "section": "\n13.5 Conditional transformations",
    "text": "13.5 Conditional transformations\nOne of the most powerful features of logical vectors are their use for conditional transformations, i.e. doing one thing for condition x, and something different for condition y. There are two important tools for this: if_else() and case_when().\n\n13.5.1 if_else()\n\nIf you want to use one value when a condition is TRUE and another value when it’s FALSE, you can use dplyr::if_else()4. You’ll always use the first three argument of if_else(). The first argument, condition, is a logical vector, the second, true, gives the output when the condition is true, and the third, false, gives the output if the condition is false.\nLet’s begin with a simple example of labeling a numeric vector as either “+ve” (positive) or “-ve” (negative):\n\nx <- c(-3:3, NA)\nif_else(x > 0, \"+ve\", \"-ve\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" NA\n\nThere’s an optional fourth argument, missing which will be used if the input is NA:\n\nif_else(x > 0, \"+ve\", \"-ve\", \"???\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"-ve\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nYou can also use vectors for the the true and false arguments. For example, this allows us to create a minimal implementation of abs():\n\nif_else(x < 0, -x, x)\n#> [1]  3  2  1  0  1  2  3 NA\n\nSo far all the arguments have used the same vectors, but you can of course mix and match. For example, you could implement a simple version of coalesce() like this:\n\nx1 <- c(NA, 1, 2, NA)\ny1 <- c(3, NA, 4, 6)\nif_else(is.na(x1), y1, x1)\n#> [1] 3 1 2 6\n\nYou might have noticed a small infelicity in our labeling example above: zero is neither positive nor negative. We could resolve this by adding an additional if_else():\n\nif_else(x == 0, \"0\", if_else(x < 0, \"-ve\", \"+ve\"), \"???\")\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is already a little hard to read, and you can imagine it would only get harder if you have more conditions. Instead, you can switch to dplyr::case_when().\n\n13.5.2 case_when()\n\ndplyr’s case_when() is inspired by SQL’s CASE statement and provides a flexible way of performing different computations for different conditions. It has a special syntax that unfortunately looks like nothing else you’ll use in the tidyverse. It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.\nThis means we could recreate our previous nested if_else() as follows:\n\nx <- c(-3:3, NA)\ncase_when(\n  x == 0   ~ \"0\",\n  x < 0    ~ \"-ve\", \n  x > 0    ~ \"+ve\",\n  is.na(x) ~ \"???\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"0\"   \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nThis is more code, but it’s also more explicit.\nTo explain how case_when() works, let’s explore some simpler cases. If none of the cases match, the output gets an NA:\n\ncase_when(\n  x < 0 ~ \"-ve\",\n  x > 0 ~ \"+ve\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nIf you want to create a “default”/catch all value, use TRUE on the left hand side:\n\ncase_when(\n  x < 0 ~ \"-ve\",\n  x > 0 ~ \"+ve\",\n  TRUE ~ \"???\"\n)\n#> [1] \"-ve\" \"-ve\" \"-ve\" \"???\" \"+ve\" \"+ve\" \"+ve\" \"???\"\n\nAnd note that if multiple conditions match, only the first will be used:\n\ncase_when(\n  x > 0 ~ \"+ve\",\n  x > 2 ~ \"big\"\n)\n#> [1] NA    NA    NA    NA    \"+ve\" \"+ve\" \"+ve\" NA\n\nJust like with if_else() you can use variables on both sides of the ~ and you can mix and match variables as needed for your problem. For example, we could use case_when() to provide some human readable labels for the arrival delay:\n\nflights |> \n  mutate(\n    status = case_when(\n      is.na(arr_delay)      ~ \"cancelled\",\n      arr_delay < -30       ~ \"very early\",\n      arr_delay < -15       ~ \"early\",\n      abs(arr_delay) <= 15  ~ \"on time\",\n      arr_delay < 60        ~ \"late\",\n      arr_delay < Inf       ~ \"very late\",\n    ),\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 2\n#>   arr_delay status \n#>       <dbl> <chr>  \n#> 1        11 on time\n#> 2        20 late   \n#> 3        33 late   \n#> 4       -18 early  \n#> 5       -25 early  \n#> 6        12 on time\n#> # ℹ 336,770 more rows\n\nBe wary when writing this sort of complex case_when() statement; my first two attempts used a mix of < and > and I kept accidentally creating overlapping conditions.\n\n13.5.3 Compatible types\nNote that both if_else() and case_when() require compatible types in the output. If they’re not compatible, you’ll see errors like this:\n\nif_else(TRUE, \"a\", 1)\n#> Error in `if_else()`:\n#> ! Can't combine `true` <character> and `false` <double>.\n\ncase_when(\n  x < -1 ~ TRUE,  \n  x > 0  ~ now()\n)\n#> Error in `case_when()`:\n#> ! Can't combine `..1 (right)` <logical> and `..2 (right)` <datetime<local>>.\n\nOverall, relatively few types are compatible, because automatically converting one type of vector to another is a common source of errors. Here are the most important cases that are compatible:\n\nNumeric and logical vectors are compatible, as we discussed in Section 13.4.2.\nStrings and factors (Chapter 17) are compatible, because you can think of a factor as a string with a restricted set of values.\nDates and date-times, which we’ll discuss in ?sec-dates-and-times, are compatible because you can think of a date as a special case of date-time.\n\nNA, which is technically a logical vector, is compatible with everything because every vector has some way of representing a missing value.\n\nWe don’t expect you to memorize these rules, but they should become second nature over time because they are applied consistently throughout the tidyverse.\n\n13.5.4 Exercises\n\nA number is even if it’s divisible by two, which in R you can find out with x %% 2 == 0. Use this fact and if_else() to determine whether each number between 0 and 20 is even or odd.\nGiven a vector of days like x <- c(\"Monday\", \"Saturday\", \"Wednesday\"), use an ifelse() statement to label them as weekends or weekdays.\nUse ifelse() to compute the absolute value of a numeric vector called x.\nWrite a case_when() statement that uses the month and day columns from flights to label a selection of important US holidays (e.g., New Years Day, 4th of July, Thanksgiving, and Christmas). First create a logical column that is either TRUE or FALSE, and then create a character column that either gives the name of the holiday or is NA."
  },
  {
    "objectID": "logicals.html#summary",
    "href": "logicals.html#summary",
    "title": "13  Logical vectors",
    "section": "\n13.6 Summary",
    "text": "13.6 Summary\nThe definition of a logical vector is simple because each value must be either TRUE, FALSE, or NA. But logical vectors provide a huge amount of power. In this chapter, you learned how to create logical vectors with >, <, <=, =>, ==, !=, and is.na(), how to combine them with !, &, and |, and how to summarize them with any(), all(), sum(), and mean(). You also learned the powerful if_else() and case_when() functions that allow you to return values depending on the value of a logical vector.\nWe’ll see logical vectors again and again in the following chapters. For example in Chapter 15 you’ll learn about str_detect(x, pattern) which returns a logical vector that’s TRUE for the elements of x that match the pattern, and in ?sec-dates-and-times you’ll create logical vectors from the comparison of dates and times. But for now, we’re going to move onto the next most important type of vector: numeric vectors."
  },
  {
    "objectID": "numbers.html#introduction",
    "href": "numbers.html#introduction",
    "title": "14  Numbers",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nNumeric vectors are the backbone of data science, and you’ve already used them a bunch of times earlier in the book. Now it’s time to systematically survey what you can do with them in R, ensuring that you’re well situated to tackle any future problem involving numeric vectors.\nWe’ll start by giving you a couple of tools to make numbers if you have strings, and then going into a little more detail of count(). Then we’ll dive into various numeric transformations that pair well with mutate(), including more general transformations that can be applied to other types of vectors, but are often used with numeric vectors. We’ll finish off by covering the summary functions that pair well with summarize() and show you how they can also be used with mutate().\n\n14.1.1 Prerequisites\nThis chapter mostly uses functions from base R, which are available without loading any packages. But we still need the tidyverse because we’ll use these base R functions inside of tidyverse functions like mutate() and filter(). Like in the last chapter, we’ll use real examples from nycflights13, as well as toy examples made with c() and tribble().\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3"
  },
  {
    "objectID": "numbers.html#making-numbers",
    "href": "numbers.html#making-numbers",
    "title": "14  Numbers",
    "section": "\n14.2 Making numbers",
    "text": "14.2 Making numbers\nIn most cases, you’ll get numbers already recorded in one of R’s numeric types: integer or double. In some cases, however, you’ll encounter them as strings, possibly because you’ve created them by pivoting from column headers or because something has gone wrong in your data import process.\nreadr provides two useful functions for parsing strings into numbers: parse_double() and parse_number(). Use parse_double() when you have numbers that have been written as strings:\n\nx <- c(\"1.2\", \"5.6\", \"1e3\")\nparse_double(x)\n#> [1]    1.2    5.6 1000.0\n\nUse parse_number() when the string contains non-numeric text that you want to ignore. This is particularly useful for currency data and percentages:\n\nx <- c(\"$1,234\", \"USD 3,513\", \"59%\")\nparse_number(x)\n#> [1] 1234 3513   59"
  },
  {
    "objectID": "numbers.html#sec-counts",
    "href": "numbers.html#sec-counts",
    "title": "14  Numbers",
    "section": "\n14.3 Counts",
    "text": "14.3 Counts\nIt’s surprising how much data science you can do with just counts and a little basic arithmetic, so dplyr strives to make counting as easy as possible with count(). This function is great for quick exploration and checks during analysis:\n\nflights |> count(dest)\n#> # A tibble: 105 × 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ABQ     254\n#> 2 ACK     265\n#> 3 ALB     439\n#> 4 ANC       8\n#> 5 ATL   17215\n#> 6 AUS    2439\n#> # ℹ 99 more rows\n\n(Despite the advice in Chapter 5, we usually put count() on a single line because it’s usually used at the console for a quick check that a calculation is working as expected.)\nIf you want to see the most common values, add sort = TRUE:\n\nflights |> count(dest, sort = TRUE)\n#> # A tibble: 105 × 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ORD   17283\n#> 2 ATL   17215\n#> 3 LAX   16174\n#> 4 BOS   15508\n#> 5 MCO   14082\n#> 6 CLT   14064\n#> # ℹ 99 more rows\n\nAnd remember that if you want to see all the values, you can use |> View() or |> print(n = Inf).\nYou can perform the same computation “by hand” with group_by(), summarize() and n(). This is useful because it allows you to compute other summaries at the same time:\n\nflights |> \n  group_by(dest) |> \n  summarize(\n    n = n(),\n    delay = mean(arr_delay, na.rm = TRUE)\n  )\n#> # A tibble: 105 × 3\n#>   dest      n delay\n#>   <chr> <int> <dbl>\n#> 1 ABQ     254  4.38\n#> 2 ACK     265  4.85\n#> 3 ALB     439 14.4 \n#> 4 ANC       8 -2.5 \n#> 5 ATL   17215 11.3 \n#> 6 AUS    2439  6.02\n#> # ℹ 99 more rows\n\nn() is a special summary function that doesn’t take any arguments and instead accesses information about the “current” group. This means that it only works inside dplyr verbs:\n\nn()\n#> Error in `n()`:\n#> ! Must only be used inside data-masking verbs like `mutate()`,\n#>   `filter()`, and `group_by()`.\n\nThere are a couple of variants of n() and count() that you might find useful:\n\n\nn_distinct(x) counts the number of distinct (unique) values of one or more variables. For example, we could figure out which destinations are served by the most carriers:\n\nflights |> \n  group_by(dest) |> \n  summarize(carriers = n_distinct(carrier)) |> \n  arrange(desc(carriers))\n#> # A tibble: 105 × 2\n#>   dest  carriers\n#>   <chr>    <int>\n#> 1 ATL          7\n#> 2 BOS          7\n#> 3 CLT          7\n#> 4 ORD          7\n#> 5 TPA          7\n#> 6 AUS          6\n#> # ℹ 99 more rows\n\n\n\nA weighted count is a sum. For example you could “count” the number of miles each plane flew:\n\nflights |> \n  group_by(tailnum) |> \n  summarize(miles = sum(distance))\n#> # A tibble: 4,044 × 2\n#>   tailnum  miles\n#>   <chr>    <dbl>\n#> 1 D942DN    3418\n#> 2 N0EGMQ  250866\n#> 3 N10156  115966\n#> 4 N102UW   25722\n#> 5 N103US   24619\n#> 6 N104UW   25157\n#> # ℹ 4,038 more rows\n\nWeighted counts are a common problem so count() has a wt argument that does the same thing:\n\nflights |> count(tailnum, wt = distance)\n\n\n\nYou can count missing values by combining sum() and is.na(). In the flights dataset this represents flights that are cancelled:\n\nflights |> \n  group_by(dest) |> \n  summarize(n_cancelled = sum(is.na(dep_time))) \n#> # A tibble: 105 × 2\n#>   dest  n_cancelled\n#>   <chr>       <int>\n#> 1 ABQ             0\n#> 2 ACK             0\n#> 3 ALB            20\n#> 4 ANC             0\n#> 5 ATL           317\n#> 6 AUS            21\n#> # ℹ 99 more rows\n\n\n\n\n14.3.1 Exercises\n\nHow can you use count() to count the number rows with a missing value for a given variable?\nExpand the following calls to count() to instead use group_by(), summarize(), and arrange():\n\nflights |> count(dest, sort = TRUE)\nflights |> count(tailnum, wt = distance)"
  },
  {
    "objectID": "numbers.html#numeric-transformations",
    "href": "numbers.html#numeric-transformations",
    "title": "14  Numbers",
    "section": "\n14.4 Numeric transformations",
    "text": "14.4 Numeric transformations\nTransformation functions work well with mutate() because their output is the same length as the input. The vast majority of transformation functions are already built into base R. It’s impractical to list them all so this section will show the most useful ones. As an example, while R provides all the trigonometric functions that you might dream of, we don’t list them here because they’re rarely needed for data science.\n\n14.4.1 Arithmetic and recycling rules\nWe introduced the basics of arithmetic (+, -, *, /, ^) in Chapter 3 and have used them a bunch since. These functions don’t need a huge amount of explanation because they do what you learned in grade school. But we need to briefly talk about the recycling rules which determine what happens when the left and right hand sides have different lengths. This is important for operations like flights |> mutate(air_time = air_time / 60) because there are 336,776 numbers on the left of / but only one on the right.\nR handles mismatched lengths by recycling, or repeating, the short vector. We can see this in operation more easily if we create some vectors outside of a data frame:\n\nx <- c(1, 2, 10, 20)\nx / 5\n#> [1] 0.2 0.4 2.0 4.0\n# is shorthand for\nx / c(5, 5, 5, 5)\n#> [1] 0.2 0.4 2.0 4.0\n\nGenerally, you only want to recycle single numbers (i.e. vectors of length 1), but R will recycle any shorter length vector. It usually (but not always) gives you a warning if the longer vector isn’t a multiple of the shorter:\n\nx * c(1, 2)\n#> [1]  1  4 10 40\nx * c(1, 2, 3)\n#> Warning in x * c(1, 2, 3): longer object length is not a multiple of shorter\n#> object length\n#> [1]  1  4 30 20\n\nThese recycling rules are also applied to logical comparisons (==, <, <=, >, >=, !=) and can lead to a surprising result if you accidentally use == instead of %in% and the data frame has an unfortunate number of rows. For example, take this code which attempts to find all flights in January and February:\n\nflights |> \n  filter(month == c(1, 2))\n#> # A tibble: 25,977 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      542            540         2      923            850\n#> 3  2013     1     1      554            600        -6      812            837\n#> 4  2013     1     1      555            600        -5      913            854\n#> 5  2013     1     1      557            600        -3      838            846\n#> 6  2013     1     1      558            600        -2      849            851\n#> # ℹ 25,971 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nThe code runs without error, but it doesn’t return what you want. Because of the recycling rules it finds flights in odd numbered rows that departed in January and flights in even numbered rows that departed in February. And unfortunately there’s no warning because flights has an even number of rows.\nTo protect you from this type of silent failure, most tidyverse functions use a stricter form of recycling that only recycles single values. Unfortunately that doesn’t help here, or in many other cases, because the key computation is performed by the base R function ==, not filter().\n\n14.4.2 Minimum and maximum\nThe arithmetic functions work with pairs of variables. Two closely related functions are pmin() and pmax(), which when given two or more variables will return the smallest or largest value in each row:\n\ndf <- tribble(\n  ~x, ~y,\n  1,  3,\n  5,  2,\n  7, NA,\n)\n\ndf |> \n  mutate(\n    min = pmin(x, y, na.rm = TRUE),\n    max = pmax(x, y, na.rm = TRUE)\n  )\n#> # A tibble: 3 × 4\n#>       x     y   min   max\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     3     1     3\n#> 2     5     2     2     5\n#> 3     7    NA     7     7\n\nNote that these are different to the summary functions min() and max() which take multiple observations and return a single value. You can tell that you’ve used the wrong form when all the minimums and all the maximums have the same value:\n\ndf |> \n  mutate(\n    min = min(x, y, na.rm = TRUE),\n    max = max(x, y, na.rm = TRUE)\n  )\n#> # A tibble: 3 × 4\n#>       x     y   min   max\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1     1     3     1     7\n#> 2     5     2     1     7\n#> 3     7    NA     1     7\n\n\n14.4.3 Modular arithmetic\nModular arithmetic is the technical name for the type of math you did before you learned about decimal places, i.e. division that yields a whole number and a remainder. In R, %/% does integer division and %% computes the remainder:\n\n1:10 %/% 3\n#>  [1] 0 0 1 1 1 2 2 2 3 3\n1:10 %% 3\n#>  [1] 1 2 0 1 2 0 1 2 0 1\n\nModular arithmetic is handy for the flights dataset, because we can use it to unpack the sched_dep_time variable into hour and minute:\n\nflights |> \n  mutate(\n    hour = sched_dep_time %/% 100,\n    minute = sched_dep_time %% 100,\n    .keep = \"used\"\n  )\n#> # A tibble: 336,776 × 3\n#>   sched_dep_time  hour minute\n#>            <int> <dbl>  <dbl>\n#> 1            515     5     15\n#> 2            529     5     29\n#> 3            540     5     40\n#> 4            545     5     45\n#> 5            600     6      0\n#> 6            558     5     58\n#> # ℹ 336,770 more rows\n\nWe can combine that with the mean(is.na(x)) trick from Section 13.4 to see how the proportion of cancelled flights varies over the course of the day. The results are shown in Figure 14.1.\n\nflights |> \n  group_by(hour = sched_dep_time %/% 100) |> \n  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |> \n  filter(hour > 1) |> \n  ggplot(aes(x = hour, y = prop_cancelled)) +\n  geom_line(color = \"grey50\") + \n  geom_point(aes(size = n))\n\n\n\nFigure 14.1: A line plot with scheduled departure hour on the x-axis, and proportion of cancelled flights on the y-axis. Cancellations seem to accumulate over the course of the day until 8pm, very late flights are much less likely to be cancelled.\n\n\n\n\n\n14.4.4 Logarithms\nLogarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude and converting exponential growth to linear growth. In R, you have a choice of three logarithms: log() (the natural log, base e), log2() (base 2), and log10() (base 10). We recommend using log2() or log10(). log2() is easy to interpret because a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving; whereas log10() is easy to back-transform because (e.g.) 3 is 10^3 = 1000. The inverse of log() is exp(); to compute the inverse of log2() or log10() you’ll need to use 2^ or 10^.\n\n14.4.5 Rounding\nUse round(x) to round a number to the nearest integer:\n\nround(123.456)\n#> [1] 123\n\nYou can control the precision of the rounding with the second argument, digits. round(x, digits) rounds to the nearest 10^-n so digits = 2 will round to the nearest 0.01. This definition is useful because it implies round(x, -3) will round to the nearest thousand, which indeed it does:\n\nround(123.456, 2)  # two digits\n#> [1] 123.46\nround(123.456, 1)  # one digit\n#> [1] 123.5\nround(123.456, -1) # round to nearest ten\n#> [1] 120\nround(123.456, -2) # round to nearest hundred\n#> [1] 100\n\nThere’s one weirdness with round() that seems surprising at first glance:\n\nround(c(1.5, 2.5))\n#> [1] 2 2\n\nround() uses what’s known as “round half to even” or Banker’s rounding: if a number is half way between two integers, it will be rounded to the even integer. This is a good strategy because it keeps the rounding unbiased: half of all 0.5s are rounded up, and half are rounded down.\nround() is paired with floor() which always rounds down and ceiling() which always rounds up:\n\nx <- 123.456\n\nfloor(x)\n#> [1] 123\nceiling(x)\n#> [1] 124\n\nThese functions don’t have a digits argument, so you can instead scale down, round, and then scale back up:\n\n# Round down to nearest two digits\nfloor(x / 0.01) * 0.01\n#> [1] 123.45\n# Round up to nearest two digits\nceiling(x / 0.01) * 0.01\n#> [1] 123.46\n\nYou can use the same technique if you want to round() to a multiple of some other number:\n\n# Round to nearest multiple of 4\nround(x / 4) * 4\n#> [1] 124\n\n# Round to nearest 0.25\nround(x / 0.25) * 0.25\n#> [1] 123.5\n\n\n14.4.6 Cutting numbers into ranges\nUse cut()1 to break up (aka bin) a numeric vector into discrete buckets:\n\nx <- c(1, 2, 5, 10, 15, 20)\ncut(x, breaks = c(0, 5, 10, 15, 20))\n#> [1] (0,5]   (0,5]   (0,5]   (5,10]  (10,15] (15,20]\n#> Levels: (0,5] (5,10] (10,15] (15,20]\n\nThe breaks don’t need to be evenly spaced:\n\ncut(x, breaks = c(0, 5, 10, 100))\n#> [1] (0,5]    (0,5]    (0,5]    (5,10]   (10,100] (10,100]\n#> Levels: (0,5] (5,10] (10,100]\n\nYou can optionally supply your own labels. Note that there should be one less labels than breaks.\n\ncut(x, \n  breaks = c(0, 5, 10, 15, 20), \n  labels = c(\"sm\", \"md\", \"lg\", \"xl\")\n)\n#> [1] sm sm sm md lg xl\n#> Levels: sm md lg xl\n\nAny values outside of the range of the breaks will become NA:\n\ny <- c(NA, -10, 5, 10, 30)\ncut(y, breaks = c(0, 5, 10, 15, 20))\n#> [1] <NA>   <NA>   (0,5]  (5,10] <NA>  \n#> Levels: (0,5] (5,10] (10,15] (15,20]\n\nSee the documentation for other useful arguments like right and include.lowest, which control if the intervals are [a, b) or (a, b] and if the lowest interval should be [a, b].\n\n14.4.7 Cumulative and rolling aggregates\nBase R provides cumsum(), cumprod(), cummin(), cummax() for running, or cumulative, sums, products, mins and maxes. dplyr provides cummean() for cumulative means. Cumulative sums tend to come up the most in practice:\n\nx <- 1:10\ncumsum(x)\n#>  [1]  1  3  6 10 15 21 28 36 45 55\n\nIf you need more complex rolling or sliding aggregates, try the slider package.\n\n14.4.8 Exercises\n\nExplain in words what each line of the code used to generate Figure 14.1 does.\nWhat trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?\n\nCurrently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem by running the code below: there’s a gap between each hour.\n\nflights |> \n  filter(month == 1, day == 1) |> \n  ggplot(aes(x = sched_dep_time, y = dep_delay)) +\n  geom_point()\n\nConvert them to a more truthful representation of time (either fractional hours or minutes since midnight).\n\nRound dep_time and arr_time to the nearest five minutes."
  },
  {
    "objectID": "numbers.html#general-transformations",
    "href": "numbers.html#general-transformations",
    "title": "14  Numbers",
    "section": "\n14.5 General transformations",
    "text": "14.5 General transformations\nThe following sections describe some general transformations which are often used with numeric vectors, but can be applied to all other column types.\n\n14.5.1 Ranks\ndplyr provides a number of ranking functions inspired by SQL, but you should always start with dplyr::min_rank(). It uses the typical method for dealing with ties, e.g., 1st, 2nd, 2nd, 4th.\n\nx <- c(1, 2, 2, 3, 4, NA)\nmin_rank(x)\n#> [1]  1  2  2  4  5 NA\n\nNote that the smallest values get the lowest ranks; use desc(x) to give the largest values the smallest ranks:\n\nmin_rank(desc(x))\n#> [1]  5  3  3  2  1 NA\n\nIf min_rank() doesn’t do what you need, look at the variants dplyr::row_number(), dplyr::dense_rank(), dplyr::percent_rank(), and dplyr::cume_dist(). See the documentation for details.\n\ndf <- tibble(x = x)\ndf |> \n  mutate(\n    row_number = row_number(x),\n    dense_rank = dense_rank(x),\n    percent_rank = percent_rank(x),\n    cume_dist = cume_dist(x)\n  )\n#> # A tibble: 6 × 5\n#>       x row_number dense_rank percent_rank cume_dist\n#>   <dbl>      <int>      <int>        <dbl>     <dbl>\n#> 1     1          1          1         0          0.2\n#> 2     2          2          2         0.25       0.6\n#> 3     2          3          2         0.25       0.6\n#> 4     3          4          3         0.75       0.8\n#> 5     4          5          4         1          1  \n#> 6    NA         NA         NA        NA         NA\n\nYou can achieve many of the same results by picking the appropriate ties.method argument to base R’s rank(); you’ll probably also want to set na.last = \"keep\" to keep NAs as NA.\nrow_number() can also be used without any arguments when inside a dplyr verb. In this case, it’ll give the number of the “current” row. When combined with %% or %/% this can be a useful tool for dividing data into similarly sized groups:\n\ndf <- tibble(id = 1:10)\n\ndf |> \n  mutate(\n    row0 = row_number() - 1,\n    three_groups = row0 %% 3,\n    three_in_each_group = row0 %/% 3\n  )\n#> # A tibble: 10 × 4\n#>      id  row0 three_groups three_in_each_group\n#>   <int> <dbl>        <dbl>               <dbl>\n#> 1     1     0            0                   0\n#> 2     2     1            1                   0\n#> 3     3     2            2                   0\n#> 4     4     3            0                   1\n#> 5     5     4            1                   1\n#> 6     6     5            2                   1\n#> # ℹ 4 more rows\n\n\n14.5.2 Offsets\ndplyr::lead() and dplyr::lag() allow you to refer the values just before or just after the “current” value. They return a vector of the same length as the input, padded with NAs at the start or end:\n\nx <- c(2, 5, 11, 11, 19, 35)\nlag(x)\n#> [1] NA  2  5 11 11 19\nlead(x)\n#> [1]  5 11 11 19 35 NA\n\n\n\nx - lag(x) gives you the difference between the current and previous value.\n\nx - lag(x)\n#> [1] NA  3  6  0  8 16\n\n\n\nx == lag(x) tells you when the current value changes.\n\nx == lag(x)\n#> [1]    NA FALSE FALSE  TRUE FALSE FALSE\n\n\n\nYou can lead or lag by more than one position by using the second argument, n.\n\n14.5.3 Consecutive identifiers\nSometimes you want to start a new group every time some event occurs. For example, when you’re looking at website data, it’s common to want to break up events into sessions, where you begin a new session after gap of more than x minutes since the last activity. For example, imagine you have the times when someone visited a website:\n\nevents <- tibble(\n  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)\n)\n\nAnd you’ve computed the time between each event, and figured out if there’s a gap that’s big enough to qualify:\n\nevents <- events |> \n  mutate(\n    diff = time - lag(time, default = first(time)),\n    has_gap = diff >= 5\n  )\nevents\n#> # A tibble: 14 × 3\n#>    time  diff has_gap\n#>   <dbl> <dbl> <lgl>  \n#> 1     0     0 FALSE  \n#> 2     1     1 FALSE  \n#> 3     2     1 FALSE  \n#> 4     3     1 FALSE  \n#> 5     5     2 FALSE  \n#> 6    10     5 TRUE   \n#> # ℹ 8 more rows\n\nBut how do we go from that logical vector to something that we can group_by()? cumsum(), from Section 14.4.7, comes to the rescue as gap, i.e. has_gap is TRUE, will increment group by one (Section 13.4.2):\n\nevents |> mutate(\n  group = cumsum(has_gap)\n)\n#> # A tibble: 14 × 4\n#>    time  diff has_gap group\n#>   <dbl> <dbl> <lgl>   <int>\n#> 1     0     0 FALSE       0\n#> 2     1     1 FALSE       0\n#> 3     2     1 FALSE       0\n#> 4     3     1 FALSE       0\n#> 5     5     2 FALSE       0\n#> 6    10     5 TRUE        1\n#> # ℹ 8 more rows\n\nAnother approach for creating grouping variables is consecutive_id(), which starts a new group every time one of its arguments changes. For example, inspired by this stackoverflow question, imagine you have a data frame with a bunch of repeated values:\n\ndf <- tibble(\n  x = c(\"a\", \"a\", \"a\", \"b\", \"c\", \"c\", \"d\", \"e\", \"a\", \"a\", \"b\", \"b\"),\n  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4, 8, 10, 199)\n)\n\nIf you want to keep the first row from each repeated x, you could use group_by(), consecutive_id(), and slice_head():\n\ndf |> \n  group_by(id = consecutive_id(x)) |> \n  slice_head(n = 1)\n#> # A tibble: 7 × 3\n#> # Groups:   id [7]\n#>   x         y    id\n#>   <chr> <dbl> <int>\n#> 1 a         1     1\n#> 2 b         2     2\n#> 3 c         4     3\n#> 4 d         3     4\n#> 5 e         9     5\n#> 6 a         4     6\n#> # ℹ 1 more row\n\n\n14.5.4 Exercises\n\nFind the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().\nWhich plane (tailnum) has the worst on-time record?\nWhat time of day should you fly if you want to avoid delays as much as possible?\nWhat does flights |> group_by(dest) |> filter(row_number() < 4) do? What does flights |> group_by(dest) |> filter(row_number(dep_delay) < 4) do?\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\n\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.\n\nflights |> \n  mutate(hour = dep_time %/% 100) |> \n  group_by(year, month, day, hour) |> \n  summarize(\n    dep_delay = mean(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  filter(n > 5)\n\n\nLook at each destination. Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?\nFind all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination."
  },
  {
    "objectID": "numbers.html#numeric-summaries",
    "href": "numbers.html#numeric-summaries",
    "title": "14  Numbers",
    "section": "\n14.6 Numeric summaries",
    "text": "14.6 Numeric summaries\nJust using the counts, means, and sums that we’ve introduced already can get you a long way, but R provides many other useful summary functions. Here is a selection that you might find useful.\n\n14.6.1 Center\nSo far, we’ve mostly used mean() to summarize the center of a vector of values. As we’ve seen in ?sec-sample-size, because the mean is the sum divided by the count, it is sensitive to even just a few unusually high or low values. An alternative is to use the median(), which finds a value that lies in the “middle” of the vector, i.e. 50% of the values is above it and 50% are below it. Depending on the shape of the distribution of the variable you’re interested in, mean or median might be a better measure of center. For example, for symmetric distributions we generally report the mean while for skewed distributions we usually report the median.\nFigure 14.2 compares the mean vs. the median departure delay (in minutes) for each destination. The median delay is always smaller than the mean delay because flights sometimes leave multiple hours late, but never leave multiple hours early.\n\nflights |>\n  group_by(year, month, day) |>\n  summarize(\n    mean = mean(dep_delay, na.rm = TRUE),\n    median = median(dep_delay, na.rm = TRUE),\n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  ggplot(aes(x = mean, y = median)) + \n  geom_abline(slope = 1, intercept = 0, color = \"white\", linewidth = 2) +\n  geom_point()\n\n\n\nFigure 14.2: A scatterplot showing the differences of summarizing daily depature delay with median instead of mean.\n\n\n\n\nYou might also wonder about the mode, or the most common value. This is a summary that only works well for very simple cases (which is why you might have learned about it in high school), but it doesn’t work well for many real datasets. If the data is discrete, there may be multiple most common values, and if the data is continuous, there might be no most common value because every value is ever so slightly different. For these reasons, the mode tends not to be used by statisticians and there’s no mode function included in base R2.\n\n14.6.2 Minimum, maximum, and quantiles\nWhat if you’re interested in locations other than the center? min() and max() will give you the largest and smallest values. Another powerful tool is quantile() which is a generalization of the median: quantile(x, 0.25) will find the value of x that is greater than 25% of the values, quantile(x, 0.5) is equivalent to the median, and quantile(x, 0.95) will find the value that’s greater than 95% of the values.\nFor the flights data, you might want to look at the 95% quantile of delays rather than the maximum, because it will ignore the 5% of most delayed flights which can be quite extreme.\n\nflights |>\n  group_by(year, month, day) |>\n  summarize(\n    max = max(dep_delay, na.rm = TRUE),\n    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n#> # A tibble: 365 × 5\n#>    year month   day   max   q95\n#>   <int> <int> <int> <dbl> <dbl>\n#> 1  2013     1     1   853  70.1\n#> 2  2013     1     2   379  85  \n#> 3  2013     1     3   291  68  \n#> 4  2013     1     4   288  60  \n#> 5  2013     1     5   327  41  \n#> 6  2013     1     6   202  51  \n#> # ℹ 359 more rows\n\n\n14.6.3 Spread\nSometimes you’re not so interested in where the bulk of the data lies, but in how it is spread out. Two commonly used summaries are the standard deviation, sd(x), and the inter-quartile range, IQR(). We won’t explain sd() here since you’re probably already familiar with it, but IQR() might be new — it’s quantile(x, 0.75) - quantile(x, 0.25) and gives you the range that contains the middle 50% of the data.\nWe can use this to reveal a small oddity in the flights data. You might expect the spread of the distance between origin and destination to be zero, since airports are always in the same place. But the code below reveals a data oddity for airport EGE:\n\nflights |> \n  group_by(origin, dest) |> \n  summarize(\n    distance_sd = IQR(distance), \n    n = n(),\n    .groups = \"drop\"\n  ) |> \n  filter(distance_sd > 0)\n#> # A tibble: 2 × 4\n#>   origin dest  distance_sd     n\n#>   <chr>  <chr>       <dbl> <int>\n#> 1 EWR    EGE             1   110\n#> 2 JFK    EGE             1   103\n\n\n14.6.4 Distributions\nIt’s worth remembering that all of the summary statistics described above are a way of reducing the distribution down to a single number. This means that they’re fundamentally reductive, and if you pick the wrong summary, you can easily miss important differences between groups. That’s why it’s always a good idea to visualize the distribution before committing to your summary statistics.\nFigure 14.3 shows the overall distribution of departure delays. The distribution is so skewed that we have to zoom in to see the bulk of the data. This suggests that the mean is unlikely to be a good summary and we might prefer the median instead.\n\n#> Warning: package 'patchwork' was built under R version 4.2.1\n\n\n\nFigure 14.3: (Left) The histogram of the full data is extremely skewed making it hard to get any details. (Right) Zooming into delays of less than two hours makes it possible to see what’s happening with the bulk of the observations.\n\n\n\n\nIt’s also a good idea to check that distributions for subgroups resemble the whole. In the following plot 365 frequency polygons of dep_delay, one for each day, are overlaid. The distributions seem to follow a common pattern, suggesting it’s fine to use the same summary for each day.\n\nflights |>\n  filter(dep_delay < 120) |> \n  ggplot(aes(x = dep_delay, group = interaction(day, month))) + \n  geom_freqpoly(binwidth = 5, alpha = 1/5)\n\n\n\n\nDon’t be afraid to explore your own custom summaries specifically tailored for the data that you’re working with. In this case, that might mean separately summarizing the flights that left early vs. the flights that left late, or given that the values are so heavily skewed, you might try a log-transformation. Finally, don’t forget what you learned in ?sec-sample-size: whenever creating numerical summaries, it’s a good idea to include the number of observations in each group.\n\n14.6.5 Positions\nThere’s one final type of summary that’s useful for numeric vectors, but also works with every other type of value: extracting a value at a specific position: first(x), last(x), and nth(x, n).\nFor example, we can find the first and last departure for each day:\n\nflights |> \n  group_by(year, month, day) |> \n  summarize(\n    first_dep = first(dep_time, na_rm = TRUE), \n    fifth_dep = nth(dep_time, 5, na_rm = TRUE),\n    last_dep = last(dep_time, na_rm = TRUE)\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using\n#> the `.groups` argument.\n#> # A tibble: 365 × 6\n#> # Groups:   year, month [12]\n#>    year month   day first_dep fifth_dep last_dep\n#>   <int> <int> <int>     <int>     <int>    <int>\n#> 1  2013     1     1       517       554     2356\n#> 2  2013     1     2        42       535     2354\n#> 3  2013     1     3        32       520     2349\n#> 4  2013     1     4        25       531     2358\n#> 5  2013     1     5        14       534     2357\n#> 6  2013     1     6        16       555     2355\n#> # ℹ 359 more rows\n\n(NB: Because dplyr functions use _ to separate components of function and arguments names, these functions use na_rm instead of na.rm.)\nIf you’re familiar with [, which we’ll come back to in ?sec-subset-many, you might wonder if you ever need these functions. There are three reasons: the default argument allows you to provide a default if the specified position doesn’t exist, the order_by argument allows you to locally override the order of the rows, and the na_rm argument allows you to drop missing values.\nExtracting values at positions is complementary to filtering on ranks. Filtering gives you all variables, with each observation in a separate row:\n\nflights |> \n  group_by(year, month, day) |> \n  mutate(r = min_rank(sched_dep_time)) |> \n  filter(r %in% c(1, max(r)))\n#> # A tibble: 1,195 × 20\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1     2353           2359        -6      425            445\n#> 3  2013     1     1     2353           2359        -6      418            442\n#> 4  2013     1     1     2356           2359        -3      425            437\n#> 5  2013     1     2       42           2359        43      518            442\n#> 6  2013     1     2      458            500        -2      703            650\n#> # ℹ 1,189 more rows\n#> # ℹ 12 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n\n14.6.6 With mutate()\n\nAs the names suggest, the summary functions are typically paired with summarize(). However, because of the recycling rules we discussed in Section 14.4.1 they can also be usefully paired with mutate(), particularly when you want do some sort of group standardization. For example:\n\n\nx / sum(x) calculates the proportion of a total.\n\n(x - mean(x)) / sd(x) computes a Z-score (standardized to mean 0 and sd 1).\n\n(x - min(x)) / (max(x) - min(x)) standardizes to range [0, 1].\n\nx / first(x) computes an index based on the first observation.\n\n14.6.7 Exercises\n\nBrainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median() useful? When might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes?\nWhich destinations show the greatest variation in air speed?\nCreate a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations? Can you find another variable that might explain the difference?"
  },
  {
    "objectID": "numbers.html#summary",
    "href": "numbers.html#summary",
    "title": "14  Numbers",
    "section": "\n14.7 Summary",
    "text": "14.7 Summary\nYou’re already familiar with many tools for working with numbers, and after reading this chapter you now know how to use them in R. You’ve also learned a handful of useful general transformations that are commonly, but not exclusively, applied to numeric vectors like ranks and offsets. Finally, you worked through a number of numeric summaries, and discussed a few of the statistical challenges that you should consider.\nOver the next two chapters, we’ll dive into working with strings with the stringr package. Strings are a big topic so they get two chapters, one on the fundamentals of strings and one on regular expressions."
  },
  {
    "objectID": "strings.html#introduction",
    "href": "strings.html#introduction",
    "title": "15  Strings",
    "section": "\n15.1 Introduction",
    "text": "15.1 Introduction\nSo far, you’ve used a bunch of strings without learning much about the details. Now it’s time to dive into them, learn what makes strings tick, and master some of the powerful string manipulation tools you have at your disposal.\nWe’ll begin with the details of creating strings and character vectors. You’ll then dive into creating strings from data, then the opposite: extracting strings from data. We’ll then discuss tools that work with individual letters. The chapter finishes with functions that work with individual letters and a brief discussion of where your expectations from English might steer you wrong when working with other languages.\nWe’ll keep working with strings in the next chapter, where you’ll learn more about the power of regular expressions.\n\n15.1.1 Prerequisites\nIn this chapter, we’ll use functions from the stringr package, which is part of the core tidyverse. We’ll also use the babynames data since it provides some fun strings to manipulate.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(babynames)\n#> Warning: package 'babynames' was built under R version 4.2.3\n\nYou can quickly tell when you’re using a stringr function because all stringr functions start with str_. This is particularly useful if you use RStudio because typing str_ will trigger autocomplete, allowing you to jog your memory of the available functions."
  },
  {
    "objectID": "strings.html#creating-a-string",
    "href": "strings.html#creating-a-string",
    "title": "15  Strings",
    "section": "\n15.2 Creating a string",
    "text": "15.2 Creating a string\nWe’ve created strings in passing earlier in the book but didn’t discuss the details. Firstly, you can create a string using either single quotes (') or double quotes (\"). There’s no difference in behavior between the two, so in the interests of consistency, the tidyverse style guide recommends using \", unless the string contains multiple \".\n\nstring1 <- \"This is a string\"\nstring2 <- 'If I want to include a \"quote\" inside a string, I use single quotes'\n\nIf you forget to close a quote, you’ll see +, the continuation prompt:\n> \"This is a string without a closing quote\n+ \n+ \n+ HELP I'M STUCK IN A STRING\nIf this happens to you and you can’t figure out which quote to close, press Escape to cancel and try again.\n\n15.2.1 Escapes\nTo include a literal single or double quote in a string, you can use \\ to “escape” it:\n\ndouble_quote <- \"\\\"\" # or '\"'\nsingle_quote <- '\\'' # or \"'\"\n\nSo if you want to include a literal backslash in your string, you’ll need to escape it: \"\\\\\":\n\nbackslash <- \"\\\\\"\n\nBeware that the printed representation of a string is not the same as the string itself because the printed representation shows the escapes (in other words, when you print a string, you can copy and paste the output to recreate that string). To see the raw contents of the string, use str_view()1:\n\nx <- c(single_quote, double_quote, backslash)\nx\n#> [1] \"'\"  \"\\\"\" \"\\\\\"\n\nstr_view(x)\n#> [1] │ '\n#> [2] │ \"\n#> [3] │ \\\n\n\n15.2.2 Raw strings\nCreating a string with multiple quotes or backslashes gets confusing quickly. To illustrate the problem, let’s create a string that contains the contents of the code block where we define the double_quote and single_quote variables:\n\ntricky <- \"double_quote <- \\\"\\\\\\\"\\\" # or '\\\"'\nsingle_quote <- '\\\\'' # or \\\"'\\\"\"\nstr_view(tricky)\n#> [1] │ double_quote <- \"\\\"\" # or '\"'\n#>     │ single_quote <- '\\'' # or \"'\"\n\nThat’s a lot of backslashes! (This is sometimes called leaning toothpick syndrome.) To eliminate the escaping, you can instead use a raw string2:\n\ntricky <- r\"(double_quote <- \"\\\"\" # or '\"'\nsingle_quote <- '\\'' # or \"'\")\"\nstr_view(tricky)\n#> [1] │ double_quote <- \"\\\"\" # or '\"'\n#>     │ single_quote <- '\\'' # or \"'\"\n\nA raw string usually starts with r\"( and finishes with )\". But if your string contains )\" you can instead use r\"[]\" or r\"{}\", and if that’s still not enough, you can insert any number of dashes to make the opening and closing pairs unique, e.g., `r\"--()--\", `r\"---()---\", etc. Raw strings are flexible enough to handle any text.\n\n15.2.3 Other special characters\nAs well as \\\", \\', and \\\\, there are a handful of other special characters that may come in handy. The most common are \\n, a new line, and \\t, tab. You’ll also sometimes see strings containing Unicode escapes that start with \\u or \\U. This is a way of writing non-English characters that work on all systems. You can see the complete list of other special characters in ?Quotes.\n\nx <- c(\"one\\ntwo\", \"one\\ttwo\", \"\\u00b5\", \"\\U0001f604\")\nx\n#> [1] \"one\\ntwo\" \"one\\ttwo\" \"µ\"        \"😄\"\nstr_view(x)\n#> [1] │ one\n#>     │ two\n#> [2] │ one{\\t}two\n#> [3] │ µ\n#> [4] │ 😄\n\nNote that str_view() uses a blue background for tabs to make them easier to spot. One of the challenges of working with text is that there’s a variety of ways that white space can end up in the text, so this background helps you recognize that something strange is going on.\n\n15.2.4 Exercises\n\n\nCreate strings that contain the following values:\n\nHe said \"That's amazing!\"\n\\a\\b\\c\\d\n\\\\\\\\\\\\\n\n\n\nCreate the string in your R session and print it. What happens to the special “\\u00a0”? How does str_view() display it? Can you do a little googling to figure out what this special character is?\n\nx <- \"This\\u00a0is\\u00a0tricky\""
  },
  {
    "objectID": "strings.html#creating-many-strings-from-data",
    "href": "strings.html#creating-many-strings-from-data",
    "title": "15  Strings",
    "section": "\n15.3 Creating many strings from data",
    "text": "15.3 Creating many strings from data\nNow that you’ve learned the basics of creating a string or two by “hand”, we’ll go into the details of creating strings from other strings. This will help you solve the common problem where you have some text you wrote that you want to combine with strings from a data frame. For example, you might combine “Hello” with a name variable to create a greeting. We’ll show you how to do this with str_c() and str_glue() and how you can use them with mutate(). That naturally raises the question of what stringr functions you might use with summarize(), so we’ll finish this section with a discussion of str_flatten(), which is a summary function for strings.\n\n15.3.1 str_c()\n\nstr_c() takes any number of vectors as arguments and returns a character vector:\n\nstr_c(\"x\", \"y\")\n#> [1] \"xy\"\nstr_c(\"x\", \"y\", \"z\")\n#> [1] \"xyz\"\nstr_c(\"Hello \", c(\"John\", \"Susan\"))\n#> [1] \"Hello John\"  \"Hello Susan\"\n\nstr_c() is very similar to the base paste0(), but is designed to be used with mutate() by obeying the usual tidyverse rules for recycling and propagating missing values:\n\ndf <- tibble(name = c(\"Flora\", \"David\", \"Terra\", NA))\ndf |> mutate(greeting = str_c(\"Hi \", name, \"!\"))\n#> # A tibble: 4 × 2\n#>   name  greeting \n#>   <chr> <chr>    \n#> 1 Flora Hi Flora!\n#> 2 David Hi David!\n#> 3 Terra Hi Terra!\n#> 4 <NA>  <NA>\n\nIf you want missing values to display in another way, use coalesce() to replace them. Depending on what you want, you might use it either inside or outside of str_c():\n\ndf |> \n  mutate(\n    greeting1 = str_c(\"Hi \", coalesce(name, \"you\"), \"!\"),\n    greeting2 = coalesce(str_c(\"Hi \", name, \"!\"), \"Hi!\")\n  )\n#> # A tibble: 4 × 3\n#>   name  greeting1 greeting2\n#>   <chr> <chr>     <chr>    \n#> 1 Flora Hi Flora! Hi Flora!\n#> 2 David Hi David! Hi David!\n#> 3 Terra Hi Terra! Hi Terra!\n#> 4 <NA>  Hi you!   Hi!\n\n\n15.3.2 str_glue()\n\nIf you are mixing many fixed and variable strings with str_c(), you’ll notice that you type a lot of \"s, making it hard to see the overall goal of the code. An alternative approach is provided by the glue package via str_glue()3. You give it a single string that has a special feature: anything inside {} will be evaluated like it’s outside of the quotes:\n\ndf |> mutate(greeting = str_glue(\"Hi {name}!\"))\n#> # A tibble: 4 × 2\n#>   name  greeting \n#>   <chr> <glue>   \n#> 1 Flora Hi Flora!\n#> 2 David Hi David!\n#> 3 Terra Hi Terra!\n#> 4 <NA>  Hi NA!\n\nAs you can see, str_glue() currently converts missing values to the string \"NA\" unfortunately making it inconsistent with str_c().\nYou also might wonder what happens if you need to include a regular { or } in your string. You’re on the right track if you guess you’ll need to escape it somehow. The trick is that glue uses a slightly different escaping technique: instead of prefixing with special character like \\, you double up the special characters:\n\ndf |> mutate(greeting = str_glue(\"{{Hi {name}!}}\"))\n#> # A tibble: 4 × 2\n#>   name  greeting   \n#>   <chr> <glue>     \n#> 1 Flora {Hi Flora!}\n#> 2 David {Hi David!}\n#> 3 Terra {Hi Terra!}\n#> 4 <NA>  {Hi NA!}\n\n\n15.3.3 str_flatten()\n\nstr_c() and str_glue() work well with mutate() because their output is the same length as their inputs. What if you want a function that works well with summarize(), i.e. something that always returns a single string? That’s the job of str_flatten()4: it takes a character vector and combines each element of the vector into a single string:\n\nstr_flatten(c(\"x\", \"y\", \"z\"))\n#> [1] \"xyz\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \")\n#> [1] \"x, y, z\"\nstr_flatten(c(\"x\", \"y\", \"z\"), \", \", last = \", and \")\n#> [1] \"x, y, and z\"\n\nThis makes it work well with summarize():\n\ndf <- tribble(\n  ~ name, ~ fruit,\n  \"Carmen\", \"banana\",\n  \"Carmen\", \"apple\",\n  \"Marvin\", \"nectarine\",\n  \"Terence\", \"cantaloupe\",\n  \"Terence\", \"papaya\",\n  \"Terence\", \"mandarin\"\n)\ndf |>\n  group_by(name) |> \n  summarize(fruits = str_flatten(fruit, \", \"))\n#> # A tibble: 3 × 2\n#>   name    fruits                      \n#>   <chr>   <chr>                       \n#> 1 Carmen  banana, apple               \n#> 2 Marvin  nectarine                   \n#> 3 Terence cantaloupe, papaya, mandarin\n\n\n15.3.4 Exercises\n\n\nCompare and contrast the results of paste0() with str_c() for the following inputs:\n\nstr_c(\"hi \", NA)\nstr_c(letters[1:2], letters[1:3])\n\n\nWhat’s the difference between paste() and paste0()? How can you recreate the equivalent of paste() with str_c()?\n\nConvert the following expressions from str_c() to str_glue() or vice versa:\n\nstr_c(\"The price of \", food, \" is \", price)\nstr_glue(\"I'm {age} years old and live in {country}\")\nstr_c(\"\\\\section{\", title, \"}\")"
  },
  {
    "objectID": "strings.html#extracting-data-from-strings",
    "href": "strings.html#extracting-data-from-strings",
    "title": "15  Strings",
    "section": "\n15.4 Extracting data from strings",
    "text": "15.4 Extracting data from strings\nIt’s very common for multiple variables to be crammed together into a single string. In this section, you’ll learn how to use four tidyr functions to extract them:\n\ndf |> separate_longer_delim(col, delim)\ndf |> separate_longer_position(col, width)\ndf |> separate_wider_delim(col, delim, names)\ndf |> separate_wider_position(col, widths)\n\nIf you look closely, you can see there’s a common pattern here: separate_, then longer or wider, then _, then by delim or position. That’s because these four functions are composed of two simpler primitives:\n\nJust like with pivot_longer() and pivot_wider(), _longer functions make the input data frame longer by creating new rows and _wider functions make the input data frame wider by generating new columns.\n\ndelim splits up a string with a delimiter like \", \" or \" \"; position splits at specified widths, like c(3, 5, 2).\n\nWe’ll return to the last member of this family, separate_wider_regex(), in Chapter 16. It’s the most flexible of the wider functions, but you need to know something about regular expressions before you can use it.\nThe following two sections will give you the basic idea behind these separate functions, first separating into rows (which is a little simpler) and then separating into columns. We’ll finish off by discussing the tools that the wider functions give you to diagnose problems.\n\n15.4.1 Separating into rows\nSeparating a string into rows tends to be most useful when the number of components varies from row to row. The most common case is requiring separate_longer_delim() to split based on a delimiter:\n\ndf1 <- tibble(x = c(\"a,b,c\", \"d,e\", \"f\"))\ndf1 |> \n  separate_longer_delim(x, delim = \",\")\n#> # A tibble: 6 × 1\n#>   x    \n#>   <chr>\n#> 1 a    \n#> 2 b    \n#> 3 c    \n#> 4 d    \n#> 5 e    \n#> 6 f\n\nIt’s rarer to see separate_longer_position() in the wild, but some older datasets do use a very compact format where each character is used to record a value:\n\ndf2 <- tibble(x = c(\"1211\", \"131\", \"21\"))\ndf2 |> \n  separate_longer_position(x, width = 1)\n#> # A tibble: 9 × 1\n#>   x    \n#>   <chr>\n#> 1 1    \n#> 2 2    \n#> 3 1    \n#> 4 1    \n#> 5 1    \n#> 6 3    \n#> # ℹ 3 more rows\n\n\n15.4.2 Separating into columns\nSeparating a string into columns tends to be most useful when there are a fixed number of components in each string, and you want to spread them into columns. They are slightly more complicated than their longer equivalents because you need to name the columns. For example, in this following dataset, x is made up of a code, an edition number, and a year, separated by \".\". To use separate_wider_delim(), we supply the delimiter and the names in two arguments:\n\ndf3 <- tibble(x = c(\"a10.1.2022\", \"b10.2.2011\", \"e15.1.2015\"))\ndf3 |> \n  separate_wider_delim(\n    x,\n    delim = \".\",\n    names = c(\"code\", \"edition\", \"year\")\n  )\n#> # A tibble: 3 × 3\n#>   code  edition year \n#>   <chr> <chr>   <chr>\n#> 1 a10   1       2022 \n#> 2 b10   2       2011 \n#> 3 e15   1       2015\n\nIf a specific piece is not useful you can use an NA name to omit it from the results:\n\ndf3 |> \n  separate_wider_delim(\n    x,\n    delim = \".\",\n    names = c(\"code\", NA, \"year\")\n  )\n#> # A tibble: 3 × 2\n#>   code  year \n#>   <chr> <chr>\n#> 1 a10   2022 \n#> 2 b10   2011 \n#> 3 e15   2015\n\nseparate_wider_position() works a little differently because you typically want to specify the width of each column. So you give it a named integer vector, where the name gives the name of the new column, and the value is the number of characters it occupies. You can omit values from the output by not naming them:\n\ndf4 <- tibble(x = c(\"202215TX\", \"202122LA\", \"202325CA\")) \ndf4 |> \n  separate_wider_position(\n    x,\n    widths = c(year = 4, age = 2, state = 2)\n  )\n#> # A tibble: 3 × 3\n#>   year  age   state\n#>   <chr> <chr> <chr>\n#> 1 2022  15    TX   \n#> 2 2021  22    LA   \n#> 3 2023  25    CA\n\n\n15.4.3 Diagnosing widening problems\nseparate_wider_delim()5 requires a fixed and known set of columns. What happens if some of the rows don’t have the expected number of pieces? There are two possible problems, too few or too many pieces, so separate_wider_delim() provides two arguments to help: too_few and too_many. Let’s first look at the too_few case with the following sample dataset:\n\ndf <- tibble(x = c(\"1-1-1\", \"1-1-2\", \"1-3\", \"1-3-2\", \"1\"))\n\ndf |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\")\n  )\n#> Error in `separate_wider_delim()`:\n#> ! Expected 3 pieces in each element of `x`.\n#> ! 2 values were too short.\n#> ℹ Use `too_few = \"debug\"` to diagnose the problem.\n#> ℹ Use `too_few = \"align_start\"/\"align_end\"` to silence this message.\n\nYou’ll notice that we get an error, but the error gives us some suggestions on how you might proceed. Let’s start by debugging the problem:\n\ndebug <- df |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_few = \"debug\"\n  )\n#> Warning: Debug mode activated: adding variables `x_ok`, `x_pieces`, and\n#> `x_remainder`.\ndebug\n#> # A tibble: 5 × 6\n#>   x     y     z     x_ok  x_pieces x_remainder\n#>   <chr> <chr> <chr> <lgl>    <int> <chr>      \n#> 1 1-1-1 1     1     TRUE         3 \"\"         \n#> 2 1-1-2 1     2     TRUE         3 \"\"         \n#> 3 1-3   3     <NA>  FALSE        2 \"\"         \n#> 4 1-3-2 3     2     TRUE         3 \"\"         \n#> 5 1     <NA>  <NA>  FALSE        1 \"\"\n\nWhen you use the debug mode, you get three extra columns added to the output: x_ok, x_pieces, and x_remainder (if you separate a variable with a different name, you’ll get a different prefix). Here, x_ok lets you quickly find the inputs that failed:\n\ndebug |> filter(!x_ok)\n#> # A tibble: 2 × 6\n#>   x     y     z     x_ok  x_pieces x_remainder\n#>   <chr> <chr> <chr> <lgl>    <int> <chr>      \n#> 1 1-3   3     <NA>  FALSE        2 \"\"         \n#> 2 1     <NA>  <NA>  FALSE        1 \"\"\n\nx_pieces tells us how many pieces were found, compared to the expected 3 (the length of names). x_remainder isn’t useful when there are too few pieces, but we’ll see it again shortly.\nSometimes looking at this debugging information will reveal a problem with your delimiter strategy or suggest that you need to do more preprocessing before separating. In that case, fix the problem upstream and make sure to remove too_few = \"debug\" to ensure that new problems become errors.\nIn other cases, you may want to fill in the missing pieces with NAs and move on. That’s the job of too_few = \"align_start\" and too_few = \"align_end\" which allow you to control where the NAs should go:\n\ndf |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_few = \"align_start\"\n  )\n#> # A tibble: 5 × 3\n#>   x     y     z    \n#>   <chr> <chr> <chr>\n#> 1 1     1     1    \n#> 2 1     1     2    \n#> 3 1     3     <NA> \n#> 4 1     3     2    \n#> 5 1     <NA>  <NA>\n\nThe same principles apply if you have too many pieces:\n\ndf <- tibble(x = c(\"1-1-1\", \"1-1-2\", \"1-3-5-6\", \"1-3-2\", \"1-3-5-7-9\"))\n\ndf |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\")\n  )\n#> Error in `separate_wider_delim()`:\n#> ! Expected 3 pieces in each element of `x`.\n#> ! 2 values were too long.\n#> ℹ Use `too_many = \"debug\"` to diagnose the problem.\n#> ℹ Use `too_many = \"drop\"/\"merge\"` to silence this message.\n\nBut now, when we debug the result, you can see the purpose of x_remainder:\n\ndebug <- df |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"debug\"\n  )\n#> Warning: Debug mode activated: adding variables `x_ok`, `x_pieces`, and\n#> `x_remainder`.\ndebug |> filter(!x_ok)\n#> # A tibble: 2 × 6\n#>   x         y     z     x_ok  x_pieces x_remainder\n#>   <chr>     <chr> <chr> <lgl>    <int> <chr>      \n#> 1 1-3-5-6   3     5     FALSE        4 -6         \n#> 2 1-3-5-7-9 3     5     FALSE        5 -7-9\n\nYou have a slightly different set of options for handling too many pieces: you can either silently “drop” any additional pieces or “merge” them all into the final column:\n\ndf |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"drop\"\n  )\n#> # A tibble: 5 × 3\n#>   x     y     z    \n#>   <chr> <chr> <chr>\n#> 1 1     1     1    \n#> 2 1     1     2    \n#> 3 1     3     5    \n#> 4 1     3     2    \n#> 5 1     3     5\n\n\ndf |> \n  separate_wider_delim(\n    x,\n    delim = \"-\",\n    names = c(\"x\", \"y\", \"z\"),\n    too_many = \"merge\"\n  )\n#> # A tibble: 5 × 3\n#>   x     y     z    \n#>   <chr> <chr> <chr>\n#> 1 1     1     1    \n#> 2 1     1     2    \n#> 3 1     3     5-6  \n#> 4 1     3     2    \n#> 5 1     3     5-7-9"
  },
  {
    "objectID": "strings.html#letters",
    "href": "strings.html#letters",
    "title": "15  Strings",
    "section": "\n15.5 Letters",
    "text": "15.5 Letters\nIn this section, we’ll introduce you to functions that allow you to work with the individual letters within a string. You’ll learn how to find the length of a string, extract substrings, and handle long strings in plots and tables.\n\n15.5.1 Length\nstr_length() tells you the number of letters in the string:\n\nstr_length(c(\"a\", \"R for data science\", NA))\n#> [1]  1 18 NA\n\nYou could use this with count() to find the distribution of lengths of US babynames and then with filter() to look at the longest names, which happen to have 15 letters6:\n\nbabynames |>\n  count(length = str_length(name), wt = n)\n#> # A tibble: 14 × 2\n#>   length        n\n#>    <int>    <int>\n#> 1      2   338150\n#> 2      3  8589596\n#> 3      4 48506739\n#> 4      5 87011607\n#> 5      6 90749404\n#> 6      7 72120767\n#> # ℹ 8 more rows\n\nbabynames |> \n  filter(str_length(name) == 15) |> \n  count(name, wt = n, sort = TRUE)\n#> # A tibble: 34 × 2\n#>   name                n\n#>   <chr>           <int>\n#> 1 Franciscojavier   123\n#> 2 Christopherjohn   118\n#> 3 Johnchristopher   118\n#> 4 Christopherjame   108\n#> 5 Christophermich    52\n#> 6 Ryanchristopher    45\n#> # ℹ 28 more rows\n\n\n15.5.2 Subsetting\nYou can extract parts of a string using str_sub(string, start, end), where start and end are the positions where the substring should start and end. The start and end arguments are inclusive, so the length of the returned string will be end - start + 1:\n\nx <- c(\"Apple\", \"Banana\", \"Pear\")\nstr_sub(x, 1, 3)\n#> [1] \"App\" \"Ban\" \"Pea\"\n\nYou can use negative values to count back from the end of the string: -1 is the last character, -2 is the second to last character, etc.\n\nstr_sub(x, -3, -1)\n#> [1] \"ple\" \"ana\" \"ear\"\n\nNote that str_sub() won’t fail if the string is too short: it will just return as much as possible:\n\nstr_sub(\"a\", 1, 5)\n#> [1] \"a\"\n\nWe could use str_sub() with mutate() to find the first and last letter of each name:\n\nbabynames |> \n  mutate(\n    first = str_sub(name, 1, 1),\n    last = str_sub(name, -1, -1)\n  )\n#> # A tibble: 1,924,665 × 7\n#>    year sex   name          n   prop first last \n#>   <dbl> <chr> <chr>     <int>  <dbl> <chr> <chr>\n#> 1  1880 F     Mary       7065 0.0724 M     y    \n#> 2  1880 F     Anna       2604 0.0267 A     a    \n#> 3  1880 F     Emma       2003 0.0205 E     a    \n#> 4  1880 F     Elizabeth  1939 0.0199 E     h    \n#> 5  1880 F     Minnie     1746 0.0179 M     e    \n#> 6  1880 F     Margaret   1578 0.0162 M     t    \n#> # ℹ 1,924,659 more rows\n\n\n15.5.3 Exercises\n\nWhen computing the distribution of the length of babynames, why did we use wt = n?\nUse str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?\nAre there any major trends in the length of babynames over time? What about the popularity of first and last letters?"
  },
  {
    "objectID": "strings.html#sec-other-languages",
    "href": "strings.html#sec-other-languages",
    "title": "15  Strings",
    "section": "\n15.6 Non-English text",
    "text": "15.6 Non-English text\nSo far, we’ve focused on English language text which is particularly easy to work with for two reasons. Firstly, the English alphabet is relatively simple: there are just 26 letters. Secondly (and maybe more importantly), the computing infrastructure we use today was predominantly designed by English speakers. Unfortunately, we don’t have room for a full treatment of non-English languages. Still, we wanted to draw your attention to some of the biggest challenges you might encounter: encoding, letter variations, and locale-dependent functions.\n\n15.6.1 Encoding\nWhen working with non-English text, the first challenge is often the encoding. To understand what’s going on, we need to dive into how computers represent strings. In R, we can get at the underlying representation of a string using charToRaw():\n\ncharToRaw(\"Hadley\")\n#> [1] 48 61 64 6c 65 79\n\nEach of these six hexadecimal numbers represents one letter: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case, the encoding is called ASCII. ASCII does a great job of representing English characters because it’s the American Standard Code for Information Interchange.\nThings aren’t so easy for languages other than English. In the early days of computing, there were many competing standards for encoding non-English characters. For example, there were two different encodings for Europe: Latin1 (aka ISO-8859-1) was used for Western European languages, and Latin2 (aka ISO-8859-2) was used for Central European languages. In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today and many extra symbols like emojis.\nreadr uses UTF-8 everywhere. This is a good default but will fail for data produced by older systems that don’t use UTF-8. If this happens, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times, you’ll get complete gibberish. For example here are two inline CSVs with unusual encodings7:\n\nx1 <- \"text\\nEl Ni\\xf1o was particularly bad this year\"\nread_csv(x1)$text\n#> [1] \"El Ni\\xf1o was particularly bad this year\"\n\nx2 <- \"text\\n\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd\"\nread_csv(x2)$text\n#> [1] \"\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd\"\n\nTo read these correctly, you specify the encoding via the locale argument:\n\nread_csv(x1, locale = locale(encoding = \"Latin1\"))$text\n#> [1] \"El Niño was particularly bad this year\"\n\nread_csv(x2, locale = locale(encoding = \"Shift-JIS\"))$text\n#> [1] \"こんにちは\"\n\nHow do you find the correct encoding? If you’re lucky, it’ll be included somewhere in the data documentation. Unfortunately, that’s rarely the case, so readr provides guess_encoding() to help you figure it out. It’s not foolproof and works better when you have lots of text (unlike here), but it’s a reasonable place to start. Expect to try a few different encodings before you find the right one.\nEncodings are a rich and complex topic; we’ve only scratched the surface here. If you’d like to learn more, we recommend reading the detailed explanation at http://kunststube.net/encoding/.\n\n15.6.2 Letter variations\nWorking in languages with accents poses a significant challenge when determining the position of letters (e.g., with str_length() and str_sub()) as accented letters might be encoded as a single individual character (e.g., ü) or as two characters by combining an unaccented letter (e.g., u) with a diacritic mark (e.g., ¨). For example, this code shows two ways of representing ü that look identical:\n\nu <- c(\"\\u00fc\", \"u\\u0308\")\nstr_view(u)\n#> [1] │ ü\n#> [2] │ ü\n\nBut both strings differ in length, and their first characters are different:\n\nstr_length(u)\n#> [1] 1 2\nstr_sub(u, 1, 1)\n#> [1] \"ü\" \"u\"\n\nFinally, note that a comparison of these strings with == interprets these strings as different, while the handy str_equal() function in stringr recognizes that both have the same appearance:\n\nu[[1]] == u[[2]]\n#> [1] FALSE\n\nstr_equal(u[[1]], u[[2]])\n#> [1] TRUE\n\n\n15.6.3 Locale-dependent functions\nFinally, there are a handful of stringr functions whose behavior depends on your locale. A locale is similar to a language but includes an optional region specifier to handle regional variations within a language. A locale is specified by a lower-case language abbreviation, optionally followed by a _ and an upper-case region identifier. For example, “en” is English, “en_GB” is British English, and “en_US” is American English. If you don’t already know the code for your language, Wikipedia has a good list, and you can see which are supported in stringr by looking at stringi::stri_locale_list().\nBase R string functions automatically use the locale set by your operating system. This means that base R string functions do what you expect for your language, but your code might work differently if you share it with someone who lives in a different country. To avoid this problem, stringr defaults to English rules by using the “en” locale and requires you to specify the locale argument to override it. Fortunately, there are two sets of functions where the locale really matters: changing case and sorting.\nThe rules for changing cases differ among languages. For example, Turkish has two i’s: with and without a dot. Since they’re two distinct letters, they’re capitalized differently:\n\nstr_to_upper(c(\"i\", \"ı\"))\n#> [1] \"I\" \"I\"\nstr_to_upper(c(\"i\", \"ı\"), locale = \"tr\")\n#> [1] \"İ\" \"I\"\n\nSorting strings depends on the order of the alphabet, and the order of the alphabet is not the same in every language8! Here’s an example: in Czech, “ch” is a compound letter that appears after h in the alphabet.\n\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"))\n#> [1] \"a\"  \"c\"  \"ch\" \"h\"  \"z\"\nstr_sort(c(\"a\", \"c\", \"ch\", \"h\", \"z\"), locale = \"cs\")\n#> [1] \"a\"  \"c\"  \"h\"  \"ch\" \"z\"\n\nThis also comes up when sorting strings with dplyr::arrange(), which is why it also has a locale argument."
  },
  {
    "objectID": "strings.html#summary",
    "href": "strings.html#summary",
    "title": "15  Strings",
    "section": "\n15.7 Summary",
    "text": "15.7 Summary\nIn this chapter, you’ve learned about some of the power of the stringr package: how to create, combine, and extract strings, and about some of the challenges you might face with non-English strings. Now it’s time to learn one of the most important and powerful tools for working with strings: regular expressions. Regular expressions are a very concise but very expressive language for describing patterns within strings and are the topic of the next chapter."
  },
  {
    "objectID": "regexps.html#introduction",
    "href": "regexps.html#introduction",
    "title": "16  Regular expressions",
    "section": "\n16.1 Introduction",
    "text": "16.1 Introduction\nIn Chapter 15, you learned a whole bunch of useful functions for working with strings. This chapter will focus on functions that use regular expressions, a concise and powerful language for describing patterns within strings. The term “regular expression” is a bit of a mouthful, so most people abbreviate it to “regex”1 or “regexp”.\nThe chapter starts with the basics of regular expressions and the most useful stringr functions for data analysis. We’ll then expand your knowledge of patterns and cover seven important new topics (escaping, anchoring, character classes, shorthand classes, quantifiers, precedence, and grouping). Next, we’ll talk about some of the other types of patterns that stringr functions can work with and the various “flags” that allow you to tweak the operation of regular expressions. We’ll finish with a survey of other places in the tidyverse and base R where you might use regexes.\n\n16.1.1 Prerequisites\nIn this chapter, we’ll use regular expression functions from stringr and tidyr, both core members of the tidyverse, as well as data from the babynames package.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(babynames)\n#> Warning: package 'babynames' was built under R version 4.2.3\n\nThrough this chapter, we’ll use a mix of very simple inline examples so you can get the basic idea, the baby names data, and three character vectors from stringr:\n\n\nfruit contains the names of 80 fruits.\n\nwords contains 980 common English words.\n\nsentences contains 720 short sentences."
  },
  {
    "objectID": "regexps.html#sec-reg-basics",
    "href": "regexps.html#sec-reg-basics",
    "title": "16  Regular expressions",
    "section": "\n16.2 Pattern basics",
    "text": "16.2 Pattern basics\nWe’ll use str_view() to learn how regex patterns work. We used str_view() in the last chapter to better understand a string vs. its printed representation, and now we’ll use it with its second argument, a regular expression. When this is supplied, str_view() will show only the elements of the string vector that match, surrounding each match with <>, and, where possible, highlighting the match in blue.\nThe simplest patterns consist of letters and numbers which match those characters exactly:\n\nstr_view(fruit, \"berry\")\n#>  [6] │ bil<berry>\n#>  [7] │ black<berry>\n#> [10] │ blue<berry>\n#> [11] │ boysen<berry>\n#> [19] │ cloud<berry>\n#> [21] │ cran<berry>\n#> ... and 8 more\n\nLetters and numbers match exactly and are called literal characters. Most punctuation characters, like ., +, *, [, ], and ?, have special meanings2 and are called metacharacters. For example, . will match any character3, so \"a.\" will match any string that contains an “a” followed by another character :\n\nstr_view(c(\"a\", \"ab\", \"ae\", \"bd\", \"ea\", \"eab\"), \"a.\")\n#> [2] │ <ab>\n#> [3] │ <ae>\n#> [6] │ e<ab>\n\nOr we could find all the fruits that contain an “a”, followed by three letters, followed by an “e”:\n\nstr_view(fruit, \"a...e\")\n#>  [1] │ <apple>\n#>  [7] │ bl<ackbe>rry\n#> [48] │ mand<arine>\n#> [51] │ nect<arine>\n#> [62] │ pine<apple>\n#> [64] │ pomegr<anate>\n#> ... and 2 more\n\nQuantifiers control how many times a pattern can match:\n\n\n? makes a pattern optional (i.e. it matches 0 or 1 times)\n\n+ lets a pattern repeat (i.e. it matches at least once)\n\n* lets a pattern be optional or repeat (i.e. it matches any number of times, including 0).\n\n\n# ab? matches an \"a\", optionally followed by a \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab?\")\n#> [1] │ <a>\n#> [2] │ <ab>\n#> [3] │ <ab>b\n\n# ab+ matches an \"a\", followed by at least one \"b\".\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab+\")\n#> [2] │ <ab>\n#> [3] │ <abb>\n\n# ab* matches an \"a\", followed by any number of \"b\"s.\nstr_view(c(\"a\", \"ab\", \"abb\"), \"ab*\")\n#> [1] │ <a>\n#> [2] │ <ab>\n#> [3] │ <abb>\n\nCharacter classes are defined by [] and let you match a set of characters, e.g., [abcd] matches “a”, “b”, “c”, or “d”. You can also invert the match by starting with ^: [^abcd] matches anything except “a”, “b”, “c”, or “d”. We can use this idea to find the words containing an “x” surrounded by vowels, or a “y” surrounded by consonants:\n\nstr_view(words, \"[aeiou]x[aeiou]\")\n#> [284] │ <exa>ct\n#> [285] │ <exa>mple\n#> [288] │ <exe>rcise\n#> [289] │ <exi>st\nstr_view(words, \"[^aeiou]y[^aeiou]\")\n#> [836] │ <sys>tem\n#> [901] │ <typ>e\n\nYou can use alternation, |, to pick between one or more alternative patterns. For example, the following patterns look for fruits containing “apple”, “melon”, or “nut”, or a repeated vowel.\n\nstr_view(fruit, \"apple|melon|nut\")\n#>  [1] │ <apple>\n#> [13] │ canary <melon>\n#> [20] │ coco<nut>\n#> [52] │ <nut>\n#> [62] │ pine<apple>\n#> [72] │ rock <melon>\n#> ... and 1 more\nstr_view(fruit, \"aa|ee|ii|oo|uu\")\n#>  [9] │ bl<oo>d orange\n#> [33] │ g<oo>seberry\n#> [47] │ lych<ee>\n#> [66] │ purple mangost<ee>n\n\nRegular expressions are very compact and use a lot of punctuation characters, so they can seem overwhelming and hard to read at first. Don’t worry; you’ll get better with practice, and simple patterns will soon become second nature. Let’s kick off that process by practicing with some useful stringr functions."
  },
  {
    "objectID": "regexps.html#sec-stringr-regex-funs",
    "href": "regexps.html#sec-stringr-regex-funs",
    "title": "16  Regular expressions",
    "section": "\n16.3 Key functions",
    "text": "16.3 Key functions\nNow that you’ve got the basics of regular expressions under your belt, let’s use them with some stringr and tidyr functions. In the following section, you’ll learn how to detect the presence or absence of a match, how to count the number of matches, how to replace a match with fixed text, and how to extract text using a pattern.\n\n16.3.1 Detect matches\nstr_detect() returns a logical vector that is TRUE if the pattern matches an element of the character vector and FALSE otherwise:\n\nstr_detect(c(\"a\", \"b\", \"c\"), \"[aeiou]\")\n#> [1]  TRUE FALSE FALSE\n\nSince str_detect() returns a logical vector of the same length as the initial vector, it pairs well with filter(). For example, this code finds all the most popular names containing a lower-case “x”:\n\nbabynames |> \n  filter(str_detect(name, \"x\")) |> \n  count(name, wt = n, sort = TRUE)\n#> # A tibble: 974 × 2\n#>   name           n\n#>   <chr>      <int>\n#> 1 Alexander 665492\n#> 2 Alexis    399551\n#> 3 Alex      278705\n#> 4 Alexandra 232223\n#> 5 Max       148787\n#> 6 Alexa     123032\n#> # ℹ 968 more rows\n\nWe can also use str_detect() with summarize() by pairing it with sum() or mean(): sum(str_detect(x, pattern)) tells you the number of observations that match and mean(str_detect(x, pattern)) tells you the proportion that match. For example, the following snippet computes and visualizes the proportion of baby names4 that contain “x”, broken down by year. It looks like they’ve radically increased in popularity lately!\n\nbabynames |> \n  group_by(year) |> \n  summarize(prop_x = mean(str_detect(name, \"x\"))) |> \n  ggplot(aes(x = year, y = prop_x)) + \n  geom_line()\n\n\n\n\nThere are two functions that are closely related to str_detect(): str_subset() and str_which(). str_subset() returns a character vector containing only the strings that match. str_which() returns an integer vector giving the positions of the strings that match.\n\n16.3.2 Count matches\nThe next step up in complexity from str_detect() is str_count(): rather than a true or false, it tells you how many matches there are in each string.\n\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"p\")\n#> [1] 2 0 1\n\nNote that each match starts at the end of the previous match, i.e. regex matches never overlap. For example, in \"abababa\", how many times will the pattern \"aba\" match? Regular expressions say two, not three:\n\nstr_count(\"abababa\", \"aba\")\n#> [1] 2\nstr_view(\"abababa\", \"aba\")\n#> [1] │ <aba>b<aba>\n\nIt’s natural to use str_count() with mutate(). The following example uses str_count() with character classes to count the number of vowels and consonants in each name.\n\nbabynames |> \n  count(name) |> \n  mutate(\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n#> # A tibble: 97,310 × 4\n#>   name          n vowels consonants\n#>   <chr>     <int>  <int>      <int>\n#> 1 Aaban        10      2          3\n#> 2 Aabha         5      2          3\n#> 3 Aabid         2      2          3\n#> 4 Aabir         1      2          3\n#> 5 Aabriella     5      4          5\n#> 6 Aada          1      2          2\n#> # ℹ 97,304 more rows\n\nIf you look closely, you’ll notice that there’s something off with our calculations: “Aaban” contains three “a”s, but our summary reports only two vowels. That’s because regular expressions are case sensitive. There are three ways we could fix this:\n\nAdd the upper case vowels to the character class: str_count(name, \"[aeiouAEIOU]\").\nTell the regular expression to ignore case: str_count(name, regex(\"[aeiou]\", ignore_case = TRUE)). We’ll talk about more in Section 16.5.1.\nUse str_to_lower() to convert the names to lower case: str_count(str_to_lower(name), \"[aeiou]\").\n\nThis variety of approaches is pretty typical when working with strings — there are often multiple ways to reach your goal, either by making your pattern more complicated or by doing some preprocessing on your string. If you get stuck trying one approach, it can often be useful to switch gears and tackle the problem from a different perspective.\nIn this case, since we’re applying two functions to the name, I think it’s easier to transform it first:\n\nbabynames |> \n  count(name) |> \n  mutate(\n    name = str_to_lower(name),\n    vowels = str_count(name, \"[aeiou]\"),\n    consonants = str_count(name, \"[^aeiou]\")\n  )\n#> # A tibble: 97,310 × 4\n#>   name          n vowels consonants\n#>   <chr>     <int>  <int>      <int>\n#> 1 aaban        10      3          2\n#> 2 aabha         5      3          2\n#> 3 aabid         2      3          2\n#> 4 aabir         1      3          2\n#> 5 aabriella     5      5          4\n#> 6 aada          1      3          1\n#> # ℹ 97,304 more rows\n\n\n16.3.3 Replace values\nAs well as detecting and counting matches, we can also modify them with str_replace() and str_replace_all(). str_replace() replaces the first match, and as the name suggests, str_replace_all() replaces all matches.\n\nx <- c(\"apple\", \"pear\", \"banana\")\nstr_replace_all(x, \"[aeiou]\", \"-\")\n#> [1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\nstr_remove() and str_remove_all() are handy shortcuts for str_replace(x, pattern, \"\"):\n\nx <- c(\"apple\", \"pear\", \"banana\")\nstr_remove_all(x, \"[aeiou]\")\n#> [1] \"ppl\" \"pr\"  \"bnn\"\n\nThese functions are naturally paired with mutate() when doing data cleaning, and you’ll often apply them repeatedly to peel off layers of inconsistent formatting.\n\n16.3.4 Extract variables\nThe last function we’ll discuss uses regular expressions to extract data out of one column into one or more new columns: separate_wider_regex(). It’s a peer of the separate_wider_position() and separate_wider_delim() functions that you learned about in Section 15.4.2. These functions live in tidyr because they operate on (columns of) data frames, rather than individual vectors.\nLet’s create a simple dataset to show how it works. Here we have some data derived from babynames where we have the name, gender, and age of a bunch of people in a rather weird format5:\n\ndf <- tribble(\n  ~str,\n  \"<Sheryl>-F_34\",\n  \"<Kisha>-F_45\", \n  \"<Brandon>-N_33\",\n  \"<Sharon>-F_38\", \n  \"<Penny>-F_58\",\n  \"<Justin>-M_41\", \n  \"<Patricia>-F_84\", \n)\n\nTo extract this data using separate_wider_regex() we just need to construct a sequence of regular expressions that match each piece. If we want the contents of that piece to appear in the output, we give it a name:\n\ndf |> \n  separate_wider_regex(\n    str,\n    patterns = c(\n      \"<\", \n      name = \"[A-Za-z]+\", \n      \">-\", \n      gender = \".\", \"_\", \n      age = \"[0-9]+\"\n    )\n  )\n#> # A tibble: 7 × 3\n#>   name    gender age  \n#>   <chr>   <chr>  <chr>\n#> 1 Sheryl  F      34   \n#> 2 Kisha   F      45   \n#> 3 Brandon N      33   \n#> 4 Sharon  F      38   \n#> 5 Penny   F      58   \n#> 6 Justin  M      41   \n#> # ℹ 1 more row\n\nIf the match fails, you can use too_short = \"debug\" to figure out what went wrong, just like separate_wider_delim() and separate_wider_position().\n\n16.3.5 Exercises\n\nWhat baby name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)\nReplace all forward slashes in \"a/b/c/d/e\" with backslashes. What happens if you attempt to undo the transformation by replacing all backslashes with forward slashes? (We’ll discuss the problem very soon.)\nImplement a simple version of str_to_lower() using str_replace_all().\nCreate a regular expression that will match telephone numbers as commonly written in your country."
  },
  {
    "objectID": "regexps.html#pattern-details",
    "href": "regexps.html#pattern-details",
    "title": "16  Regular expressions",
    "section": "\n16.4 Pattern details",
    "text": "16.4 Pattern details\nNow that you understand the basics of the pattern language and how to use it with some stringr and tidyr functions, it’s time to dig into more of the details. First, we’ll start with escaping, which allows you to match metacharacters that would otherwise be treated specially. Next, you’ll learn about anchors which allow you to match the start or end of the string. Then, you’ll learn more about character classes and their shortcuts which allow you to match any character from a set. Next, you’ll learn the final details of quantifiers which control how many times a pattern can match. Then, we have to cover the important (but complex) topic of operator precedence and parentheses. And we’ll finish off with some details of grouping components of the pattern.\nThe terms we use here are the technical names for each component. They’re not always the most evocative of their purpose, but it’s very helpful to know the correct terms if you later want to Google for more details.\n\n16.4.1 Escaping\nIn order to match a literal ., you need an escape which tells the regular expression to match metacharacters6 literally. Like strings, regexps use the backslash for escaping. So, to match a ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So to create the regular expression \\. we need the string \"\\\\.\", as the following example shows.\n\n# To create the regular expression \\., we need to use \\\\.\ndot <- \"\\\\.\"\n\n# But the expression itself only contains one \\\nstr_view(dot)\n#> [1] │ \\.\n\n# And this tells R to look for an explicit .\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n#> [2] │ <a.c>\n\nIn this book, we’ll usually write regular expression without quotes, like \\.. If we need to emphasize what you’ll actually type, we’ll surround it with quotes and add extra escapes, like \"\\\\.\".\nIf \\ is used as an escape character in regular expressions, how do you match a literal \\? Well, you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write \"\\\\\\\\\" — you need four backslashes to match one!\n\nx <- \"a\\\\b\"\nstr_view(x)\n#> [1] │ a\\b\nstr_view(x, \"\\\\\\\\\")\n#> [1] │ a<\\>b\n\nAlternatively, you might find it easier to use the raw strings you learned about in Section 15.2.2). That lets you avoid one layer of escaping:\n\nstr_view(x, r\"{\\\\}\")\n#> [1] │ a<\\>b\n\nIf you’re trying to match a literal ., $, |, *, +, ?, {, }, (, ), there’s an alternative to using a backslash escape: you can use a character class: [.], [$], [|], ... all match the literal values.\n\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \"a[.]c\")\n#> [2] │ <a.c>\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \".[*]c\")\n#> [3] │ <a*c>\n\n\n16.4.2 Anchors\nBy default, regular expressions will match any part of a string. If you want to match at the start or end you need to anchor the regular expression using ^ to match the start or $ to match the end:\n\nstr_view(fruit, \"^a\")\n#> [1] │ <a>pple\n#> [2] │ <a>pricot\n#> [3] │ <a>vocado\nstr_view(fruit, \"a$\")\n#>  [4] │ banan<a>\n#> [15] │ cherimoy<a>\n#> [30] │ feijo<a>\n#> [36] │ guav<a>\n#> [56] │ papay<a>\n#> [74] │ satsum<a>\n\nIt’s tempting to think that $ should match the start of a string, because that’s how we write dollar amounts, but that’s not what regular expressions want.\nTo force a regular expression to match only the full string, anchor it with both ^ and $:\n\nstr_view(fruit, \"apple\")\n#>  [1] │ <apple>\n#> [62] │ pine<apple>\nstr_view(fruit, \"^apple$\")\n#> [1] │ <apple>\n\nYou can also match the boundary between words (i.e. the start or end of a word) with \\b. This can be particularly useful when using RStudio’s find and replace tool. For example, if to find all uses of sum(), you can search for \\bsum\\b to avoid matching summarize, summary, rowsum and so on:\n\nx <- c(\"summary(x)\", \"summarize(df)\", \"rowsum(x)\", \"sum(x)\")\nstr_view(x, \"sum\")\n#> [1] │ <sum>mary(x)\n#> [2] │ <sum>marize(df)\n#> [3] │ row<sum>(x)\n#> [4] │ <sum>(x)\nstr_view(x, \"\\\\bsum\\\\b\")\n#> [4] │ <sum>(x)\n\nWhen used alone, anchors will produce a zero-width match:\n\nstr_view(\"abc\", c(\"$\", \"^\", \"\\\\b\"))\n#> [1] │ abc<>\n#> [2] │ <>abc\n#> [3] │ <>abc<>\n\nThis helps you understand what happens when you replace a standalone anchor:\n\nstr_replace_all(\"abc\", c(\"$\", \"^\", \"\\\\b\"), \"--\")\n#> [1] \"abc--\"   \"--abc\"   \"--abc--\"\n\n\n16.4.3 Character classes\nA character class, or character set, allows you to match any character in a set. As we discussed above, you can construct your own sets with [], where [abc] matches “a”, “b”, or “c” and [^abc] matches any character except “a”, “b”, or “c”. Apart from ^ there are two other characters that have special meaning inside of []:\n\n\n- defines a range, e.g., [a-z] matches any lower case letter and [0-9] matches any number.\n\n\\ escapes special characters, so [\\^\\-\\]] matches ^, -, or ].\n\nHere are few examples:\n\nx <- \"abcd ABCD 12345 -!@#%.\"\nstr_view(x, \"[abc]+\")\n#> [1] │ <abc>d ABCD 12345 -!@#%.\nstr_view(x, \"[a-z]+\")\n#> [1] │ <abcd> ABCD 12345 -!@#%.\nstr_view(x, \"[^a-z0-9]+\")\n#> [1] │ abcd< ABCD >12345< -!@#%.>\n\n# You need an escape to match characters that are otherwise\n# special inside of []\nstr_view(\"a-b-c\", \"[a-c]\")\n#> [1] │ <a>-<b>-<c>\nstr_view(\"a-b-c\", \"[a\\\\-c]\")\n#> [1] │ <a><->b<-><c>\n\nSome character classes are used so commonly that they get their own shortcut. You’ve already seen ., which matches any character apart from a newline. There are three other particularly useful pairs7:\n\n\n\\d matches any digit;\\D matches anything that isn’t a digit.\n\n\\s matches any whitespace (e.g., space, tab, newline);\\S matches anything that isn’t whitespace.\n\n\\w matches any “word” character, i.e. letters and numbers;\\W matches any “non-word” character.\n\nThe following code demonstrates the six shortcuts with a selection of letters, numbers, and punctuation characters.\n\nx <- \"abcd ABCD 12345 -!@#%.\"\nstr_view(x, \"\\\\d+\")\n#> [1] │ abcd ABCD <12345> -!@#%.\nstr_view(x, \"\\\\D+\")\n#> [1] │ <abcd ABCD >12345< -!@#%.>\nstr_view(x, \"\\\\s+\")\n#> [1] │ abcd< >ABCD< >12345< >-!@#%.\nstr_view(x, \"\\\\S+\")\n#> [1] │ <abcd> <ABCD> <12345> <-!@#%.>\nstr_view(x, \"\\\\w+\")\n#> [1] │ <abcd> <ABCD> <12345> -!@#%.\nstr_view(x, \"\\\\W+\")\n#> [1] │ abcd< >ABCD< >12345< -!@#%.>\n\n\n16.4.4 Quantifiers\nQuantifiers control how many times a pattern matches. In Section 16.2 you learned about ? (0 or 1 matches), + (1 or more matches), and * (0 or more matches). For example, colou?r will match American or British spelling, \\d+ will match one or more digits, and \\s? will optionally match a single item of whitespace. You can also specify the number of matches precisely with {}:\n\n\n{n} matches exactly n times.\n\n{n,} matches at least n times.\n\n{n,m} matches between n and m times.\n\n16.4.5 Operator precedence and parentheses\nWhat does ab+ match? Does it match “a” followed by one or more “b”s, or does it match “ab” repeated any number of times? What does ^a|b$ match? Does it match the complete string a or the complete string b, or does it match a string starting with a or a string ending with b?\nThe answer to these questions is determined by operator precedence, similar to the PEMDAS or BEDMAS rules you might have learned in school. You know that a + b * c is equivalent to a + (b * c) not (a + b) * c because * has higher precedence and + has lower precedence: you compute * before +.\nSimilarly, regular expressions have their own precedence rules: quantifiers have high precedence and alternation has low precedence which means that ab+ is equivalent to a(b+), and ^a|b$ is equivalent to (^a)|(b$). Just like with algebra, you can use parentheses to override the usual order. But unlike algebra you’re unlikely to remember the precedence rules for regexes, so feel free to use parentheses liberally.\n\n16.4.6 Grouping and capturing\nAs well as overriding operator precedence, parentheses have another important effect: they create capturing groups that allow you to use sub-components of the match.\nThe first way to use a capturing group is to refer back to it within a match with back reference: \\1 refers to the match contained in the first parenthesis, \\2 in the second parenthesis, and so on. For example, the following pattern finds all fruits that have a repeated pair of letters:\n\nstr_view(fruit, \"(..)\\\\1\")\n#>  [4] │ b<anan>a\n#> [20] │ <coco>nut\n#> [22] │ <cucu>mber\n#> [41] │ <juju>be\n#> [56] │ <papa>ya\n#> [73] │ s<alal> berry\n\nAnd this one finds all words that start and end with the same pair of letters:\n\nstr_view(words, \"^(..).*\\\\1$\")\n#> [152] │ <church>\n#> [217] │ <decide>\n#> [617] │ <photograph>\n#> [699] │ <require>\n#> [739] │ <sense>\n\nYou can also use back references in str_replace(). For example, this code switches the order of the second and third words in sentences:\n\nsentences |> \n  str_replace(\"(\\\\w+) (\\\\w+) (\\\\w+)\", \"\\\\1 \\\\3 \\\\2\") |> \n  str_view()\n#> [1] │ The canoe birch slid on the smooth planks.\n#> [2] │ Glue sheet the to the dark blue background.\n#> [3] │ It's to easy tell the depth of a well.\n#> [4] │ These a days chicken leg is a rare dish.\n#> [5] │ Rice often is served in round bowls.\n#> [6] │ The of juice lemons makes fine punch.\n#> ... and 714 more\n\nIf you want to extract the matches for each group you can use str_match(). But str_match() returns a matrix, so it’s not particularly easy to work with8:\n\nsentences |> \n  str_match(\"the (\\\\w+) (\\\\w+)\") |> \n  head()\n#>      [,1]                [,2]     [,3]    \n#> [1,] \"the smooth planks\" \"smooth\" \"planks\"\n#> [2,] \"the sheet to\"      \"sheet\"  \"to\"    \n#> [3,] \"the depth of\"      \"depth\"  \"of\"    \n#> [4,] NA                  NA       NA      \n#> [5,] NA                  NA       NA      \n#> [6,] NA                  NA       NA\n\nYou could convert to a tibble and name the columns:\n\nsentences |> \n  str_match(\"the (\\\\w+) (\\\\w+)\") |> \n  as_tibble(.name_repair = \"minimal\") |> \n  set_names(\"match\", \"word1\", \"word2\")\n#> # A tibble: 720 × 3\n#>   match             word1  word2 \n#>   <chr>             <chr>  <chr> \n#> 1 the smooth planks smooth planks\n#> 2 the sheet to      sheet  to    \n#> 3 the depth of      depth  of    \n#> 4 <NA>              <NA>   <NA>  \n#> 5 <NA>              <NA>   <NA>  \n#> 6 <NA>              <NA>   <NA>  \n#> # ℹ 714 more rows\n\nBut then you’ve basically recreated your own version of separate_wider_regex(). Indeed, behind the scenes, separate_wider_regex() converts your vector of patterns to a single regex that uses grouping to capture the named components.\nOccasionally, you’ll want to use parentheses without creating matching groups. You can create a non-capturing group with (?:).\n\nx <- c(\"a gray cat\", \"a grey dog\")\nstr_match(x, \"gr(e|a)y\")\n#>      [,1]   [,2]\n#> [1,] \"gray\" \"a\" \n#> [2,] \"grey\" \"e\"\nstr_match(x, \"gr(?:e|a)y\")\n#>      [,1]  \n#> [1,] \"gray\"\n#> [2,] \"grey\"\n\n\n16.4.7 Exercises\n\nHow would you match the literal string \"'\\? How about \"$^$\"?\nExplain why each of these patterns don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\".\n\nGiven the corpus of common words in stringr::words, create regular expressions that find all words that:\n\nStart with “y”.\nDon’t start with “y”.\nEnd with “x”.\nAre exactly three letters long. (Don’t cheat by using str_length()!)\nHave seven letters or more.\nContain a vowel-consonant pair.\nContain at least two vowel-consonant pairs in a row.\nOnly consist of repeated vowel-consonant pairs.\n\n\nCreate 11 regular expressions that match the British or American spellings for each of the following words: airplane/aeroplane, aluminum/aluminium, analog/analogue, ass/arse, center/centre, defense/defence, donut/doughnut, gray/grey, modeling/modelling, skeptic/sceptic, summarize/summarise. Try and make the shortest possible regex!\nSwitch the first and last letters in words. Which of those strings are still words?\n\nDescribe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)\n\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\n\\..\\..\\..\n(.)\\1\\1\n\"(..)\\\\1\"\n\n\nSolve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner."
  },
  {
    "objectID": "regexps.html#pattern-control",
    "href": "regexps.html#pattern-control",
    "title": "16  Regular expressions",
    "section": "\n16.5 Pattern control",
    "text": "16.5 Pattern control\nIt’s possible to exercise extra control over the details of the match by using a pattern object instead of just a string. This allows you to control the so called regex flags and match various types of fixed strings, as described below.\n\n16.5.1 Regex flags\nThere are a number of settings that can be used to control the details of the regexp. These settings are often called flags in other programming languages. In stringr, you can use these by wrapping the pattern in a call to regex(). The most useful flag is probably ignore_case = TRUE because it allows characters to match either their uppercase or lowercase forms:\n\nbananas <- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\n#> [1] │ <banana>\nstr_view(bananas, regex(\"banana\", ignore_case = TRUE))\n#> [1] │ <banana>\n#> [2] │ <Banana>\n#> [3] │ <BANANA>\n\nIf you’re doing a lot of work with multiline strings (i.e. strings that contain \\n), dotalland multiline may also be useful:\n\n\ndotall = TRUE lets . match everything, including \\n:\n\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \".Line\")\nstr_view(x, regex(\".Line\", dotall = TRUE))\n#> [1] │ Line 1<\n#>     │ Line> 2<\n#>     │ Line> 3\n\n\n\nmultiline = TRUE makes ^ and $ match the start and end of each line rather than the start and end of the complete string:\n\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_view(x, \"^Line\")\n#> [1] │ <Line> 1\n#>     │ Line 2\n#>     │ Line 3\nstr_view(x, regex(\"^Line\", multiline = TRUE))\n#> [1] │ <Line> 1\n#>     │ <Line> 2\n#>     │ <Line> 3\n\n\n\nFinally, if you’re writing a complicated regular expression and you’re worried you might not understand it in the future, you might try comments = TRUE. It tweaks the pattern language to ignore spaces and new lines, as well as everything after #. This allows you to use comments and whitespace to make complex regular expressions more understandable9, as in the following example:\n\nphone <- regex(\n  r\"(\n    \\(?     # optional opening parens\n    (\\d{3}) # area code\n    [)\\-]?  # optional closing parens or dash\n    \\ ?     # optional space\n    (\\d{3}) # another three numbers\n    [\\ -]?  # optional space or dash\n    (\\d{4}) # four more numbers\n  )\", \n  comments = TRUE\n)\n\nstr_extract(c(\"514-791-8141\", \"(123) 456 7890\", \"123456\"), phone)\n#> [1] \"514-791-8141\"   \"(123) 456 7890\" NA\n\nIf you’re using comments and want to match a space, newline, or #, you’ll need to escape it with \\.\n\n16.5.2 Fixed matches\nYou can opt-out of the regular expression rules by using fixed():\n\nstr_view(c(\"\", \"a\", \".\"), fixed(\".\"))\n#> [3] │ <.>\n\nfixed() also gives you the ability to ignore case:\n\nstr_view(\"x X\", \"X\")\n#> [1] │ x <X>\nstr_view(\"x X\", fixed(\"X\", ignore_case = TRUE))\n#> [1] │ <x> <X>\n\nIf you’re working with non-English text, you will probably want coll() instead of fixed(), as it implements the full rules for capitalization as used by the locale you specify. See Section 15.6 for more details on locales.\n\nstr_view(\"i İ ı I\", fixed(\"İ\", ignore_case = TRUE))\n#> [1] │ i <İ> ı I\nstr_view(\"i İ ı I\", coll(\"İ\", ignore_case = TRUE, locale = \"tr\"))\n#> [1] │ <i> <İ> ı I"
  },
  {
    "objectID": "regexps.html#practice",
    "href": "regexps.html#practice",
    "title": "16  Regular expressions",
    "section": "\n16.6 Practice",
    "text": "16.6 Practice\nTo put these ideas into practice we’ll solve a few semi-authentic problems next. We’ll discuss three general techniques:\n\nchecking your work by creating simple positive and negative controls\ncombining regular expressions with Boolean algebra\ncreating complex patterns using string manipulation\n\n\n16.6.1 Check your work\nFirst, let’s find all sentences that start with “The”. Using the ^ anchor alone is not enough:\n\nstr_view(sentences, \"^The\")\n#>  [1] │ <The> birch canoe slid on the smooth planks.\n#>  [4] │ <The>se days a chicken leg is a rare dish.\n#>  [6] │ <The> juice of lemons makes fine punch.\n#>  [7] │ <The> box was thrown beside the parked truck.\n#>  [8] │ <The> hogs were fed chopped corn and garbage.\n#> [11] │ <The> boy was there when the sun rose.\n#> ... and 271 more\n\nBecause that pattern also matches sentences starting with words like They or These. We need to make sure that the “e” is the last letter in the word, which we can do by adding a word boundary:\n\nstr_view(sentences, \"^The\\\\b\")\n#>  [1] │ <The> birch canoe slid on the smooth planks.\n#>  [6] │ <The> juice of lemons makes fine punch.\n#>  [7] │ <The> box was thrown beside the parked truck.\n#>  [8] │ <The> hogs were fed chopped corn and garbage.\n#> [11] │ <The> boy was there when the sun rose.\n#> [13] │ <The> source of the huge river is the clear spring.\n#> ... and 250 more\n\nWhat about finding all sentences that begin with a pronoun?\n\nstr_view(sentences, \"^She|He|It|They\\\\b\")\n#>  [3] │ <It>'s easy to tell the depth of a well.\n#> [15] │ <He>lp the woman get back to her feet.\n#> [27] │ <He>r purse was full of useless trash.\n#> [29] │ <It> snowed, rained, and hailed the same morning.\n#> [63] │ <He> ran half way to the hardware store.\n#> [90] │ <He> lay prone and hardly moved a limb.\n#> ... and 57 more\n\nA quick inspection of the results shows that we’re getting some spurious matches. That’s because we’ve forgotten to use parentheses:\n\nstr_view(sentences, \"^(She|He|It|They)\\\\b\")\n#>   [3] │ <It>'s easy to tell the depth of a well.\n#>  [29] │ <It> snowed, rained, and hailed the same morning.\n#>  [63] │ <He> ran half way to the hardware store.\n#>  [90] │ <He> lay prone and hardly moved a limb.\n#> [116] │ <He> ordered peach pie with ice cream.\n#> [127] │ <It> caught its hind paw in a rusty trap.\n#> ... and 51 more\n\nYou might wonder how you might spot such a mistake if it didn’t occur in the first few matches. A good technique is to create a few positive and negative matches and use them to test that your pattern works as expected:\n\npos <- c(\"He is a boy\", \"She had a good time\")\nneg <- c(\"Shells come from the sea\", \"Hadley said 'It's a great day'\")\n\npattern <- \"^(She|He|It|They)\\\\b\"\nstr_detect(pos, pattern)\n#> [1] TRUE TRUE\nstr_detect(neg, pattern)\n#> [1] FALSE FALSE\n\nIt’s typically much easier to come up with good positive examples than negative examples, because it takes a while before you’re good enough with regular expressions to predict where your weaknesses are. Nevertheless, they’re still useful: as you work on the problem you can slowly accumulate a collection of your mistakes, ensuring that you never make the same mistake twice.\n\n16.6.2 Boolean operations\nImagine we want to find words that only contain consonants. One technique is to create a character class that contains all letters except for the vowels ([^aeiou]), then allow that to match any number of letters ([^aeiou]+), then force it to match the whole string by anchoring to the beginning and the end (^[^aeiou]+$):\n\nstr_view(words, \"^[^aeiou]+$\")\n#> [123] │ <by>\n#> [249] │ <dry>\n#> [328] │ <fly>\n#> [538] │ <mrs>\n#> [895] │ <try>\n#> [952] │ <why>\n\nBut you can make this problem a bit easier by flipping the problem around. Instead of looking for words that contain only consonants, we could look for words that don’t contain any vowels:\n\nstr_view(words[!str_detect(words, \"[aeiou]\")])\n#> [1] │ by\n#> [2] │ dry\n#> [3] │ fly\n#> [4] │ mrs\n#> [5] │ try\n#> [6] │ why\n\nThis is a useful technique whenever you’re dealing with logical combinations, particularly those involving “and” or “not”. For example, imagine if you want to find all words that contain “a” and “b”. There’s no “and” operator built in to regular expressions so we have to tackle it by looking for all words that contain an “a” followed by a “b”, or a “b” followed by an “a”:\n\nstr_view(words, \"a.*b|b.*a\")\n#>  [2] │ <ab>le\n#>  [3] │ <ab>out\n#>  [4] │ <ab>solute\n#> [62] │ <availab>le\n#> [66] │ <ba>by\n#> [67] │ <ba>ck\n#> ... and 24 more\n\nIt’s simpler to combine the results of two calls to str_detect():\n\nwords[str_detect(words, \"a\") & str_detect(words, \"b\")]\n#>  [1] \"able\"      \"about\"     \"absolute\"  \"available\" \"baby\"      \"back\"     \n#>  [7] \"bad\"       \"bag\"       \"balance\"   \"ball\"      \"bank\"      \"bar\"      \n#> [13] \"base\"      \"basis\"     \"bear\"      \"beat\"      \"beauty\"    \"because\"  \n#> [19] \"black\"     \"board\"     \"boat\"      \"break\"     \"brilliant\" \"britain\"  \n#> [25] \"debate\"    \"husband\"   \"labour\"    \"maybe\"     \"probable\"  \"table\"\n\nWhat if we wanted to see if there was a word that contains all vowels? If we did it with patterns we’d need to generate 5! (120) different patterns:\n\nwords[str_detect(words, \"a.*e.*i.*o.*u\")]\n# ...\nwords[str_detect(words, \"u.*o.*i.*e.*a\")]\n\nIt’s much simpler to combine five calls to str_detect():\n\nwords[\n  str_detect(words, \"a\") &\n  str_detect(words, \"e\") &\n  str_detect(words, \"i\") &\n  str_detect(words, \"o\") &\n  str_detect(words, \"u\")\n]\n#> character(0)\n\nIn general, if you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one.\n\n16.6.3 Creating a pattern with code\nWhat if we wanted to find all sentences that mention a color? The basic idea is simple: we just combine alternation with word boundaries.\n\nstr_view(sentences, \"\\\\b(red|green|blue)\\\\b\")\n#>   [2] │ Glue the sheet to the dark <blue> background.\n#>  [26] │ Two <blue> fish swam in the tank.\n#>  [92] │ A wisp of cloud hung in the <blue> air.\n#> [148] │ The spot on the blotter was made by <green> ink.\n#> [160] │ The sofa cushion is <red> and of light weight.\n#> [174] │ The sky that morning was clear and bright <blue>.\n#> ... and 20 more\n\nBut as the number of colors grows, it would quickly get tedious to construct this pattern by hand. Wouldn’t it be nice if we could store the colors in a vector?\n\nrgb <- c(\"red\", \"green\", \"blue\")\n\nWell, we can! We’d just need to create the pattern from the vector using str_c() and str_flatten():\n\nstr_c(\"\\\\b(\", str_flatten(rgb, \"|\"), \")\\\\b\")\n#> [1] \"\\\\b(red|green|blue)\\\\b\"\n\nWe could make this pattern more comprehensive if we had a good list of colors. One place we could start from is the list of built-in colors that R can use for plots:\n\nstr_view(colors())\n#> [1] │ white\n#> [2] │ aliceblue\n#> [3] │ antiquewhite\n#> [4] │ antiquewhite1\n#> [5] │ antiquewhite2\n#> [6] │ antiquewhite3\n#> ... and 651 more\n\nBut lets first eliminate the numbered variants:\n\ncols <- colors()\ncols <- cols[!str_detect(cols, \"\\\\d\")]\nstr_view(cols)\n#> [1] │ white\n#> [2] │ aliceblue\n#> [3] │ antiquewhite\n#> [4] │ aquamarine\n#> [5] │ azure\n#> [6] │ beige\n#> ... and 137 more\n\nThen we can turn this into one giant pattern. We won’t show the pattern here because it’s huge, but you can see it working:\n\npattern <- str_c(\"\\\\b(\", str_flatten(cols, \"|\"), \")\\\\b\")\nstr_view(sentences, pattern)\n#>   [2] │ Glue the sheet to the dark <blue> background.\n#>  [12] │ A rod is used to catch <pink> <salmon>.\n#>  [26] │ Two <blue> fish swam in the tank.\n#>  [66] │ Cars and busses stalled in <snow> drifts.\n#>  [92] │ A wisp of cloud hung in the <blue> air.\n#> [112] │ Leaves turn <brown> and <yellow> in the fall.\n#> ... and 57 more\n\nIn this example, cols only contains numbers and letters so you don’t need to worry about metacharacters. But in general, whenever you create patterns from existing strings it’s wise to run them through str_escape() to ensure they match literally.\n\n16.6.4 Exercises\n\n\nFor each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.\n\nFind all words that start or end with x.\nFind all words that start with a vowel and end with a consonant.\nAre there any words that contain at least one of each different vowel?\n\n\nConstruct patterns to find evidence for and against the rule “i before e except after c”?\ncolors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and then removed the colors that are modified).\nCreate a regular expression that finds any base R dataset. You can get a list of these datasets via a special use of the data() function: data(package = \"datasets\")$results[, \"Item\"]. Note that a number of old datasets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to strip those off."
  },
  {
    "objectID": "regexps.html#regular-expressions-in-other-places",
    "href": "regexps.html#regular-expressions-in-other-places",
    "title": "16  Regular expressions",
    "section": "\n16.7 Regular expressions in other places",
    "text": "16.7 Regular expressions in other places\nJust like in the stringr and tidyr functions, there are many other places in R where you can use regular expressions. The following sections describe some other useful functions in the wider tidyverse and base R.\n\n16.7.1 tidyverse\nThere are three other particularly useful places where you might want to use a regular expressions\n\nmatches(pattern) will select all variables whose name matches the supplied pattern. It’s a “tidyselect” function that you can use anywhere in any tidyverse function that selects variables (e.g., select(), rename_with() and across()).\npivot_longer()'s names_pattern argument takes a vector of regular expressions, just like separate_wider_regex(). It’s useful when extracting data out of variable names with a complex structure\nThe delim argument in separate_longer_delim() and separate_wider_delim() usually matches a fixed string, but you can use regex() to make it match a pattern. This is useful, for example, if you want to match a comma that is optionally followed by a space, i.e. regex(\", ?\").\n\n16.7.2 Base R\napropos(pattern) searches all objects available from the global environment that match the given pattern. This is useful if you can’t quite remember the name of a function:\n\napropos(\"replace\")\n#> [1] \"%+replace%\"       \"replace\"          \"replace_na\"      \n#> [4] \"setReplaceMethod\" \"str_replace\"      \"str_replace_all\" \n#> [7] \"str_replace_na\"   \"theme_replace\"\n\nlist.files(path, pattern) lists all files in path that match a regular expression pattern. For example, you can find all the R Markdown files in the current directory with:\n\nhead(list.files(pattern = \"\\\\.Rmd$\"))\n#> character(0)\n\nIt’s worth noting that the pattern language used by base R is very slightly different to that used by stringr. That’s because stringr is built on top of the stringi package, which is in turn built on top of the ICU engine, whereas base R functions use either the TRE engine or the PCRE engine, depending on whether or not you’ve set perl = TRUE. Fortunately, the basics of regular expressions are so well established that you’ll encounter few variations when working with the patterns you’ll learn in this book. You only need to be aware of the difference when you start to rely on advanced features like complex Unicode character ranges or special features that use the (?…) syntax."
  },
  {
    "objectID": "regexps.html#summary",
    "href": "regexps.html#summary",
    "title": "16  Regular expressions",
    "section": "\n16.8 Summary",
    "text": "16.8 Summary\nWith every punctuation character potentially overloaded with meaning, regular expressions are one of the most compact languages out there. They’re definitely confusing at first but as you train your eyes to read them and your brain to understand them, you unlock a powerful skill that you can use in R and in many other places.\nIn this chapter, you’ve started your journey to become a regular expression master by learning the most useful stringr functions and the most important components of the regular expression language. And there are plenty of resources to learn more.\nA good place to start is vignette(\"regular-expressions\", package = \"stringr\"): it documents the full set of syntax supported by stringr. Another useful reference is https://www.regular-expressions.info/. It’s not R specific, but you can use it to learn about the most advanced features of regexes and how they work under the hood.\nIt’s also good to know that stringr is implemented on top of the stringi package by Marek Gagolewski. If you’re struggling to find a function that does what you need in stringr, don’t be afraid to look in stringi. You’ll find stringi very easy to pick up because it follows many of the the same conventions as stringr.\nIn the next chapter, we’ll talk about a data structure closely related to strings: factors. Factors are used to represent categorical data in R, i.e. data with a fixed and known set of possible values identified by a vector of strings."
  },
  {
    "objectID": "factors.html#introduction",
    "href": "factors.html#introduction",
    "title": "17  Factors",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nFactors are used for categorical variables, variables that have a fixed and known set of possible values. They are also useful when you want to display character vectors in a non-alphabetical order.\nWe’ll start by motivating why factors are needed for data analysis1 and how you can create them with factor(). We’ll then introduce you to the gss_cat dataset which contains a bunch of categorical variables to experiment with. You’ll then use that dataset to practice modifying the order and values of factors, before we finish up with a discussion of ordered factors.\n\n17.1.1 Prerequisites\nBase R provides some basic tools for creating and manipulating factors. We’ll supplement these with the forcats package, which is part of the core tidyverse. It provides tools for dealing with categorical variables (and it’s an anagram of factors!) using a wide range of helpers for working with factors.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "factors.html#factor-basics",
    "href": "factors.html#factor-basics",
    "title": "17  Factors",
    "section": "\n17.2 Factor basics",
    "text": "17.2 Factor basics\nImagine that you have a variable that records month:\n\nx1 <- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\n\nUsing a string to record this variable has two problems:\n\n\nThere are only twelve possible months, and there’s nothing saving you from typos:\n\nx2 <- c(\"Dec\", \"Apr\", \"Jam\", \"Mar\")\n\n\n\nIt doesn’t sort in a useful way:\n\nsort(x1)\n#> [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\n\n\n\nYou can fix both of these problems with a factor. To create a factor you must start by creating a list of the valid levels:\n\nmonth_levels <- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\n\nNow you can create a factor:\n\ny1 <- factor(x1, levels = month_levels)\ny1\n#> [1] Dec Apr Jan Mar\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nsort(y1)\n#> [1] Jan Mar Apr Dec\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nAnd any values not in the level will be silently converted to NA:\n\ny2 <- factor(x2, levels = month_levels)\ny2\n#> [1] Dec  Apr  <NA> Mar \n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nThis seems risky, so you might want to use forcats::fct() instead:\n\ny2 <- fct(x2, levels = month_levels)\n#> Error in `fct()`:\n#> ! All values of `x` must appear in `levels` or `na`\n#> ℹ Missing level: \"Jam\"\n\nIf you omit the levels, they’ll be taken from the data in alphabetical order:\n\nfactor(x1)\n#> [1] Dec Apr Jan Mar\n#> Levels: Apr Dec Jan Mar\n\nSorting alphabetically is slightly risky because not every computer will sort strings in the same way. So forcats::fct() orders by first appearance:\n\nfct(x1)\n#> [1] Dec Apr Jan Mar\n#> Levels: Dec Apr Jan Mar\n\nIf you ever need to access the set of valid levels directly, you can do so with levels():\n\nlevels(y2)\n#>  [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\nYou can also create a factor when reading your data with readr with col_factor():\n\ncsv <- \"\nmonth,value\nJan,12\nFeb,56\nMar,12\"\n\ndf <- read_csv(csv, col_types = cols(month = col_factor(month_levels)))\ndf$month\n#> [1] Jan Feb Mar\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec"
  },
  {
    "objectID": "factors.html#general-social-survey",
    "href": "factors.html#general-social-survey",
    "title": "17  Factors",
    "section": "\n17.3 General Social Survey",
    "text": "17.3 General Social Survey\nFor the rest of this chapter, we’re going to use forcats::gss_cat. It’s a sample of data from the General Social Survey, a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey has thousands of questions, so in gss_cat Hadley selected a handful that will illustrate some common challenges you’ll encounter when working with factors.\n\ngss_cat\n#> # A tibble: 21,483 × 9\n#>    year marital         age race  rincome        partyid           \n#>   <int> <fct>         <int> <fct> <fct>          <fct>             \n#> 1  2000 Never married    26 White $8000 to 9999  Ind,near rep      \n#> 2  2000 Divorced         48 White $8000 to 9999  Not str republican\n#> 3  2000 Widowed          67 White Not applicable Independent       \n#> 4  2000 Never married    39 White Not applicable Ind,near rep      \n#> 5  2000 Divorced         25 White Not applicable Not str democrat  \n#> 6  2000 Married          25 White $20000 - 24999 Strong democrat   \n#> # ℹ 21,477 more rows\n#> # ℹ 3 more variables: relig <fct>, denom <fct>, tvhours <int>\n\n(Remember, since this dataset is provided by a package, you can get more information about the variables with ?gss_cat.)\nWhen factors are stored in a tibble, you can’t see their levels so easily. One way to view them is with count():\n\ngss_cat |>\n  count(race)\n#> # A tibble: 3 × 2\n#>   race      n\n#>   <fct> <int>\n#> 1 Other  1959\n#> 2 Black  3129\n#> 3 White 16395\n\nWhen working with factors, the two most common operations are changing the order of the levels, and changing the values of the levels. Those operations are described in the sections below.\n\n17.3.1 Exercise\n\nExplore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?\nWhat is the most common relig in this survey? What’s the most common partyid?\nWhich relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?"
  },
  {
    "objectID": "factors.html#sec-modifying-factor-order",
    "href": "factors.html#sec-modifying-factor-order",
    "title": "17  Factors",
    "section": "\n17.4 Modifying factor order",
    "text": "17.4 Modifying factor order\nIt’s often useful to change the order of the factor levels in a visualization. For example, imagine you want to explore the average number of hours spent watching TV per day across religions:\n\nrelig_summary <- gss_cat |>\n  group_by(relig) |>\n  summarize(\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) + \n  geom_point()\n\n\n\n\nIt is hard to read this plot because there’s no overall pattern. We can improve it by reordering the levels of relig using fct_reorder(). fct_reorder() takes three arguments:\n\n\nf, the factor whose levels you want to modify.\n\nx, a numeric vector that you want to use to reorder the levels.\nOptionally, fun, a function that’s used if there are multiple values of x for each value of f. The default value is median.\n\n\nggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n  geom_point()\n\n\n\n\nReordering religion makes it much easier to see that people in the “Don’t know” category watch much more TV, and Hinduism & Other Eastern religions watch much less.\nAs you start making more complicated transformations, we recommend moving them out of aes() and into a separate mutate() step. For example, you could rewrite the plot above as:\n\nrelig_summary |>\n  mutate(\n    relig = fct_reorder(relig, tvhours)\n  ) |>\n  ggplot(aes(x = tvhours, y = relig)) +\n  geom_point()\n\nWhat if we create a similar plot looking at how average age varies across reported income level?\n\nrincome_summary <- gss_cat |>\n  group_by(rincome) |>\n  summarize(\n    age = mean(age, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(rincome_summary, aes(x = age, y = fct_reorder(rincome, age))) + \n  geom_point()\n\n\n\n\nHere, arbitrarily reordering the levels isn’t a good idea! That’s because rincome already has a principled order that we shouldn’t mess with. Reserve fct_reorder() for factors whose levels are arbitrarily ordered.\nHowever, it does make sense to pull “Not applicable” to the front with the other special levels. You can use fct_relevel(). It takes a factor, f, and then any number of levels that you want to move to the front of the line.\n\nggplot(rincome_summary, aes(x = age, y = fct_relevel(rincome, \"Not applicable\"))) +\n  geom_point()\n\n\n\n\nWhy do you think the average age for “Not applicable” is so high?\nAnother type of reordering is useful when you are coloring the lines on a plot. fct_reorder2(f, x, y) reorders the factor f by the y values associated with the largest x values. This makes the plot easier to read because the colors of the line at the far right of the plot will line up with the legend.\n\nby_age <- gss_cat |>\n  filter(!is.na(age)) |> \n  count(age, marital) |>\n  group_by(age) |>\n  mutate(\n    prop = n / sum(n)\n  )\n\nggplot(by_age, aes(x = age, y = prop, color = marital)) +\n  geom_line(linewidth = 1) + \n  scale_color_brewer(palette = \"Set1\")\n\nggplot(by_age, aes(x = age, y = prop, color = fct_reorder2(marital, age, prop))) +\n  geom_line(linewidth = 1) +\n  scale_color_brewer(palette = \"Set1\") + \n  labs(color = \"marital\") \n\n\n\n{fig-alt=’A line plot with age on the x-axis and proportion on the y-axis. There is one line for each category of marital status: no answer, never married, separated, divorced, widowed, and married. It is a little hard to read the plot because the order of the legend is unrelated to the lines on the plot.\n\n\nRearranging the legend makes the plot easier to read because the legend colors now match the order of the lines on the far right of the plot. You can see some unsurprising patterns: the proportion never married decreases with age, married forms an upside down U shape, and widowed starts off low but increases steeply after age 60.’ width=288}\n\n\n\n\n{fig-alt=’A line plot with age on the x-axis and proportion on the y-axis. There is one line for each category of marital status: no answer, never married, separated, divorced, widowed, and married. It is a little hard to read the plot because the order of the legend is unrelated to the lines on the plot.\n\n\nRearranging the legend makes the plot easier to read because the legend colors now match the order of the lines on the far right of the plot. You can see some unsurprising patterns: the proportion never married decreases with age, married forms an upside down U shape, and widowed starts off low but increases steeply after age 60.’ width=288}\n\n\n\n\nFinally, for bar plots, you can use fct_infreq() to order levels in decreasing frequency: this is the simplest type of reordering because it doesn’t need any extra variables. Combine it with fct_rev() if you want them in increasing frequency so that in the bar plot largest values are on the right, not the left.\n\ngss_cat |>\n  mutate(marital = marital |> fct_infreq() |> fct_rev()) |>\n  ggplot(aes(x = marital)) +\n  geom_bar()\n\n\n\n\n\n17.4.1 Exercises\n\nThere are some suspiciously high numbers in tvhours. Is the mean a good summary?\nFor each factor in gss_cat identify whether the order of the levels is arbitrary or principled.\nWhy did moving “Not applicable” to the front of the levels move it to the bottom of the plot?"
  },
  {
    "objectID": "factors.html#modifying-factor-levels",
    "href": "factors.html#modifying-factor-levels",
    "title": "17  Factors",
    "section": "\n17.5 Modifying factor levels",
    "text": "17.5 Modifying factor levels\nMore powerful than changing the orders of the levels is changing their values. This allows you to clarify labels for publication, and collapse levels for high-level displays. The most general and powerful tool is fct_recode(). It allows you to recode, or change, the value of each level. For example, take the partyid variable from the gss_cat data frame:\n\ngss_cat |> count(partyid)\n#> # A tibble: 10 × 2\n#>   partyid                n\n#>   <fct>              <int>\n#> 1 No answer            154\n#> 2 Don't know             1\n#> 3 Other party          393\n#> 4 Strong republican   2314\n#> 5 Not str republican  3032\n#> 6 Ind,near rep        1791\n#> # ℹ 4 more rows\n\nThe levels are terse and inconsistent. Let’s tweak them to be longer and use a parallel construction. Like most rename and recoding functions in the tidyverse, the new values go on the left and the old values go on the right:\n\ngss_cat |>\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\"\n    )\n  ) |>\n  count(partyid)\n#> # A tibble: 10 × 2\n#>   partyid                   n\n#>   <fct>                 <int>\n#> 1 No answer               154\n#> 2 Don't know                1\n#> 3 Other party             393\n#> 4 Republican, strong     2314\n#> 5 Republican, weak       3032\n#> 6 Independent, near rep  1791\n#> # ℹ 4 more rows\n\nfct_recode() will leave the levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.\nTo combine groups, you can assign multiple old levels to the same new level:\n\ngss_cat |>\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\",\n      \"Other\"                 = \"No answer\",\n      \"Other\"                 = \"Don't know\",\n      \"Other\"                 = \"Other party\"\n    )\n  )\n\nUse this technique with care: if you group together categories that are truly different you will end up with misleading results.\nIf you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode(). For each new variable, you can provide a vector of old levels:\n\ngss_cat |>\n  mutate(\n    partyid = fct_collapse(partyid,\n      \"other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n      \"rep\" = c(\"Strong republican\", \"Not str republican\"),\n      \"ind\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n      \"dem\" = c(\"Not str democrat\", \"Strong democrat\")\n    )\n  ) |>\n  count(partyid)\n#> # A tibble: 4 × 2\n#>   partyid     n\n#>   <fct>   <int>\n#> 1 other     548\n#> 2 rep      5346\n#> 3 ind      8409\n#> 4 dem      7180\n\nSometimes you just want to lump together the small groups to make a plot or table simpler. That’s the job of the fct_lump_*() family of functions. fct_lump_lowfreq() is a simple starting point that progressively lumps the smallest groups categories into “Other”, always keeping “Other” as the smallest category.\n\ngss_cat |>\n  mutate(relig = fct_lump_lowfreq(relig)) |>\n  count(relig)\n#> # A tibble: 2 × 2\n#>   relig          n\n#>   <fct>      <int>\n#> 1 Protestant 10846\n#> 2 Other      10637\n\nIn this case it’s not very helpful: it is true that the majority of Americans in this survey are Protestant, but we’d probably like to see some more details! Instead, we can use the fct_lump_n() to specify that we want exactly 10 groups:\n\ngss_cat |>\n  mutate(relig = fct_lump_n(relig, n = 10)) |>\n  count(relig, sort = TRUE)\n#> # A tibble: 10 × 2\n#>   relig          n\n#>   <fct>      <int>\n#> 1 Protestant 10846\n#> 2 Catholic    5124\n#> 3 None        3523\n#> 4 Christian    689\n#> 5 Other        458\n#> 6 Jewish       388\n#> # ℹ 4 more rows\n\nRead the documentation to learn about fct_lump_min() and fct_lump_prop() which are useful in other cases.\n\n17.5.1 Exercises\n\nHow have the proportions of people identifying as Democrat, Republican, and Independent changed over time?\nHow could you collapse rincome into a small set of categories?\nNotice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)"
  },
  {
    "objectID": "factors.html#sec-ordered-factors",
    "href": "factors.html#sec-ordered-factors",
    "title": "17  Factors",
    "section": "\n17.6 Ordered factors",
    "text": "17.6 Ordered factors\nBefore we go on, there’s a special type of factor that needs to be mentioned briefly: ordered factors. Ordered factors, created with ordered(), imply a strict ordering and equal distance between levels: the first level is “less than” the second level by the same amount that the second level is “less than” the third level, and so on. You can recognize them when printing because they use < between the factor levels:\n\nordered(c(\"a\", \"b\", \"c\"))\n#> [1] a b c\n#> Levels: a < b < c\n\nIn practice, ordered() factors behave very similarly to regular factors. There are only two places where you might notice different behavior:\n\nIf you map an ordered factor to color or fill in ggplot2, it will default to scale_color_viridis()/scale_fill_viridis(), a color scale that implies a ranking.\nIf you use an ordered function in a linear model, it will use “polygonal contrasts”. These are mildly useful, but you are unlikely to have heard of them unless you have a PhD in Statistics, and even then you probably don’t routinely interpret them. If you want to learn more, we recommend vignette(\"contrasts\", package = \"faux\") by Lisa DeBruine.\n\nGiven the arguable utility of these differences, we don’t generally recommend using ordered factors."
  },
  {
    "objectID": "factors.html#summary",
    "href": "factors.html#summary",
    "title": "17  Factors",
    "section": "\n17.7 Summary",
    "text": "17.7 Summary\nThis chapter introduced you to the handy forcats package for working with factors, introducing you to the most commonly used functions. forcats contains a wide range of other helpers that we didn’t have space to discuss here, so whenever you’re facing a factor analysis challenge that you haven’t encountered before, I highly recommend skimming the reference index to see if there’s a canned function that can help solve your problem.\nIf you want to learn more about factors after reading this chapter, we recommend reading Amelia McNamara and Nicholas Horton’s paper, Wrangling categorical data in R. This paper lays out some of the history discussed in stringsAsFactors: An unauthorized biography and stringsAsFactors = <sigh>, and compares the tidy approaches to categorical data outlined in this book with base R methods. An early version of the paper helped motivate and scope the forcats package; thanks Amelia & Nick!\nIn the next chapter we’ll switch gears to start learning about dates and times in R. Dates and times seem deceptively simple, but as you’ll soon see, the more you learn about them, the more complex they seem to get!"
  },
  {
    "objectID": "missing-values.html#introduction",
    "href": "missing-values.html#introduction",
    "title": "19  Missing values",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction\nYou’ve already learned the basics of missing values earlier in the book. You first saw them in ?sec-data-visualization where they resulted in a warning when making a plot as well as in ?sec-summarize where they interfered with computing summary statistics, and you learned about their infectious nature and how to check for their presence in Section 13.2.2. Now we’ll come back to them in more depth, so you can learn more of the details.\nWe’ll start by discussing some general tools for working with missing values recorded as NAs. We’ll then explore the idea of implicitly missing values, values are that are simply absent from your data, and show some tools you can use to make them explicit. We’ll finish off with a related discussion of empty groups, caused by factor levels that don’t appear in the data.\n\n19.1.1 Prerequisites\nThe functions for working with missing data mostly come from dplyr and tidyr, which are core members of the tidyverse.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "missing-values.html#explicit-missing-values",
    "href": "missing-values.html#explicit-missing-values",
    "title": "19  Missing values",
    "section": "\n19.2 Explicit missing values",
    "text": "19.2 Explicit missing values\nTo begin, let’s explore a few handy tools for creating or eliminating missing explicit values, i.e. cells where you see an NA.\n\n19.2.1 Last observation carried forward\nA common use for missing values is as a data entry convenience. When data is entered by hand, missing values sometimes indicate that the value in the previous row has been repeated (or carried forward):\n\ntreatment <- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n\nYou can fill in these missing values with tidyr::fill(). It works like select(), taking a set of columns:\n\ntreatment |>\n  fill(everything())\n#> # A tibble: 4 × 3\n#>   person           treatment response\n#>   <chr>                <dbl>    <dbl>\n#> 1 Derrick Whitmore         1        7\n#> 2 Derrick Whitmore         2       10\n#> 3 Derrick Whitmore         3       10\n#> 4 Katherine Burke          1        4\n\nThis treatment is sometimes called “last observation carried forward”, or locf for short. You can use the .direction argument to fill in missing values that have been generated in more exotic ways.\n\n19.2.2 Fixed values\nSome times missing values represent some fixed and known value, most commonly 0. You can use dplyr::coalesce() to replace them:\n\nx <- c(1, 4, 5, 7, NA)\ncoalesce(x, 0)\n#> [1] 1 4 5 7 0\n\nSometimes you’ll hit the opposite problem where some concrete value actually represents a missing value. This typically arises in data generated by older software that doesn’t have a proper way to represent missing values, so it must instead use some special value like 99 or -999.\nIf possible, handle this when reading in the data, for example, by using the na argument to readr::read_csv(), e.g., read_csv(path, na = \"99\"). If you discover the problem later, or your data source doesn’t provide a way to handle it on read, you can use dplyr::na_if():\n\nx <- c(1, 4, 5, 7, -99)\nna_if(x, -99)\n#> [1]  1  4  5  7 NA\n\n\n19.2.3 NaN\nBefore we continue, there’s one special type of missing value that you’ll encounter from time to time: a NaN (pronounced “nan”), or not a number. It’s not that important to know about because it generally behaves just like NA:\n\nx <- c(NA, NaN)\nx * 10\n#> [1]  NA NaN\nx == 1\n#> [1] NA NA\nis.na(x)\n#> [1] TRUE TRUE\n\nIn the rare case you need to distinguish an NA from a NaN, you can use is.nan(x).\nYou’ll generally encounter a NaN when you perform a mathematical operation that has an indeterminate result:\n\n0 / 0 \n#> [1] NaN\n0 * Inf\n#> [1] NaN\nInf - Inf\n#> [1] NaN\nsqrt(-1)\n#> Warning in sqrt(-1): NaNs produced\n#> [1] NaN"
  },
  {
    "objectID": "missing-values.html#sec-missing-implicit",
    "href": "missing-values.html#sec-missing-implicit",
    "title": "19  Missing values",
    "section": "\n19.3 Implicit missing values",
    "text": "19.3 Implicit missing values\nSo far we’ve talked about missing values that are explicitly missing, i.e. you can see an NA in your data. But missing values can also be implicitly missing, if an entire row of data is simply absent from the data. Let’s illustrate the difference with a simple dataset that records the price of some stock each quarter:\n\nstocks <- tibble(\n  year  = c(2020, 2020, 2020, 2020, 2021, 2021, 2021),\n  qtr   = c(   1,    2,    3,    4,    2,    3,    4),\n  price = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\n\nThis dataset has two missing observations:\n\nThe price in the fourth quarter of 2020 is explicitly missing, because its value is NA.\nThe price for the first quarter of 2021 is implicitly missing, because it simply does not appear in the dataset.\n\nOne way to think about the difference is with this Zen-like koan:\n\nAn explicit missing value is the presence of an absence.\nAn implicit missing value is the absence of a presence.\n\nSometimes you want to make implicit missings explicit in order to have something physical to work with. In other cases, explicit missings are forced upon you by the structure of the data and you want to get rid of them. The following sections discuss some tools for moving between implicit and explicit missingness.\n\n19.3.1 Pivoting\nYou’ve already seen one tool that can make implicit missings explicit and vice versa: pivoting. Making data wider can make implicit missing values explicit because every combination of the rows and new columns must have some value. For example, if we pivot stocks to put the quarter in the columns, both missing values become explicit:\n\nstocks |>\n  pivot_wider(\n    names_from = qtr, \n    values_from = price\n  )\n#> # A tibble: 2 × 5\n#>    year   `1`   `2`   `3`   `4`\n#>   <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  2020  1.88  0.59  0.35 NA   \n#> 2  2021 NA     0.92  0.17  2.66\n\nBy default, making data longer preserves explicit missing values, but if they are structurally missing values that only exist because the data is not tidy, you can drop them (make them implicit) by setting values_drop_na = TRUE. See the examples in ?sec-tidy-data for more details.\n\n19.3.2 Complete\ntidyr::complete() allows you to generate explicit missing values by providing a set of variables that define the combination of rows that should exist. For example, we know that all combinations of year and qtr should exist in the stocks data:\n\nstocks |>\n  complete(year, qtr)\n#> # A tibble: 8 × 3\n#>    year   qtr price\n#>   <dbl> <dbl> <dbl>\n#> 1  2020     1  1.88\n#> 2  2020     2  0.59\n#> 3  2020     3  0.35\n#> 4  2020     4 NA   \n#> 5  2021     1 NA   \n#> 6  2021     2  0.92\n#> # ℹ 2 more rows\n\nTypically, you’ll call complete() with names of existing variables, filling in the missing combinations. However, sometimes the individual variables are themselves incomplete, so you can instead provide your own data. For example, you might know that the stocks dataset is supposed to run from 2019 to 2021, so you could explicitly supply those values for year:\n\nstocks |>\n  complete(year = 2019:2021, qtr)\n#> # A tibble: 12 × 3\n#>    year   qtr price\n#>   <dbl> <dbl> <dbl>\n#> 1  2019     1 NA   \n#> 2  2019     2 NA   \n#> 3  2019     3 NA   \n#> 4  2019     4 NA   \n#> 5  2020     1  1.88\n#> 6  2020     2  0.59\n#> # ℹ 6 more rows\n\nIf the range of a variable is correct, but not all values are present, you could use full_seq(x, 1) to generate all values from min(x) to max(x) spaced out by 1.\nIn some cases, the complete set of observations can’t be generated by a simple combination of variables. In that case, you can do manually what complete() does for you: create a data frame that contains all the rows that should exist (using whatever combination of techniques you need), then combine it with your original dataset with dplyr::full_join().\n\n19.3.3 Joins\nThis brings us to another important way of revealing implicitly missing observations: joins. You’ll learn more about joins in Chapter 20, but we wanted to quickly mention them to you here since you can often only know that values are missing from one dataset when you compare it to another.\ndplyr::anti_join(x, y) is a particularly useful tool here because it selects only the rows in x that don’t have a match in y. For example, we can use two anti_join()s to reveal that we’re missing information for four airports and 722 planes mentioned in flights:\n\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3\n\nflights |> \n  distinct(faa = dest) |> \n  anti_join(airports)\n#> Joining with `by = join_by(faa)`\n#> # A tibble: 4 × 1\n#>   faa  \n#>   <chr>\n#> 1 BQN  \n#> 2 SJU  \n#> 3 STT  \n#> 4 PSE\n\nflights |> \n  distinct(tailnum) |> \n  anti_join(planes)\n#> Joining with `by = join_by(tailnum)`\n#> # A tibble: 722 × 1\n#>   tailnum\n#>   <chr>  \n#> 1 N3ALAA \n#> 2 N3DUAA \n#> 3 N542MQ \n#> 4 N730MQ \n#> 5 N9EAMQ \n#> 6 N532UA \n#> # ℹ 716 more rows\n\n\n19.3.4 Exercises\n\nCan you find any relationship between the carrier and the rows that appear to be missing from planes?"
  },
  {
    "objectID": "missing-values.html#factors-and-empty-groups",
    "href": "missing-values.html#factors-and-empty-groups",
    "title": "19  Missing values",
    "section": "\n19.4 Factors and empty groups",
    "text": "19.4 Factors and empty groups\nA final type of missingness is the empty group, a group that doesn’t contain any observations, which can arise when working with factors. For example, imagine we have a dataset that contains some health information about people:\n\nhealth <- tibble(\n  name   = c(\"Ikaia\", \"Oletta\", \"Leriah\", \"Dashay\", \"Tresaun\"),\n  smoker = factor(c(\"no\", \"no\", \"no\", \"no\", \"no\"), levels = c(\"yes\", \"no\")),\n  age    = c(34, 88, 75, 47, 56),\n)\n\nAnd we want to count the number of smokers with dplyr::count():\n\nhealth |> count(smoker)\n#> # A tibble: 1 × 2\n#>   smoker     n\n#>   <fct>  <int>\n#> 1 no         5\n\nThis dataset only contains non-smokers, but we know that smokers exist; the group of non-smoker is empty. We can request count() to keep all the groups, even those not seen in the data by using .drop = FALSE:\n\nhealth |> count(smoker, .drop = FALSE)\n#> # A tibble: 2 × 2\n#>   smoker     n\n#>   <fct>  <int>\n#> 1 yes        0\n#> 2 no         5\n\nThe same principle applies to ggplot2’s discrete axes, which will also drop levels that don’t have any values. You can force them to display by supplying drop = FALSE to the appropriate discrete axis:\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete()\n\nggplot(health, aes(x = smoker)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\n\n\n\n{fig-alt=’A bar chart with a single value on the x-axis, “no”.\n\n\nThe same bar chart as the last plot, but now with two values on the x-axis, “yes” and “no”. There is no bar for the “yes” category.’ width=288}\n\n\n\n\n{fig-alt=’A bar chart with a single value on the x-axis, “no”.\n\n\nThe same bar chart as the last plot, but now with two values on the x-axis, “yes” and “no”. There is no bar for the “yes” category.’ width=288}\n\n\n\n\nThe same problem comes up more generally with dplyr::group_by(). And again you can use .drop = FALSE to preserve all factor levels:\n\nhealth |> \n  group_by(smoker, .drop = FALSE) |> \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  )\n#> # A tibble: 2 × 6\n#>   smoker     n mean_age min_age max_age sd_age\n#>   <fct>  <int>    <dbl>   <dbl>   <dbl>  <dbl>\n#> 1 yes        0      NaN     Inf    -Inf   NA  \n#> 2 no         5       60      34      88   21.6\n\nWe get some interesting results here because when summarizing an empty group, the summary functions are applied to zero-length vectors. There’s an important distinction between empty vectors, which have length 0, and missing values, each of which has length 1.\n\n# A vector containing two missing values\nx1 <- c(NA, NA)\nlength(x1)\n#> [1] 2\n\n# A vector containing nothing\nx2 <- numeric()\nlength(x2)\n#> [1] 0\n\nAll summary functions work with zero-length vectors, but they may return results that are surprising at first glance. Here we see mean(age) returning NaN because mean(age) = sum(age)/length(age) which here is 0/0. max() and min() return -Inf and Inf for empty vectors so if you combine the results with a non-empty vector of new data and recompute you’ll get the minimum or maximum of the new data1.\nSometimes a simpler approach is to perform the summary and then make the implicit missings explicit with complete().\n\nhealth |> \n  group_by(smoker) |> \n  summarize(\n    n = n(),\n    mean_age = mean(age),\n    min_age = min(age),\n    max_age = max(age),\n    sd_age = sd(age)\n  ) |> \n  complete(smoker)\n#> # A tibble: 2 × 6\n#>   smoker     n mean_age min_age max_age sd_age\n#>   <fct>  <int>    <dbl>   <dbl>   <dbl>  <dbl>\n#> 1 yes       NA       NA      NA      NA   NA  \n#> 2 no         5       60      34      88   21.6\n\nThe main drawback of this approach is that you get an NA for the count, even though you know that it should be zero."
  },
  {
    "objectID": "missing-values.html#summary",
    "href": "missing-values.html#summary",
    "title": "19  Missing values",
    "section": "\n19.5 Summary",
    "text": "19.5 Summary\nMissing values are weird! Sometimes they’re recorded as an explicit NA but other times you only notice them by their absence. This chapter has given you some tools for working with explicit missing values, tools for uncovering implicit missing values, and discussed some of the ways that implicit can become explicit and vice versa.\nIn the next chapter, we tackle the final chapter in this part of the book: joins. This is a bit of a change from the chapters so far because we’re going to discuss tools that work with data frames as a whole, not something that you put inside a data frame."
  },
  {
    "objectID": "joins.html#introduction",
    "href": "joins.html#introduction",
    "title": "20  Joins",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nIt’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in. This chapter will introduce you to two important types of joins:\n\nMutating joins, which add new variables to one data frame from matching observations in another.\nFiltering joins, which filter observations from one data frame based on whether or not they match an observation in another.\n\nWe’ll begin by discussing keys, the variables used to connect a pair of data frames in a join. We cement the theory with an examination of the keys in the datasets from the nycflights13 package, then use that knowledge to start joining data frames together. Next we’ll discuss how joins work, focusing on their action on the rows. We’ll finish up with a discussion of non-equi joins, a family of joins that provide a more flexible way of matching keys than the default equality relationship.\n\n20.1.1 Prerequisites\nIn this chapter, we’ll explore the five related datasets from nycflights13 using the join functions from dplyr.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3"
  },
  {
    "objectID": "joins.html#keys",
    "href": "joins.html#keys",
    "title": "20  Joins",
    "section": "\n20.2 Keys",
    "text": "20.2 Keys\nTo understand joins, you need to first understand how two tables can be connected through a pair of keys, within each table. In this section, you’ll learn about the two types of key and see examples of both in the datasets of the nycflights13 package. You’ll also learn how to check that your keys are valid, and what to do if your table lacks a key.\n\n20.2.1 Primary and foreign keys\nEvery join involves a pair of keys: a primary key and a foreign key. A primary key is a variable or set of variables that uniquely identifies each observation. When more than one variable is needed, the key is called a compound key. For example, in nycfights13:\n\n\nairlines records two pieces of data about each airline: its carrier code and its full name. You can identify an airline with its two letter carrier code, making carrier the primary key.\n\nairlines\n#> # A tibble: 16 × 2\n#>   carrier name                    \n#>   <chr>   <chr>                   \n#> 1 9E      Endeavor Air Inc.       \n#> 2 AA      American Airlines Inc.  \n#> 3 AS      Alaska Airlines Inc.    \n#> 4 B6      JetBlue Airways         \n#> 5 DL      Delta Air Lines Inc.    \n#> 6 EV      ExpressJet Airlines Inc.\n#> # ℹ 10 more rows\n\n\n\nairports records data about each airport. You can identify each airport by its three letter airport code, making faa the primary key.\n\nairports\n#> # A tibble: 1,458 × 8\n#>   faa   name                            lat   lon   alt    tz dst  \n#>   <chr> <chr>                         <dbl> <dbl> <dbl> <dbl> <chr>\n#> 1 04G   Lansdowne Airport              41.1 -80.6  1044    -5 A    \n#> 2 06A   Moton Field Municipal Airport  32.5 -85.7   264    -6 A    \n#> 3 06C   Schaumburg Regional            42.0 -88.1   801    -6 A    \n#> 4 06N   Randall Airport                41.4 -74.4   523    -5 A    \n#> 5 09J   Jekyll Island Airport          31.1 -81.4    11    -5 A    \n#> 6 0A9   Elizabethton Municipal Airpo…  36.4 -82.2  1593    -5 A    \n#> # ℹ 1,452 more rows\n#> # ℹ 1 more variable: tzone <chr>\n\n\n\nplanes records data about each plane. You can identify a plane by its tail number, making tailnum the primary key.\n\nplanes\n#> # A tibble: 3,322 × 9\n#>   tailnum  year type              manufacturer    model     engines\n#>   <chr>   <int> <chr>             <chr>           <chr>       <int>\n#> 1 N10156   2004 Fixed wing multi… EMBRAER         EMB-145XR       2\n#> 2 N102UW   1998 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#> 3 N103US   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#> 4 N104UW   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#> 5 N10575   2002 Fixed wing multi… EMBRAER         EMB-145LR       2\n#> 6 N105UW   1999 Fixed wing multi… AIRBUS INDUSTR… A320-214        2\n#> # ℹ 3,316 more rows\n#> # ℹ 3 more variables: seats <int>, speed <int>, engine <chr>\n\n\n\nweather records data about the weather at the origin airports. You can identify each observation by the combination of location and time, making origin and time_hour the compound primary key.\n\nweather\n#> # A tibble: 26,115 × 15\n#>   origin  year month   day  hour  temp  dewp humid wind_dir\n#>   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>\n#> 1 EWR     2013     1     1     1  39.0  26.1  59.4      270\n#> 2 EWR     2013     1     1     2  39.0  27.0  61.6      250\n#> 3 EWR     2013     1     1     3  39.0  28.0  64.4      240\n#> 4 EWR     2013     1     1     4  39.9  28.0  62.2      250\n#> 5 EWR     2013     1     1     5  39.0  28.0  64.4      260\n#> 6 EWR     2013     1     1     6  37.9  28.0  67.2      240\n#> # ℹ 26,109 more rows\n#> # ℹ 6 more variables: wind_speed <dbl>, wind_gust <dbl>, …\n\n\n\nA foreign key is a variable (or set of variables) that corresponds to a primary key in another table. For example:\n\n\nflights$tailnum is a foreign key that corresponds to the primary key planes$tailnum.\n\nflights$carrier is a foreign key that corresponds to the primary key airlines$carrier.\n\nflights$origin is a foreign key that corresponds to the primary key airports$faa.\n\nflights$dest is a foreign key that corresponds to the primary key airports$faa.\n\nflights$origin-flights$time_hour is a compound foreign key that corresponds to the compound primary key weather$origin-weather$time_hour.\n\nThese relationships are summarized visually in Figure 20.1.\n\n\n\n\nFigure 20.1: Connections between all five data frames in the nycflights13 package. Variables making up a primary key are colored grey, and are connected to their corresponding foreign keys with arrows.\n\n\n\n\nYou’ll notice a nice feature in the design of these keys: the primary and foreign keys almost always have the same names, which, as you’ll see shortly, will make your joining life much easier. It’s also worth noting the opposite relationship: almost every variable name used in multiple tables has the same meaning in each place. There’s only one exception: year means year of departure in flights and year of manufacturer in planes. This will become important when we start actually joining tables together.\n\n20.2.2 Checking primary keys\nNow that that we’ve identified the primary keys in each table, it’s good practice to verify that they do indeed uniquely identify each observation. One way to do that is to count() the primary keys and look for entries where n is greater than one. This reveals that planes and weather both look good:\n\nplanes |> \n  count(tailnum) |> \n  filter(n > 1)\n#> # A tibble: 0 × 2\n#> # ℹ 2 variables: tailnum <chr>, n <int>\n\nweather |> \n  count(time_hour, origin) |> \n  filter(n > 1)\n#> # A tibble: 0 × 3\n#> # ℹ 3 variables: time_hour <dttm>, origin <chr>, n <int>\n\nYou should also check for missing values in your primary keys — if a value is missing then it can’t identify an observation!\n\nplanes |> \n  filter(is.na(tailnum))\n#> # A tibble: 0 × 9\n#> # ℹ 9 variables: tailnum <chr>, year <int>, type <chr>, manufacturer <chr>,\n#> #   model <chr>, engines <int>, seats <int>, speed <int>, engine <chr>\n\nweather |> \n  filter(is.na(time_hour) | is.na(origin))\n#> # A tibble: 0 × 15\n#> # ℹ 15 variables: origin <chr>, year <int>, month <int>, day <int>,\n#> #   hour <int>, temp <dbl>, dewp <dbl>, humid <dbl>, wind_dir <dbl>, …\n\n\n20.2.3 Surrogate keys\nSo far we haven’t talked about the primary key for flights. It’s not super important here, because there are no data frames that use it as a foreign key, but it’s still useful to consider because it’s easier to work with observations if we have some way to describe them to others.\nAfter a little thinking and experimentation, we determined that there are three variables that together uniquely identify each flight:\n\nflights |> \n  count(time_hour, carrier, flight) |> \n  filter(n > 1)\n#> # A tibble: 0 × 4\n#> # ℹ 4 variables: time_hour <dttm>, carrier <chr>, flight <int>, n <int>\n\nDoes the absence of duplicates automatically make time_hour-carrier-flight a primary key? It’s certainly a good start, but it doesn’t guarantee it. For example, are altitude and latitude a good primary key for airports?\n\nairports |>\n  count(alt, lat) |> \n  filter(n > 1)\n#> # A tibble: 1 × 3\n#>     alt   lat     n\n#>   <dbl> <dbl> <int>\n#> 1    13  40.6     2\n\nIdentifying an airport by its altitude and latitude is clearly a bad idea, and in general it’s not possible to know from the data alone whether or not a combination of variables makes a good a primary key. But for flights, the combination of time_hour, carrier, and flight seems reasonable because it would be really confusing for an airline and its customers if there were multiple flights with the same flight number in the air at the same time.\nThat said, we might be better off introducing a simple numeric surrogate key using the row number:\n\nflights2 <- flights |> \n  mutate(id = row_number(), .before = 1)\nflights2\n#> # A tibble: 336,776 × 20\n#>      id  year month   day dep_time sched_dep_time dep_delay arr_time\n#>   <int> <int> <int> <int>    <int>          <int>     <dbl>    <int>\n#> 1     1  2013     1     1      517            515         2      830\n#> 2     2  2013     1     1      533            529         4      850\n#> 3     3  2013     1     1      542            540         2      923\n#> 4     4  2013     1     1      544            545        -1     1004\n#> 5     5  2013     1     1      554            600        -6      812\n#> 6     6  2013     1     1      554            558        -4      740\n#> # ℹ 336,770 more rows\n#> # ℹ 12 more variables: sched_arr_time <int>, arr_delay <dbl>, …\n\nSurrogate keys can be particular useful when communicating to other humans: it’s much easier to tell someone to take a look at flight 2001 than to say look at UA430 which departed 9am 2013-01-03.\n\n20.2.4 Exercises\n\nWe forgot to draw the relationship between weather and airports in Figure 20.1. What is the relationship and how should it appear in the diagram?\nweather only contains information for the three origin airports in NYC. If it contained weather records for all airports in the USA, what additional connection would it make to flights?\nThe year, month, day, hour, and origin variables almost form a compound key for weather, but there’s one hour that has duplicate observations. Can you figure out what’s special about that hour?\nWe know that some days of the year are special and fewer people than usual fly on them (e.g., Christmas eve and Christmas day). How might you represent that data as a data frame? What would be the primary key? How would it connect to the existing data frames?\nDraw a diagram illustrating the connections between the Batting, People, and Salaries data frames in the Lahman package. Draw another diagram that shows the relationship between People, Managers, AwardsManagers. How would you characterize the relationship between the Batting, Pitching, and Fielding data frames?"
  },
  {
    "objectID": "joins.html#sec-mutating-joins",
    "href": "joins.html#sec-mutating-joins",
    "title": "20  Joins",
    "section": "\n20.3 Basic joins",
    "text": "20.3 Basic joins\nNow that you understand how data frames are connected via keys, we can start using joins to better understand the flights dataset. dplyr provides six join functions: left_join(), inner_join(), right_join(), full_join(), semi_join(), and anti_join(). They all have the same interface: they take a pair of data frames (x and y) and return a data frame. The order of the rows and columns in the output is primarily determined by x.\nIn this section, you’ll learn how to use one mutating join, left_join(), and two filtering joins, semi_join() and anti_join(). In the next section, you’ll learn exactly how these functions work, and about the remaining inner_join(), right_join() and full_join().\n\n20.3.1 Mutating joins\nA mutating join allows you to combine variables from two data frames: it first matches observations by their keys, then copies across variables from one data frame to the other. Like mutate(), the join functions add variables to the right, so if your dataset has many variables, you won’t see the new ones. For these examples, we’ll make it easier to see what’s going on by creating a narrower dataset with just six variables1:\n\nflights2 <- flights |> \n  select(year, time_hour, origin, dest, tailnum, carrier)\nflights2\n#> # A tibble: 336,776 × 6\n#>    year time_hour           origin dest  tailnum carrier\n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>  \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA     \n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA     \n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA     \n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6     \n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL     \n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA     \n#> # ℹ 336,770 more rows\n\nThere are four types of mutating join, but there’s one that you’ll use almost all of the time: left_join(). It’s special because the output will always have the same rows as x2. The primary use of left_join() is to add in additional metadata. For example, we can use left_join() to add the full airline name to the flights2 data:\n\nflights2 |>\n  left_join(airlines)\n#> Joining with `by = join_by(carrier)`\n#> # A tibble: 336,776 × 7\n#>    year time_hour           origin dest  tailnum carrier name                \n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr>               \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      United Air Lines In…\n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      United Air Lines In…\n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      American Airlines I…\n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      JetBlue Airways     \n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Delta Air Lines Inc.\n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      United Air Lines In…\n#> # ℹ 336,770 more rows\n\nOr we could find out the temperature and wind speed when each plane departed:\n\nflights2 |> \n  left_join(weather |> select(origin, time_hour, temp, wind_speed))\n#> Joining with `by = join_by(time_hour, origin)`\n#> # A tibble: 336,776 × 8\n#>    year time_hour           origin dest  tailnum carrier  temp wind_speed\n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <dbl>      <dbl>\n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA       39.0       12.7\n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA       39.9       15.0\n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA       39.0       15.0\n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6       39.0       15.0\n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL       39.9       16.1\n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA       39.0       12.7\n#> # ℹ 336,770 more rows\n\nOr what size of plane was flying:\n\nflights2 |> \n  left_join(planes |> select(tailnum, type, engines, seats))\n#> Joining with `by = join_by(tailnum)`\n#> # A tibble: 336,776 × 9\n#>    year time_hour           origin dest  tailnum carrier type                \n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr>               \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Fixed wing multi en…\n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      Fixed wing multi en…\n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Fixed wing multi en…\n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      Fixed wing multi en…\n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Fixed wing multi en…\n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Fixed wing multi en…\n#> # ℹ 336,770 more rows\n#> # ℹ 2 more variables: engines <int>, seats <int>\n\nWhen left_join() fails to find a match for a row in x, it fills in the new variables with missing values. For example, there’s no information about the plane with tail number N3ALAA so the type, engines, and seats will be missing:\n\nflights2 |> \n  filter(tailnum == \"N3ALAA\") |> \n  left_join(planes |> select(tailnum, type, engines, seats))\n#> Joining with `by = join_by(tailnum)`\n#> # A tibble: 63 × 9\n#>    year time_hour           origin dest  tailnum carrier type  engines seats\n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr>   <int> <int>\n#> 1  2013 2013-01-01 06:00:00 LGA    ORD   N3ALAA  AA      <NA>       NA    NA\n#> 2  2013 2013-01-02 18:00:00 LGA    ORD   N3ALAA  AA      <NA>       NA    NA\n#> 3  2013 2013-01-03 06:00:00 LGA    ORD   N3ALAA  AA      <NA>       NA    NA\n#> 4  2013 2013-01-07 19:00:00 LGA    ORD   N3ALAA  AA      <NA>       NA    NA\n#> 5  2013 2013-01-08 17:00:00 JFK    ORD   N3ALAA  AA      <NA>       NA    NA\n#> 6  2013 2013-01-16 06:00:00 LGA    ORD   N3ALAA  AA      <NA>       NA    NA\n#> # ℹ 57 more rows\n\nWe’ll come back to this problem a few times in the rest of the chapter.\n\n20.3.2 Specifying join keys\nBy default, left_join() will use all variables that appear in both data frames as the join key, the so called natural join. This is a useful heuristic, but it doesn’t always work. For example, what happens if we try to join flights2 with the complete planes dataset?\n\nflights2 |> \n  left_join(planes)\n#> Joining with `by = join_by(year, tailnum)`\n#> # A tibble: 336,776 × 13\n#>    year time_hour           origin dest  tailnum carrier type  manufacturer\n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr> <chr>       \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      <NA>  <NA>        \n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      <NA>  <NA>        \n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      <NA>  <NA>        \n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      <NA>  <NA>        \n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      <NA>  <NA>        \n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      <NA>  <NA>        \n#> # ℹ 336,770 more rows\n#> # ℹ 5 more variables: model <chr>, engines <int>, seats <int>, …\n\nWe get a lot of missing matches because our join is trying to use tailnum and year as a compound key. Both flights and planes have a year column but they mean different things: flights$year is the year the flight occurred and planes$year is the year the plane was built. We only want to join on tailnum so we need to provide an explicit specification with join_by():\n\nflights2 |> \n  left_join(planes, join_by(tailnum))\n#> # A tibble: 336,776 × 14\n#>   year.x time_hour           origin dest  tailnum carrier year.y\n#>    <int> <dttm>              <chr>  <chr> <chr>   <chr>    <int>\n#> 1   2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA        1999\n#> 2   2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA        1998\n#> 3   2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA        1990\n#> 4   2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6        2012\n#> 5   2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL        1991\n#> 6   2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA        2012\n#> # ℹ 336,770 more rows\n#> # ℹ 7 more variables: type <chr>, manufacturer <chr>, model <chr>, …\n\nNote that the year variables are disambiguated in the output with a suffix (year.x and year.y), which tells you whether the variable came from the x or y argument. You can override the default suffixes with the suffix argument.\njoin_by(tailnum) is short for join_by(tailnum == tailnum). It’s important to know about this fuller form for two reasons. Firstly, it describes the relationship between the two tables: the keys must be equal. That’s why this type of join is often called an equi join. You’ll learn about non-equi joins in Section 20.5.\nSecondly, it’s how you specify different join keys in each table. For example, there are two ways to join the flight2 and airports table: either by dest or origin:\n\nflights2 |> \n  left_join(airports, join_by(dest == faa))\n#> # A tibble: 336,776 × 13\n#>    year time_hour           origin dest  tailnum carrier name                \n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr>               \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      George Bush Interco…\n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      George Bush Interco…\n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      Miami Intl          \n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      <NA>                \n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      Hartsfield Jackson …\n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Chicago Ohare Intl  \n#> # ℹ 336,770 more rows\n#> # ℹ 6 more variables: lat <dbl>, lon <dbl>, alt <dbl>, tz <dbl>, …\n\nflights2 |> \n  left_join(airports, join_by(origin == faa))\n#> # A tibble: 336,776 × 13\n#>    year time_hour           origin dest  tailnum carrier name               \n#>   <int> <dttm>              <chr>  <chr> <chr>   <chr>   <chr>              \n#> 1  2013 2013-01-01 05:00:00 EWR    IAH   N14228  UA      Newark Liberty Intl\n#> 2  2013 2013-01-01 05:00:00 LGA    IAH   N24211  UA      La Guardia         \n#> 3  2013 2013-01-01 05:00:00 JFK    MIA   N619AA  AA      John F Kennedy Intl\n#> 4  2013 2013-01-01 05:00:00 JFK    BQN   N804JB  B6      John F Kennedy Intl\n#> 5  2013 2013-01-01 06:00:00 LGA    ATL   N668DN  DL      La Guardia         \n#> 6  2013 2013-01-01 05:00:00 EWR    ORD   N39463  UA      Newark Liberty Intl\n#> # ℹ 336,770 more rows\n#> # ℹ 6 more variables: lat <dbl>, lon <dbl>, alt <dbl>, tz <dbl>, …\n\nIn older code you might see a different way of specifying the join keys, using a character vector:\n\n\nby = \"x\" corresponds to join_by(x).\n\nby = c(\"a\" = \"x\") corresponds to join_by(a == x).\n\nNow that it exists, we prefer join_by() since it provides a clearer and more flexible specification.\ninner_join(), right_join(), full_join() have the same interface as left_join(). The difference is which rows they keep: left join keeps all the rows in x, the right join keeps all rows in y, the full join keeps all rows in either x or y, and the inner join only keeps rows that occur in both x and y. We’ll come back to these in more detail later.\n\n20.3.3 Filtering joins\nAs you might guess the primary action of a filtering join is to filter the rows. There are two types: semi-joins and anti-joins. Semi-joins keep all rows in x that have a match in y. For example, we could use a semi-join to filter the airports dataset to show just the origin airports:\n\nairports |> \n  semi_join(flights2, join_by(faa == origin))\n#> # A tibble: 3 × 8\n#>   faa   name                  lat   lon   alt    tz dst   tzone           \n#>   <chr> <chr>               <dbl> <dbl> <dbl> <dbl> <chr> <chr>           \n#> 1 EWR   Newark Liberty Intl  40.7 -74.2    18    -5 A     America/New_York\n#> 2 JFK   John F Kennedy Intl  40.6 -73.8    13    -5 A     America/New_York\n#> 3 LGA   La Guardia           40.8 -73.9    22    -5 A     America/New_York\n\nOr just the destinations:\n\nairports |> \n  semi_join(flights2, join_by(faa == dest))\n#> # A tibble: 101 × 8\n#>   faa   name                     lat    lon   alt    tz dst   tzone          \n#>   <chr> <chr>                  <dbl>  <dbl> <dbl> <dbl> <chr> <chr>          \n#> 1 ABQ   Albuquerque Internati…  35.0 -107.   5355    -7 A     America/Denver \n#> 2 ACK   Nantucket Mem           41.3  -70.1    48    -5 A     America/New_Yo…\n#> 3 ALB   Albany Intl             42.7  -73.8   285    -5 A     America/New_Yo…\n#> 4 ANC   Ted Stevens Anchorage…  61.2 -150.    152    -9 A     America/Anchor…\n#> 5 ATL   Hartsfield Jackson At…  33.6  -84.4  1026    -5 A     America/New_Yo…\n#> 6 AUS   Austin Bergstrom Intl   30.2  -97.7   542    -6 A     America/Chicago\n#> # ℹ 95 more rows\n\nAnti-joins are the opposite: they return all rows in x that don’t have a match in y. They’re useful for finding missing values that are implicit in the data, the topic of Section 19.3. Implicitly missing values don’t show up as NAs but instead only exist as an absence. For example, we can find rows that are missing from airports by looking for flights that don’t have a matching destination airport:\n\nflights2 |> \n  anti_join(airports, join_by(dest == faa)) |> \n  distinct(dest)\n#> # A tibble: 4 × 1\n#>   dest \n#>   <chr>\n#> 1 BQN  \n#> 2 SJU  \n#> 3 STT  \n#> 4 PSE\n\nOr we can find which tailnums are missing from planes:\n\nflights2 |>\n  anti_join(planes, join_by(tailnum)) |> \n  distinct(tailnum)\n#> # A tibble: 722 × 1\n#>   tailnum\n#>   <chr>  \n#> 1 N3ALAA \n#> 2 N3DUAA \n#> 3 N542MQ \n#> 4 N730MQ \n#> 5 N9EAMQ \n#> 6 N532UA \n#> # ℹ 716 more rows\n\n\n20.3.4 Exercises\n\nFind the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns?\n\nImagine you’ve found the top 10 most popular destinations using this code:\n\ntop_dest <- flights2 |>\n  count(dest, sort = TRUE) |>\n  head(10)\n\nHow can you find all flights to those destinations?\n\nDoes every departing flight have corresponding weather data for that hour?\nWhat do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)\nAdd a column to planes that lists every carrier that has flown that plane. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned in previous chapters.\nAdd the latitude and the longitude of the origin and destination airport to flights. Is it easier to rename the columns before or after the join?\n\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:\n\nairports |>\n  semi_join(flights, join_by(faa == dest)) |>\n  ggplot(aes(x = lon, y = lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n\nYou might want to use the size or color of the points to display the average delay for each airport.\n\nWhat happened on June 13 2013? Draw a map of the delays, and then use Google to cross-reference with the weather."
  },
  {
    "objectID": "joins.html#how-do-joins-work",
    "href": "joins.html#how-do-joins-work",
    "title": "20  Joins",
    "section": "\n20.4 How do joins work?",
    "text": "20.4 How do joins work?\nNow that you’ve used joins a few times it’s time to learn more about how they work, focusing on how each row in x matches rows in y. We’ll begin by introducing a visual representation of joins, using the simple tibbles defined below and shown in Figure 20.2. In these examples we’ll use a single key called key and a single value column (val_x and val_y), but the ideas all generalize to multiple keys and multiple values.\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     3, \"x3\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     4, \"y3\"\n)\n\n\n\n\n\nFigure 20.2: Graphical representation of two simple tables. The colored key columns map background color to key value. The grey columns represent the “value” columns that are carried along for the ride.\n\n\n\n\nFigure 20.3 introduces the foundation for our visual representation. It shows all potential matches between x and y as the intersection between lines drawn from each row of x and each row of y. The rows and columns in the output are primarily determined by x, so the x table is horizontal and lines up with the output.\n\n\n\n\nFigure 20.3: To understand how joins work, it’s useful to think of every possible match. Here we show that with a grid of connecting lines.\n\n\n\n\nTo describe a specific type of join, we indicate matches with dots. The matches determine the rows in the output, a new data frame that contains the key, the x values, and the y values. For example, Figure 20.4 shows an inner join, where rows are retained if and only if the keys are equal.\n\n\n\n\nFigure 20.4: An inner join matches each row in x to the row in y that has the same value of key. Each match becomes a row in the output.\n\n\n\n\nWe can apply the same principles to explain the outer joins, which keep observations that appear in at least one of the data frames. These joins work by adding an additional “virtual” observation to each data frame. This observation has a key that matches if no other key matches, and values filled with NA. There are three types of outer joins:\n\n\nA left join keeps all observations in x, Figure 20.5. Every row of x is preserved in the output because it can fall back to matching a row of NAs in y.\n\n\n\n\nFigure 20.5: A visual representation of the left join where every row in x appears in the output.\n\n\n\n\n\n\nA right join keeps all observations in y, Figure 20.6. Every row of y is preserved in the output because it can fall back to matching a row of NAs in x. The output still matches x as much as possible; any extra rows from y are added to the end.\n\n\n\n\nFigure 20.6: A visual representation of the right join where every row of y appears in the output.\n\n\n\n\n\n\nA full join keeps all observations that appear in x or y, Figure 20.7. Every row of x and y is included in the output because both x and y have a fall back row of NAs. Again, the output starts with all rows from x, followed by the remaining unmatched y rows.\n\n\n\n\nFigure 20.7: A visual representation of the full join where every row in x and y appears in the output.\n\n\n\n\n\n\nAnother way to show how the types of outer join differ is with a Venn diagram, as in Figure 20.8. However, this is not a great representation because while it might jog your memory about which rows are preserved, it fails to illustrate what’s happening with the columns.\n\n\n\n\nFigure 20.8: Venn diagrams showing the difference between inner, left, right, and full joins.\n\n\n\n\nThe joins shown here are the so-called equi joins, where rows match if the keys are equal. Equi joins are the most common type of join, so we’ll typically omit the equi prefix, and just say “inner join” rather than “equi inner join”. We’ll come back to non-equi joins in Section 20.5.\n\n20.4.1 Row matching\nSo far we’ve explored what happens if a row in x matches zero or one rows in y. What happens if it matches more than one row? To understand what’s going let’s first narrow our focus to the inner_join() and then draw a picture, Figure 20.9.\n\n\n\n\nFigure 20.9: The three ways a row in x can match. x1 matches one row in y, x2 matches two rows in y, x3 matches zero rows in y. Note that while there are three rows in x and three rows in the output, there isn’t a direct correspondence between the rows.\n\n\n\n\nThere are three possible outcomes for a row in x:\n\nIf it doesn’t match anything, it’s dropped.\nIf it matches 1 row in y, it’s preserved.\nIf it matches more than 1 row in y, it’s duplicated once for each match.\n\nIn principle, this means that there’s no guaranteed correspondence between the rows in the output and the rows in x, but in practice, this rarely causes problems. There is, however, one particularly dangerous case which can cause a combinatorial explosion of rows. Imagine joining the following two tables:\n\ndf1 <- tibble(key = c(1, 2, 2), val_x = c(\"x1\", \"x2\", \"x3\"))\ndf2 <- tibble(key = c(1, 2, 2), val_y = c(\"y1\", \"y2\", \"y3\"))\n\nWhile the first row in df1 only matches one row in df2, the second and third rows both match two rows. This is sometimes called a many-to-many join, and will cause dplyr to emit a warning:\n\ndf1 |> \n  inner_join(df2, join_by(key))\n#> Warning in inner_join(df1, df2, join_by(key)): Detected an unexpected many-to-many relationship between `x` and `y`.\n#> ℹ Row 2 of `x` matches multiple rows in `y`.\n#> ℹ Row 2 of `y` matches multiple rows in `x`.\n#> ℹ If a many-to-many relationship is expected, set `relationship =\n#>   \"many-to-many\"` to silence this warning.\n#> # A tibble: 5 × 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x2    y3   \n#> 4     2 x3    y2   \n#> 5     2 x3    y3\n\nIf you are doing this deliberately, you can set relationship = \"many-to-many\", as the warning suggests.\n\n20.4.2 Filtering joins\nThe number of matches also determines the behavior of the filtering joins. The semi-join keeps rows in x that have one or more matches in y, as in Figure 20.10. The anti-join keeps rows in x that match zero rows in y, as in Figure 20.11. In both cases, only the existence of a match is important; it doesn’t matter how many times it matches. This means that filtering joins never duplicate rows like mutating joins do.\n\n\n\n\nFigure 20.10: In a semi-join it only matters that there is a match; otherwise values in y don’t affect the output.\n\n\n\n\n\n\n\n\nFigure 20.11: An anti-join is the inverse of a semi-join, dropping rows from x that have a match in y."
  },
  {
    "objectID": "joins.html#sec-non-equi-joins",
    "href": "joins.html#sec-non-equi-joins",
    "title": "20  Joins",
    "section": "\n20.5 Non-equi joins",
    "text": "20.5 Non-equi joins\nSo far you’ve only seen equi joins, joins where the rows match if the x key equals the y key. Now we’re going to relax that restriction and discuss other ways of determining if a pair of rows match.\nBut before we can do that, we need to revisit a simplification we made above. In equi joins the x keys and y are always equal, so we only need to show one in the output. We can request that dplyr keep both keys with keep = TRUE, leading to the code below and the re-drawn inner_join() in Figure 20.12.\n\nx |> left_join(y, by = \"key\", keep = TRUE)\n#> # A tibble: 3 × 4\n#>   key.x val_x key.y val_y\n#>   <dbl> <chr> <dbl> <chr>\n#> 1     1 x1        1 y1   \n#> 2     2 x2        2 y2   \n#> 3     3 x3       NA <NA>\n\n\n\n\n\nFigure 20.12: An inner join showing both x and y keys in the output.\n\n\n\n\nWhen we move away from equi joins we’ll always show the keys, because the key values will often be different. For example, instead of matching only when the x$key and y$key are equal, we could match whenever the x$key is greater than or equal to the y$key, leading to Figure 20.13. dplyr’s join functions understand this distinction equi and non-equi joins so will always show both keys when you perform a non-equi join.\n\n\n\n\nFigure 20.13: A non-equi join where the x key must be greater than or equal to the y key. Many rows generate multiple matches.\n\n\n\n\nNon-equi join isn’t a particularly useful term because it only tells you what the join is not, not what it is. dplyr helps by identifying four particularly useful types of non-equi join:\n\n\nCross joins match every pair of rows.\n\nInequality joins use <, <=, >, and >= instead of ==.\n\nRolling joins are similar to inequality joins but only find the closest match.\n\nOverlap joins are a special type of inequality join designed to work with ranges.\n\nEach of these is described in more detail in the following sections.\n\n20.5.1 Cross joins\nA cross join matches everything, as in Figure 20.14, generating the Cartesian product of rows. This means the output will have nrow(x) * nrow(y) rows.\n\n\n\n\nFigure 20.14: A cross join matches each row in x with every row in y.\n\n\n\n\nCross joins are useful when generating permutations. For example, the code below generates every possible pair of names. Since we’re joining df to itself, this is sometimes called a self-join. Cross joins use a different join function because there’s no distinction between inner/left/right/full when you’re matching every row.\n\ndf <- tibble(name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\ndf |> cross_join(df)\n#> # A tibble: 16 × 2\n#>   name.x name.y\n#>   <chr>  <chr> \n#> 1 John   John  \n#> 2 John   Simon \n#> 3 John   Tracy \n#> 4 John   Max   \n#> 5 Simon  John  \n#> 6 Simon  Simon \n#> # ℹ 10 more rows\n\n\n20.5.2 Inequality joins\nInequality joins use <, <=, >=, or > to restrict the set of possible matches, as in Figure 20.13 and Figure 20.15.\n\n\n\n\nFigure 20.15: An inequality join where x is joined to y on rows where the key of x is less than the key of y. This makes a triangular shape in the top-left corner.\n\n\n\n\nInequality joins are extremely general, so general that it’s hard to come up with meaningful specific use cases. One small useful technique is to use them to restrict the cross join so that instead of generating all permutations, we generate all combinations:\n\ndf <- tibble(id = 1:4, name = c(\"John\", \"Simon\", \"Tracy\", \"Max\"))\n\ndf |> left_join(df, join_by(id < id))\n#> # A tibble: 7 × 4\n#>    id.x name.x  id.y name.y\n#>   <int> <chr>  <int> <chr> \n#> 1     1 John       2 Simon \n#> 2     1 John       3 Tracy \n#> 3     1 John       4 Max   \n#> 4     2 Simon      3 Tracy \n#> 5     2 Simon      4 Max   \n#> 6     3 Tracy      4 Max   \n#> # ℹ 1 more row\n\n\n20.5.3 Rolling joins\nRolling joins are a special type of inequality join where instead of getting every row that satisfies the inequality, you get just the closest row, as in Figure 20.16. You can turn any inequality join into a rolling join by adding closest(). For example join_by(closest(x <= y)) matches the smallest y that’s greater than or equal to x, and join_by(closest(x > y)) matches the biggest y that’s less than x.\n\n\n\n\nFigure 20.16: A rolling join is similar to a greater-than-or-equal inequality join but only matches the first value.\n\n\n\n\nRolling joins are particularly useful when you have two tables of dates that don’t perfectly line up and you want to find (e.g.) the closest date in table 1 that comes before (or after) some date in table 2.\nFor example, imagine that you’re in charge of the party planning commission for your office. Your company is rather cheap so instead of having individual parties, you only have a party once each quarter. The rules for determining when a party will be held are a little complex: parties are always on a Monday, you skip the first week of January since a lot of people are on holiday, and the first Monday of Q3 2022 is July 4, so that has to be pushed back a week. That leads to the following party days:\n\nparties <- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\"))\n)\n\nNow imagine that you have a table of employee birthdays:\n\nemployees <- tibble(\n  name = sample(babynames::babynames$name, 100),\n  birthday = ymd(\"2022-01-01\") + (sample(365, 100, replace = TRUE) - 1)\n)\nemployees\n#> # A tibble: 100 × 2\n#>   name    birthday  \n#>   <chr>   <date>    \n#> 1 Case    2022-09-13\n#> 2 Shonnie 2022-03-30\n#> 3 Burnard 2022-01-10\n#> 4 Omer    2022-11-25\n#> 5 Hillel  2022-07-30\n#> 6 Curlie  2022-12-11\n#> # ℹ 94 more rows\n\nAnd for each employee we want to find the first party date that comes after (or on) their birthday. We can express that with a rolling join:\n\nemployees |> \n  left_join(parties, join_by(closest(birthday >= party)))\n#> # A tibble: 100 × 4\n#>   name    birthday       q party     \n#>   <chr>   <date>     <int> <date>    \n#> 1 Case    2022-09-13     3 2022-07-11\n#> 2 Shonnie 2022-03-30     1 2022-01-10\n#> 3 Burnard 2022-01-10     1 2022-01-10\n#> 4 Omer    2022-11-25     4 2022-10-03\n#> 5 Hillel  2022-07-30     3 2022-07-11\n#> 6 Curlie  2022-12-11     4 2022-10-03\n#> # ℹ 94 more rows\n\nThere is, however, one problem with this approach: the folks with birthdays before January 10 don’t get a party:\n\nemployees |> \n  anti_join(parties, join_by(closest(birthday >= party)))\n#> # A tibble: 0 × 2\n#> # ℹ 2 variables: name <chr>, birthday <date>\n\nTo resolve that issue we’ll need to tackle the problem a different way, with overlap joins.\n\n20.5.4 Overlap joins\nOverlap joins provide three helpers that use inequality joins to make it easier to work with intervals:\n\n\nbetween(x, y_lower, y_upper) is short for x >= y_lower, x <= y_upper.\n\nwithin(x_lower, x_upper, y_lower, y_upper) is short for x_lower >= y_lower, x_upper <= y_upper.\n\noverlaps(x_lower, x_upper, y_lower, y_upper) is short for x_lower <= y_upper, x_upper >= y_lower.\n\nLet’s continue the birthday example to see how you might use them. There’s one problem with the strategy we used above: there’s no party preceding the birthdays Jan 1-9. So it might be better to be explicit about the date ranges that each party spans, and make a special case for those early birthdays:\n\nparties <- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-11\", \"2022-10-02\", \"2022-12-31\"))\n)\nparties\n#> # A tibble: 4 × 4\n#>       q party      start      end       \n#>   <int> <date>     <date>     <date>    \n#> 1     1 2022-01-10 2022-01-01 2022-04-03\n#> 2     2 2022-04-04 2022-04-04 2022-07-11\n#> 3     3 2022-07-11 2022-07-11 2022-10-02\n#> 4     4 2022-10-03 2022-10-03 2022-12-31\n\nHadley is hopelessly bad at data entry so he also wanted to check that the party periods don’t overlap. One way to do this is by using a self-join to check if any start-end interval overlap with another:\n\nparties |> \n  inner_join(parties, join_by(overlaps(start, end, start, end), q < q)) |> \n  select(start.x, end.x, start.y, end.y)\n#> # A tibble: 1 × 4\n#>   start.x    end.x      start.y    end.y     \n#>   <date>     <date>     <date>     <date>    \n#> 1 2022-04-04 2022-07-11 2022-07-11 2022-10-02\n\nOoops, there is an overlap, so let’s fix that problem and continue:\n\nparties <- tibble(\n  q = 1:4,\n  party = ymd(c(\"2022-01-10\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  start = ymd(c(\"2022-01-01\", \"2022-04-04\", \"2022-07-11\", \"2022-10-03\")),\n  end = ymd(c(\"2022-04-03\", \"2022-07-10\", \"2022-10-02\", \"2022-12-31\"))\n)\n\nNow we can match each employee to their party. This is a good place to use unmatched = \"error\" because we want to quickly find out if any employees didn’t get assigned a party.\n\nemployees |> \n  inner_join(parties, join_by(between(birthday, start, end)), unmatched = \"error\")\n#> # A tibble: 100 × 6\n#>   name    birthday       q party      start      end       \n#>   <chr>   <date>     <int> <date>     <date>     <date>    \n#> 1 Case    2022-09-13     3 2022-07-11 2022-07-11 2022-10-02\n#> 2 Shonnie 2022-03-30     1 2022-01-10 2022-01-01 2022-04-03\n#> 3 Burnard 2022-01-10     1 2022-01-10 2022-01-01 2022-04-03\n#> 4 Omer    2022-11-25     4 2022-10-03 2022-10-03 2022-12-31\n#> 5 Hillel  2022-07-30     3 2022-07-11 2022-07-11 2022-10-02\n#> 6 Curlie  2022-12-11     4 2022-10-03 2022-10-03 2022-12-31\n#> # ℹ 94 more rows\n\n\n20.5.5 Exercises\n\n\nCan you explain what’s happening with the keys in this equi join? Why are they different?\n\nx |> full_join(y, by = \"key\")\n#> # A tibble: 4 × 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     3 x3    <NA> \n#> 4     4 <NA>  y3\n\nx |> full_join(y, by = \"key\", keep = TRUE)\n#> # A tibble: 4 × 4\n#>   key.x val_x key.y val_y\n#>   <dbl> <chr> <dbl> <chr>\n#> 1     1 x1        1 y1   \n#> 2     2 x2        2 y2   \n#> 3     3 x3       NA <NA> \n#> 4    NA <NA>      4 y3\n\n\nWhen finding if any party period overlapped with another party period we used q < q in the join_by()? Why? What happens if you remove this inequality?"
  },
  {
    "objectID": "joins.html#summary",
    "href": "joins.html#summary",
    "title": "20  Joins",
    "section": "\n20.6 Summary",
    "text": "20.6 Summary\nIn this chapter, you’ve learned how to use mutating and filtering joins to combine data from a pair of data frames. Along the way you learned how to identify keys, and the difference between primary and foreign keys. You also understand how joins work and how to figure out how many rows the output will have. Finally, you’ve gained a glimpse into the power of non-equi joins and seen a few interesting use cases.\nThis chapter concludes the “Transform” part of the book where the focus was on the tools you could use with individual columns and tibbles. You learned about dplyr and base functions for working with logical vectors, numbers, and complete tables, stringr functions for working strings, lubridate functions for working with date-times, and forcats functions for working with factors.\nIn the next part of the book, you’ll learn more about getting various types of data into R in a tidy form."
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "Import",
    "section": "",
    "text": "In this part of the book, you’ll learn how to import a wider range of data into R, as well as how to get it into a form useful form for analysis. Sometimes this is just a matter of calling a function from the appropriate data import package. But in more complex cases it might require both tidying and transformation in order to get to the tidy rectangle that you’d prefer to work with.\n\n\n\n\nFigure 1: Data import is the beginning of the data science process; without data you can’t do data science!\n\n\n\n\nIn this part of the book you’ll learn how to access data stored in the following ways:\n\nIn Chapter 21, you’ll learn how to import data from Excel spreadsheets and Google Sheets.\nIn ?sec-import-databases, you’ll learn about getting data out of a database and into R (and you’ll also learn a little about how to get data out of R and into a database).\nIn ?sec-arrow, you’ll learn about Arrow, a powerful tool for working with out-of-memory data, particularly when it’s stored in the parquet format.\nIn Chapter 24, you’ll learn how to work with hierarchical data, including the deeply nested lists produced by data stored in the JSON format.\nIn Chapter 25, you’ll learn web “scraping”, the art and science of extracting data from web pages.\n\nThere are two important tidyverse packages that we don’t discuss here: haven and xml2. If you’re working with data from SPSS, Stata, and SAS files, check out the haven package, https://haven.tidyverse.org. If you’re working with XML data, check out the xml2 package, https://xml2.r-lib.org. Otherwise, you’ll need to do some research to figure which package you’ll need to use; google is your friend here 😃."
  },
  {
    "objectID": "spreadsheets.html#introduction",
    "href": "spreadsheets.html#introduction",
    "title": "21  Spreadsheets",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction\nIn ?sec-data-import you learned about importing data from plain text files like .csv and .tsv. Now it’s time to learn how to get data out of a spreadsheet, either an Excel spreadsheet or a Google Sheet. This will build on much of what you’ve learned in ?sec-data-import, but we will also discuss additional considerations and complexities when working with data from spreadsheets.\nIf you or your collaborators are using spreadsheets for organizing data, we strongly recommend reading the paper “Data Organization in Spreadsheets” by Karl Broman and Kara Woo: https://doi.org/10.1080/00031305.2017.1375989. The best practices presented in this paper will save you much headache when you import data from a spreadsheet into R to analyze and visualize."
  },
  {
    "objectID": "spreadsheets.html#excel",
    "href": "spreadsheets.html#excel",
    "title": "21  Spreadsheets",
    "section": "\n21.2 Excel",
    "text": "21.2 Excel\nMicrosoft Excel is a widely used spreadsheet software program where data are organized in worksheets inside of spreadsheet files.\n\n21.2.1 Prerequisites\nIn this section, you’ll learn how to load data from Excel spreadsheets in R with the readxl package. This package is non-core tidyverse, so you need to load it explicitly, but it is installed automatically when you install the tidyverse package. Later, we’ll also use the writexl package, which allows us to create Excel spreadsheets.\n\nlibrary(readxl)\n#> Warning: package 'readxl' was built under R version 4.2.3\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(writexl)\n#> Warning: package 'writexl' was built under R version 4.2.3\n\n\n21.2.2 Getting started\nMost of readxl’s functions allow you to load Excel spreadsheets into R:\n\n\nread_xls() reads Excel files with xls format.\n\nread_xlsx() read Excel files with xlsx format.\n\nread_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.\n\nThese functions all have similar syntax just like other functions we have previously introduced for reading other types of files, e.g., read_csv(), read_table(), etc. For the rest of the chapter we will focus on using read_excel().\n\n21.2.3 Reading Excel spreadsheets\nFigure 21.1 shows what the spreadsheet we’re going to read into R looks like in Excel.\n\n\n\n\nFigure 21.1: Spreadsheet called students.xlsx in Excel.\n\n\n\n\nThe first argument to read_excel() is the path to the file to read.\n\nstudents <- read_excel(\"data/students.xlsx\")\n\nread_excel() will read the file in as a tibble.\n\nstudents\n#> # A tibble: 6 × 5\n#>   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#>          <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2            2 Barclay Lynn     French fries       Lunch only          5    \n#> 3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 4            4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\nWe have six students in the data and five variables on each student. However there are a few things we might want to address in this dataset:\n\n\nThe column names are all over the place. You can provide column names that follow a consistent format; we recommend snake_case using the col_names argument.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\")\n)\n#> # A tibble: 7 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>   <chr>      <chr>            <chr>              <chr>               <chr>\n#> 1 Student ID Full Name        favourite.food     mealPlan            AGE  \n#> 2 1          Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 3 2          Barclay Lynn     French fries       Lunch only          5    \n#> 4 3          Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 5 4          Leon Rossini     Anchovies          Lunch only          <NA> \n#> 6 5          Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 7 6          Güvenç Attila    Ice cream          Lunch only          6\n\nUnfortunately, this didn’t quite do the trick. We now have the variable names we want, but what was previously the header row now shows up as the first observation in the data. You can explicitly skip that row using the skip argument.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1\n)\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nIn the favourite_food column, one of the observations is N/A, which stands for “not available” but it’s currently not recognized as an NA (note the contrast between this N/A and the age of the fourth student in the list). You can specify which character strings should be recognized as NAs with the na argument. By default, only \"\" (empty string, or, in the case of reading from a spreadsheet, an empty cell or a cell with the formula =NA()) is recognized as an NA.\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\")\n)\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n\n\nOne other remaining issue is that age is read in as a character variable, but it really should be numeric. Just like with read_csv() and friends for reading data from flat files, you can supply a col_types argument to read_excel() and specify the column types for the variables you read in. The syntax is a bit different, though. Your options are \"skip\", \"guess\", \"logical\", \"numeric\", \"date\", \"text\" or \"list\".\n\nread_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"numeric\")\n)\n#> Warning: Expecting numeric in E6 / R6C5: got 'five'\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <chr>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch    NA\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\nHowever, this didn’t quite produce the desired result either. By specifying that age should be numeric, we have turned the one cell with the non-numeric entry (which had the value five) into an NA. In this case, we should read age in as \"text\" and then make the change once the data is loaded in R.\n\nstudents <- read_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = c(\"numeric\", \"text\", \"text\", \"text\", \"text\")\n)\n\nstudents <- students |>\n  mutate(\n    age = if_else(age == \"five\", \"5\", age),\n    age = parse_number(age)\n  )\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <chr>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nIt took us multiple steps and trial-and-error to load the data in exactly the format we want, and this is not unexpected. Data science is an iterative process, and the process of iteration can be even more tedious when reading data in from spreadsheets compared to other plain text, rectangular data files because humans tend to input data into spreadsheets and use them not just for data storage but also for sharing and communication.\nThere is no way to know exactly what the data will look like until you load it and take a look at it. Well, there is one way, actually. You can open the file in Excel and take a peek. If you’re going to do so, we recommend making a copy of the Excel file to open and browse interactively while leaving the original data file untouched and reading into R from the untouched file. This will ensure you don’t accidentally overwrite anything in the spreadsheet while inspecting it. You should also not be afraid of doing what we did here: load the data, take a peek, make adjustments to your code, load it again, and repeat until you’re happy with the result.\n\n21.2.4 Reading worksheets\nAn important feature that distinguishes spreadsheets from flat files is the notion of multiple sheets, called worksheets. Figure 21.2 shows an Excel spreadsheet with multiple worksheets. The data come from the palmerpenguins package. Each worksheet contains information on penguins from a different island where data were collected.\n\n\n\n\nFigure 21.2: Spreadsheet called penguins.xlsx in Excel containing three worksheets.\n\n\n\n\nYou can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.\n\nread_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\")\n#> # A tibble: 52 × 8\n#>   species island    bill_length_mm     bill_depth_mm      flipper_length_mm\n#>   <chr>   <chr>     <chr>              <chr>              <chr>            \n#> 1 Adelie  Torgersen 39.1               18.7               181              \n#> 2 Adelie  Torgersen 39.5               17.399999999999999 186              \n#> 3 Adelie  Torgersen 40.299999999999997 18                 195              \n#> 4 Adelie  Torgersen NA                 NA                 NA               \n#> 5 Adelie  Torgersen 36.700000000000003 19.3               193              \n#> 6 Adelie  Torgersen 39.299999999999997 20.6               190              \n#> # ℹ 46 more rows\n#> # ℹ 3 more variables: body_mass_g <chr>, sex <chr>, year <dbl>\n\nSome variables that appear to contain numerical data are read in as characters due to the character string \"NA\" not being recognized as a true NA.\n\npenguins_torgersen <- read_excel(\"data/penguins.xlsx\", sheet = \"Torgersen Island\", na = \"NA\")\n\npenguins_torgersen\n#> # A tibble: 52 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#>   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n#> 1 Adelie  Torgersen           39.1          18.7               181\n#> 2 Adelie  Torgersen           39.5          17.4               186\n#> 3 Adelie  Torgersen           40.3          18                 195\n#> 4 Adelie  Torgersen           NA            NA                  NA\n#> 5 Adelie  Torgersen           36.7          19.3               193\n#> 6 Adelie  Torgersen           39.3          20.6               190\n#> # ℹ 46 more rows\n#> # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n\nAlternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.\n\nexcel_sheets(\"data/penguins.xlsx\")\n#> [1] \"Torgersen Island\" \"Biscoe Island\"    \"Dream Island\"\n\nOnce you know the names of the worksheets, you can read them in individually with read_excel().\n\npenguins_biscoe <- read_excel(\"data/penguins.xlsx\", sheet = \"Biscoe Island\", na = \"NA\")\npenguins_dream  <- read_excel(\"data/penguins.xlsx\", sheet = \"Dream Island\", na = \"NA\")\n\nIn this case the full penguins dataset is spread across three worksheets in the spreadsheet. Each worksheet has the same number of columns but different numbers of rows.\n\ndim(penguins_torgersen)\n#> [1] 52  8\ndim(penguins_biscoe)\n#> [1] 168   8\ndim(penguins_dream)\n#> [1] 124   8\n\nWe can put them together with bind_rows().\n\npenguins <- bind_rows(penguins_torgersen, penguins_biscoe, penguins_dream)\npenguins\n#> # A tibble: 344 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#>   <chr>   <chr>              <dbl>         <dbl>             <dbl>\n#> 1 Adelie  Torgersen           39.1          18.7               181\n#> 2 Adelie  Torgersen           39.5          17.4               186\n#> 3 Adelie  Torgersen           40.3          18                 195\n#> 4 Adelie  Torgersen           NA            NA                  NA\n#> 5 Adelie  Torgersen           36.7          19.3               193\n#> 6 Adelie  Torgersen           39.3          20.6               190\n#> # ℹ 338 more rows\n#> # ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n\nIn Chapter 27 we’ll talk about ways of doing this sort of task without repetitive code.\n\n21.2.5 Reading part of a sheet\nSince many use Excel spreadsheets for presentation as well as for data storage, it’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R. Figure 21.3 shows such a spreadsheet: in the middle of the sheet is what looks like a data frame but there is extraneous text in cells above and below the data.\n\n\n\n\nFigure 21.3: Spreadsheet called deaths.xlsx in Excel.\n\n\n\n\nThis spreadsheet is one of the example spreadsheets provided in the readxl package. You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel() as usual.\n\ndeaths_path <- readxl_example(\"deaths.xlsx\")\ndeaths <- read_excel(deaths_path)\n#> New names:\n#> • `` -> `...2`\n#> • `` -> `...3`\n#> • `` -> `...4`\n#> • `` -> `...5`\n#> • `` -> `...6`\ndeaths\n#> # A tibble: 18 × 6\n#>   `Lots of people`    ...2       ...3  ...4     ...5          ...6           \n#>   <chr>               <chr>      <chr> <chr>    <chr>         <chr>          \n#> 1 simply cannot resi… <NA>       <NA>  <NA>     <NA>          some notes     \n#> 2 at                  the        top   <NA>     of            their spreadsh…\n#> 3 or                  merging    <NA>  <NA>     <NA>          cells          \n#> 4 Name                Profession Age   Has kids Date of birth Date of death  \n#> 5 David Bowie         musician   69    TRUE     17175         42379          \n#> 6 Carrie Fisher       actor      60    TRUE     20749         42731          \n#> # ℹ 12 more rows\n\nThe top three rows and the bottom four rows are not part of the data frame. It’s possible to eliminate these extraneous rows using the skip and n_max arguments, but we recommend using cell ranges. In Excel, the top left cell is A1. As you move across columns to the right, the cell label moves down the alphabet, i.e. B1, C1, etc. And as you move down a column, the number in the cell label increases, i.e. A2, A3, etc.\nHere the data we want to read in starts in cell A5 and ends in cell F15. In spreadsheet notation, this is A5:F15, which we supply to the range argument:\n\nread_excel(deaths_path, range = \"A5:F15\")\n#> # A tibble: 10 × 6\n#>   Name          Profession   Age `Has kids` `Date of birth`    \n#>   <chr>         <chr>      <dbl> <lgl>      <dttm>             \n#> 1 David Bowie   musician      69 TRUE       1947-01-08 00:00:00\n#> 2 Carrie Fisher actor         60 TRUE       1956-10-21 00:00:00\n#> 3 Chuck Berry   musician      90 TRUE       1926-10-18 00:00:00\n#> 4 Bill Paxton   actor         61 TRUE       1955-05-17 00:00:00\n#> 5 Prince        musician      57 TRUE       1958-06-07 00:00:00\n#> 6 Alan Rickman  actor         69 FALSE      1946-02-21 00:00:00\n#> # ℹ 4 more rows\n#> # ℹ 1 more variable: `Date of death` <dttm>\n\n\n21.2.6 Data types\nIn CSV files, all values are strings. This is not particularly true to the data, but it is simple: everything is a string.\nThe underlying data in Excel spreadsheets is more complex. A cell can be one of four things:\n\nA boolean, like TRUE, FALSE, or NA.\nA number, like “10” or “10.5”.\nA datetime, which can also include time like “11/1/21” or “11/1/21 3:00 PM”.\nA text string, like “ten”.\n\nWhen working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. For example, Excel has no notion of an integer. All numbers are stored as floating points, but you can choose to display the data with a customizable number of decimal points. Similarly, dates are actually stored as numbers, specifically the number of seconds since January 1, 1970. You can customize how you display the date by applying formatting in Excel. Confusingly, it’s also possible to have something that looks like a number but is actually a string (e.g., type '10 into a cell in Excel).\nThese differences between how the underlying data are stored vs. how they’re displayed can cause surprises when the data are loaded into R. By default readxl will guess the data type in a given column. A recommended workflow is to let readxl guess the column types, confirm that you’re happy with the guessed column types, and if not, go back and re-import specifying col_types as shown in Section 21.2.3.\nAnother challenge is when you have a column in your Excel spreadsheet that has a mix of these types, e.g., some cells are numeric, others text, others dates. When importing the data into R readxl has to make some decisions. In these cases you can set the type for this column to \"list\", which will load the column as a list of length 1 vectors, where the type of each element of the vector is guessed.\n\n\n\n\n\n\nSometimes data is stored in more exotic ways, like the color of the cell background, or whether or not the text is bold. In such cases, you might find the tidyxl package useful. See https://nacnudus.github.io/spreadsheet-munging-strategies/ for more on strategies for working with non-tabular data from Excel.\n\n\n\n\n21.2.7 Writing to Excel\nLet’s create a small data frame that we can then write out. Note that item is a factor and quantity is an integer.\n\nbake_sale <- tibble(\n  item     = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\n\nbake_sale\n#> # A tibble: 3 × 2\n#>   item    quantity\n#>   <fct>      <dbl>\n#> 1 brownie       10\n#> 2 cupcake        5\n#> 3 cookie         8\n\nYou can write data back to disk as an Excel file using the write_xlsx() from the writexl package:\n\nwrite_xlsx(bake_sale, path = \"data/bake-sale.xlsx\")\n\nFigure 21.4 shows what the data looks like in Excel. Note that column names are included and bolded. These can be turned off by setting col_names and format_headers arguments to FALSE.\n\n\n\n\nFigure 21.4: Spreadsheet called bake_sale.xlsx in Excel.\n\n\n\n\nJust like reading from a CSV, information on data type is lost when we read the data back in. This makes Excel files unreliable for caching interim results as well. For alternatives, see ?sec-writing-to-a-file.\n\nread_excel(\"data/bake-sale.xlsx\")\n#> # A tibble: 3 × 2\n#>   item    quantity\n#>   <chr>      <dbl>\n#> 1 brownie       10\n#> 2 cupcake        5\n#> 3 cookie         8\n\n\n21.2.8 Formatted output\nThe writexl package is a light-weight solution for writing a simple Excel spreadsheet, but if you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package. We won’t go into the details of using this package here, but we recommend reading https://ycphs.github.io/openxlsx/articles/Formatting.html for an extensive discussion on further formatting functionality for data written from R to Excel with openxlsx.\nNote that this package is not part of the tidyverse so the functions and workflows may feel unfamiliar. For example, function names are camelCase, multiple functions can’t be composed in pipelines, and arguments are in a different order than they tend to be in the tidyverse. However, this is ok. As your R learning and usage expands outside of this book you will encounter lots of different styles used in various R packages that you might use to accomplish specific goals in R. A good way of familiarizing yourself with the coding style used in a new package is to run the examples provided in function documentation to get a feel for the syntax and the output formats as well as reading any vignettes that might come with the package.\n\n21.2.9 Exercises\n\n\nIn an Excel file, create the following dataset and save it as survey.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\nThen, read it into R, with survey_id as a character variable and n_pets as a numerical variable.\n\n#> # A tibble: 6 × 2\n#>   survey_id n_pets\n#>   <chr>      <dbl>\n#> 1 1              0\n#> 2 2              1\n#> 3 3             NA\n#> 4 4              2\n#> 5 5              2\n#> 6 6             NA\n\n\n\nIn another Excel file, create the following dataset and save it as roster.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\nThen, read it into R. The resulting data frame should be called roster and should look like the following.\n\n#> # A tibble: 12 × 3\n#>    group subgroup    id\n#>    <dbl> <chr>    <dbl>\n#>  1     1 A            1\n#>  2     1 A            2\n#>  3     1 A            3\n#>  4     1 B            4\n#>  5     1 B            5\n#>  6     1 B            6\n#>  7     1 B            7\n#>  8     2 A            8\n#>  9     2 A            9\n#> 10     2 B           10\n#> 11     2 B           11\n#> 12     2 B           12\n\n\n\nIn a new Excel file, create the following dataset and save it as sales.xlsx. Alternatively, you can download it as an Excel file from here.\n\n\n\n\n\na. Read sales.xlsx in and save as sales. The data frame should look like the following, with id and n as column names and with 9 rows.\n\n#> # A tibble: 9 × 2\n#>   id      n    \n#>   <chr>   <chr>\n#> 1 Brand 1 n    \n#> 2 1234    8    \n#> 3 8721    2    \n#> 4 1822    3    \n#> 5 Brand 2 n    \n#> 6 3333    1    \n#> 7 2156    3    \n#> 8 3987    6    \n#> 9 3216    5\n\nb. Modify sales further to get it into the following tidy format with three columns (brand, id, and n) and 7 rows of data. Note that id and n are numeric, brand is a character variable.\n\n#> # A tibble: 7 × 3\n#>   brand      id     n\n#>   <chr>   <dbl> <dbl>\n#> 1 Brand 1  1234     8\n#> 2 Brand 1  8721     2\n#> 3 Brand 1  1822     3\n#> 4 Brand 2  3333     1\n#> 5 Brand 2  2156     3\n#> 6 Brand 2  3987     6\n#> 7 Brand 2  3216     5\n\n\nRecreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.\nIn ?sec-data-import you learned about the janitor::clean_names() function to turn columns names into snake case. Read the students.xlsx file that we introduced earlier in this section and use this function to “clean” the column names.\nWhat happens if you try to read in a file with .xlsx extension with read_xls()?"
  },
  {
    "objectID": "spreadsheets.html#google-sheets",
    "href": "spreadsheets.html#google-sheets",
    "title": "21  Spreadsheets",
    "section": "\n21.3 Google Sheets",
    "text": "21.3 Google Sheets\nGoogle Sheets is another widely used spreadsheet program. It’s free and web-based. Just like with Excel, in Google Sheets data are organized in worksheets (also called sheets) inside of spreadsheet files.\n\n21.3.1 Prerequisites\nThis section will also focus on spreadsheets, but this time you’ll be loading data from a Google Sheet with the googlesheets4 package. This package is non-core tidyverse as well, you need to load it explicitly.\n\nlibrary(googlesheets4)\n#> Warning: package 'googlesheets4' was built under R version 4.2.3\nlibrary(tidyverse)\n\nA quick note about the name of the package: googlesheets4 uses v4 of the Sheets API v4 to provide an R interface to Google Sheets, hence the name.\n\n21.3.2 Getting started\nThe main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read().\nYou can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.\nIn this section we’ll work with the same datasets as the ones in the Excel section to highlight similarities and differences between workflows for reading data from Excel and Google Sheets. readxl and googlesheets4 packages are both designed to mimic the functionality of the readr package, which provides the read_csv() function you’ve seen in ?sec-data-import. Therefore, many of the tasks can be accomplished with simply swapping out read_excel() for read_sheet(). However you’ll also see that Excel and Google Sheets don’t behave in exactly the same way, therefore other tasks may require further updates to the function calls.\n\n21.3.3 Reading Google Sheets\nFigure 21.5 shows what the spreadsheet we’re going to read into R looks like in Google Sheets. This is the same dataset as in Figure 21.1, except it’s stored in a Google Sheet instead of Excel.\n\n\n\n\nFigure 21.5: Google Sheet called students in a browser window.\n\n\n\n\nThe first argument to read_sheet() is the URL of the file to read, and it returns a tibble:https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w. These URLs are not pleasant to work with, so you’ll often want to identify a sheet by its ID.\n\n#students_sheet_id <- \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\n#students <- read_sheet(students_sheet_id)\n#students\n\nJust like we did with read_excel(), we can supply column names, NA strings, and column types to read_sheet().\n\n#students <- read_sheet(\n#  students_sheet_id,\n#  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", #\"age\"),\n#  skip = 1,\n#  na = c(\"\", \"N/A\"),\n#  col_types = \"dcccc\"\n#)\n\n#students\n\nNote that we defined column types a bit differently here, using short codes. For example, “dcccc” stands for “double, character, character, character, character”.\nIt’s also possible to read individual sheets from Google Sheets as well. Let’s read the “Torgersen Island” sheet from the penguins Google Sheet:\n\n#penguins_sheet_id <- \"1aFu8lnD_g0yjF5O-K6SFgSEWiHPpgvFCF0NY9D6LXnY\"\n#read_sheet(penguins_sheet_id, sheet = \"Torgersen Island\")\n\nYou can obtain a list of all sheets within a Google Sheet with sheet_names():\n\n#sheet_names(penguins_sheet_id)\n\nFinally, just like with read_excel(), we can read in a portion of a Google Sheet by defining a range in read_sheet(). Note that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.\n\n#deaths_url <- gs4_example(\"deaths\")\n#deaths <- read_sheet(deaths_url, range = \"A5:F15\")\n#deaths\n\n\n21.3.4 Writing to Google Sheets\nYou can write from R to Google Sheets with write_sheet(). The first argument is the data frame to write, and the second argument is the name (or other identifier) of the Google Sheet to write to:\n\nwrite_sheet(bake_sale, ss = \"bake-sale\")\n\nIf you’d like to write your data to a specific (work)sheet inside a Google Sheet, you can specify that with the sheet argument as well.\n\nwrite_sheet(bake_sale, ss = \"bake-sale\", sheet = \"Sales\")\n\n\n21.3.5 Authentication\nWhile you can read from a public Google Sheet without authenticating with your Google account, reading a private sheet or writing to a sheet requires authentication so that googlesheets4 can view and manage your Google Sheets.\nWhen you attempt to read in a sheet that requires authentication, googlesheets4 will direct you to a web browser with a prompt to sign in to your Google account and grant permission to operate on your behalf with Google Sheets. However, if you want to specify a specific Google account, authentication scope, etc. you can do so with gs4_auth(), e.g., gs4_auth(email = \"mine@example.com\"), which will force the use of a token associated with a specific email. For further authentication details, we recommend reading the documentation googlesheets4 auth vignette: https://googlesheets4.tidyverse.org/articles/auth.html.\n\n21.3.6 Exercises\n\nRead the students dataset from earlier in the chapter from Excel and also from Google Sheets, with no additional arguments supplied to the read_excel() and read_sheet() functions. Are the resulting data frames in R exactly the same? If not, how are they different?\nRead the Google Sheet titled survey from https://pos.it/r4ds-survey, with survey_id as a character variable and n_pets as a numerical variable.\n\nRead the Google Sheet titled roster from https://pos.it/r4ds-roster. The resulting data frame should be called roster and should look like the following."
  },
  {
    "objectID": "spreadsheets.html#summary",
    "href": "spreadsheets.html#summary",
    "title": "21  Spreadsheets",
    "section": "\n21.4 Summary",
    "text": "21.4 Summary\nMicrosoft Excel and Google Sheets are two of the most popular spreadsheet systems. Being able to interact with data stored in Excel and Google Sheets files directly from R is a superpower! In this chapter you learned how to read data into R from spreadsheets from Excel with read_excel() from the readxl package and from Google Sheets with read_sheet() from the googlesheets4 package. These functions work very similarly to each other and have similar arguments for specifying column names, NA strings, rows to skip on top of the file you’re reading in, etc. Additionally, both functions make it possible to read a single sheet from a spreadsheet as well.\nOn the other hand, writing to an Excel file requires a different package and function (writexl::write_xlsx()) while you can write to a Google Sheet with the googlesheets4 package, with write_sheet().\nIn the next chapter, you’ll learn about a different data source and how to read data from that source into R: databases."
  },
  {
    "objectID": "rectangling.html#introduction",
    "href": "rectangling.html#introduction",
    "title": "24  Hierarchical data",
    "section": "\n24.1 Introduction",
    "text": "24.1 Introduction\nIn this chapter, you’ll learn the art of data rectangling: taking data that is fundamentally hierarchical, or tree-like, and converting it into a rectangular data frame made up of rows and columns. This is important because hierarchical data is surprisingly common, especially when working with data that comes from the web.\nTo learn about rectangling, you’ll need to first learn about lists, the data structure that makes hierarchical data possible. Then you’ll learn about two crucial tidyr functions: tidyr::unnest_longer() and tidyr::unnest_wider(). We’ll then show you a few case studies, applying these simple functions again and again to solve real problems. We’ll finish off by talking about JSON, the most frequent source of hierarchical datasets and a common format for data exchange on the web.\n\n24.1.1 Prerequisites\nIn this chapter, we’ll use many functions from tidyr, a core member of the tidyverse. We’ll also use repurrrsive to provide some interesting datasets for rectangling practice, and we’ll finish by using jsonlite to read JSON files into R lists.\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\nlibrary(repurrrsive)\n#> Warning: package 'repurrrsive' was built under R version 4.2.3\nlibrary(jsonlite)\n#> Warning: package 'jsonlite' was built under R version 4.2.3"
  },
  {
    "objectID": "rectangling.html#lists",
    "href": "rectangling.html#lists",
    "title": "24  Hierarchical data",
    "section": "\n24.2 Lists",
    "text": "24.2 Lists\nSo far you’ve worked with data frames that contain simple vectors like integers, numbers, characters, date-times, and factors. These vectors are simple because they’re homogeneous: every element is of the same data type. If you want to store elements of different types in the same vector, you’ll need a list, which you create with list():\n\nx1 <- list(1:4, \"a\", TRUE)\nx1\n#> [[1]]\n#> [1] 1 2 3 4\n#> \n#> [[2]]\n#> [1] \"a\"\n#> \n#> [[3]]\n#> [1] TRUE\n\nIt’s often convenient to name the components, or children, of a list, which you can do in the same way as naming the columns of a tibble:\n\nx2 <- list(a = 1:2, b = 1:3, c = 1:4)\nx2\n#> $a\n#> [1] 1 2\n#> \n#> $b\n#> [1] 1 2 3\n#> \n#> $c\n#> [1] 1 2 3 4\n\nEven for these very simple lists, printing takes up quite a lot of space. A useful alternative is str(), which generates a compact display of the structure, de-emphasizing the contents:\n\nstr(x1)\n#> List of 3\n#>  $ : int [1:4] 1 2 3 4\n#>  $ : chr \"a\"\n#>  $ : logi TRUE\nstr(x2)\n#> List of 3\n#>  $ a: int [1:2] 1 2\n#>  $ b: int [1:3] 1 2 3\n#>  $ c: int [1:4] 1 2 3 4\n\nAs you can see, str() displays each child of the list on its own line. It displays the name, if present, then an abbreviation of the type, then the first few values.\n\n24.2.1 Hierarchy\nLists can contain any type of object, including other lists. This makes them suitable for representing hierarchical (tree-like) structures:\n\nx3 <- list(list(1, 2), list(3, 4))\nstr(x3)\n#> List of 2\n#>  $ :List of 2\n#>   ..$ : num 1\n#>   ..$ : num 2\n#>  $ :List of 2\n#>   ..$ : num 3\n#>   ..$ : num 4\n\nThis is notably different to c(), which generates a flat vector:\n\nc(c(1, 2), c(3, 4))\n#> [1] 1 2 3 4\n\nx4 <- c(list(1, 2), list(3, 4))\nstr(x4)\n#> List of 4\n#>  $ : num 1\n#>  $ : num 2\n#>  $ : num 3\n#>  $ : num 4\n\nAs lists get more complex, str() gets more useful, as it lets you see the hierarchy at a glance:\n\nx5 <- list(1, list(2, list(3, list(4, list(5)))))\nstr(x5)\n#> List of 2\n#>  $ : num 1\n#>  $ :List of 2\n#>   ..$ : num 2\n#>   ..$ :List of 2\n#>   .. ..$ : num 3\n#>   .. ..$ :List of 2\n#>   .. .. ..$ : num 4\n#>   .. .. ..$ :List of 1\n#>   .. .. .. ..$ : num 5\n\nAs lists get even larger and more complex, str() eventually starts to fail, and you’ll need to switch to View()1. Figure 24.1 shows the result of calling View(x5). The viewer starts by showing just the top level of the list, but you can interactively expand any of the components to see more, as in Figure 24.2. RStudio will also show you the code you need to access that element, as in Figure 24.3. We’ll come back to how this code works in ?sec-subset-one.\n\n\n\n\nFigure 24.1: The RStudio view lets you interactively explore a complex list. The viewer opens showing only the top level of the list.\n\n\n\n\n\n\n\n\nFigure 24.2: Clicking on the rightward facing triangle expands that component of the list so that you can also see its children.\n\n\n\n\n\n\n\n\nFigure 24.3: You can repeat this operation as many times as needed to get to the data you’re interested in. Note the bottom-left corner: if you click an element of the list, RStudio will give you the subsetting code needed to access it, in this case x5[[2]][[2]][[2]].\n\n\n\n\n\n24.2.2 List-columns\nLists can also live inside a tibble, where we call them list-columns. List-columns are useful because they allow you to place objects in a tibble that wouldn’t usually belong in there. In particular, list-columns are used a lot in the tidymodels ecosystem, because they allow you to store things like model outputs or resamples in a data frame.\nHere’s a simple example of a list-column:\n\ndf <- tibble(\n  x = 1:2, \n  y = c(\"a\", \"b\"),\n  z = list(list(1, 2), list(3, 4, 5))\n)\ndf\n#> # A tibble: 2 × 3\n#>       x y     z         \n#>   <int> <chr> <list>    \n#> 1     1 a     <list [2]>\n#> 2     2 b     <list [3]>\n\nThere’s nothing special about lists in a tibble; they behave like any other column:\n\ndf |> \n  filter(x == 1)\n#> # A tibble: 1 × 3\n#>       x y     z         \n#>   <int> <chr> <list>    \n#> 1     1 a     <list [2]>\n\nComputing with list-columns is harder, but that’s because computing with lists is harder in general; we’ll come back to that in Chapter 27. In this chapter, we’ll focus on unnesting list-columns out into regular variables so you can use your existing tools on them.\nThe default print method just displays a rough summary of the contents. The list column could be arbitrarily complex, so there’s no good way to print it. If you want to see it, you’ll need to pull out just the one list-column and apply one of the techniques that you’ve learned above, like df |> pull(z) |> str() or df |> pull(z) |> View().\n\n\n\n\n\n\nBase R\n\n\n\nIt’s possible to put a list in a column of a data.frame, but it’s a lot fiddlier because data.frame() treats a list as a list of columns:\n\ndata.frame(x = list(1:3, 3:5))\n#>   x.1.3 x.3.5\n#> 1     1     3\n#> 2     2     4\n#> 3     3     5\n\nYou can force data.frame() to treat a list as a list of rows by wrapping it in list I(), but the result doesn’t print particularly well:\n\ndata.frame(\n  x = I(list(1:2, 3:5)), \n  y = c(\"1, 2\", \"3, 4, 5\")\n)\n#>         x       y\n#> 1    1, 2    1, 2\n#> 2 3, 4, 5 3, 4, 5\n\nIt’s easier to use list-columns with tibbles because tibble() treats lists like vectors and the print method has been designed with lists in mind."
  },
  {
    "objectID": "rectangling.html#unnesting",
    "href": "rectangling.html#unnesting",
    "title": "24  Hierarchical data",
    "section": "\n24.3 Unnesting",
    "text": "24.3 Unnesting\nNow that you’ve learned the basics of lists and list-columns, let’s explore how you can turn them back into regular rows and columns. Here we’ll use very simple sample data so you can get the basic idea; in the next section we’ll switch to real data.\nList-columns tend to come in two basic forms: named and unnamed. When the children are named, they tend to have the same names in every row. For example, in df1, every element of list-column y has two elements named a and b. Named list-columns naturally unnest into columns: each named element becomes a new named column.\n\ndf1 <- tribble(\n  ~x, ~y,\n  1, list(a = 11, b = 12),\n  2, list(a = 21, b = 22),\n  3, list(a = 31, b = 32),\n)\n\nWhen the children are unnamed, the number of elements tends to vary from row-to-row. For example, in df2, the elements of list-column y are unnamed and vary in length from one to three. Unnamed list-columns naturally unnest into rows: you’ll get one row for each child.\n\n\ndf2 <- tribble(\n  ~x, ~y,\n  1, list(11, 12, 13),\n  2, list(21),\n  3, list(31, 32),\n)\n\ntidyr provides two functions for these two cases: unnest_wider() and unnest_longer(). The following sections explain how they work.\n\n24.3.1 unnest_wider()\n\nWhen each row has the same number of elements with the same names, like df1, it’s natural to put each component into its own column with unnest_wider():\n\ndf1 |> \n  unnest_wider(y)\n#> # A tibble: 3 × 3\n#>       x     a     b\n#>   <dbl> <dbl> <dbl>\n#> 1     1    11    12\n#> 2     2    21    22\n#> 3     3    31    32\n\nBy default, the names of the new columns come exclusively from the names of the list elements, but you can use the names_sep argument to request that they combine the column name and the element name. This is useful for disambiguating repeated names.\n\ndf1 |> \n  unnest_wider(y, names_sep = \"_\")\n#> # A tibble: 3 × 3\n#>       x   y_a   y_b\n#>   <dbl> <dbl> <dbl>\n#> 1     1    11    12\n#> 2     2    21    22\n#> 3     3    31    32\n\n\n24.3.2 unnest_longer()\n\nWhen each row contains an unnamed list, it’s most natural to put each element into its own row with unnest_longer():\n\ndf2 |> \n  unnest_longer(y)\n#> # A tibble: 6 × 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1    11\n#> 2     1    12\n#> 3     1    13\n#> 4     2    21\n#> 5     3    31\n#> 6     3    32\n\nNote how x is duplicated for each element inside of y: we get one row of output for each element inside the list-column. But what happens if one of the elements is empty, as in the following example?\n\ndf6 <- tribble(\n  ~x, ~y,\n  \"a\", list(1, 2),\n  \"b\", list(3),\n  \"c\", list()\n)\ndf6 |> unnest_longer(y)\n#> # A tibble: 3 × 2\n#>   x         y\n#>   <chr> <dbl>\n#> 1 a         1\n#> 2 a         2\n#> 3 b         3\n\nWe get zero rows in the output, so the row effectively disappears. If you want to preserve that row, adding NA in y, set keep_empty = TRUE.\n\n24.3.3 Inconsistent types\nWhat happens if you unnest a list-column that contains different types of vector? For example, take the following dataset where the list-column y contains two numbers, a character, and a logical, which can’t normally be mixed in a single column.\n\ndf4 <- tribble(\n  ~x, ~y,\n  \"a\", list(1),\n  \"b\", list(\"a\", TRUE, 5)\n)\n\nunnest_longer() always keeps the set of columns unchanged, while changing the number of rows. So what happens? How does unnest_longer() produce five rows while keeping everything in y?\n\ndf4 |> \n  unnest_longer(y)\n#> # A tibble: 4 × 2\n#>   x     y        \n#>   <chr> <list>   \n#> 1 a     <dbl [1]>\n#> 2 b     <chr [1]>\n#> 3 b     <lgl [1]>\n#> 4 b     <dbl [1]>\n\nAs you can see, the output contains a list-column, but every element of the list-column contains a single element. Because unnest_longer() can’t find a common type of vector, it keeps the original types in a list-column. You might wonder if this breaks the commandment that every element of a column must be the same type. It doesn’t: every element is a list, even though the contents are of different types.\nDealing with inconsistent types is challenging and the details depend on the precise nature of the problem and your goals, but you’ll most likely need tools from Chapter 27.\n\n24.3.4 Other functions\ntidyr has a few other useful rectangling functions that we’re not going to cover in this book:\n\n\nunnest_auto() automatically picks between unnest_longer() and unnest_wider() based on the structure of the list-column. It’s great for rapid exploration, but ultimately it’s a bad idea because it doesn’t force you to understand how your data is structured, and makes your code harder to understand.\n\nunnest() expands both rows and columns. It’s useful when you have a list-column that contains a 2d structure like a data frame, which you don’t see in this book, but you might encounter if you use the tidymodels ecosystem.\n\nThese functions are good to know about as you might encounter them when reading other people’s code or tackling rarer rectangling challenges yourself.\n\n24.3.5 Exercises\n\nWhat happens when you use unnest_wider() with unnamed list-columns like df2? What argument is now necessary? What happens to missing values?\nWhat happens when you use unnest_longer() with named list-columns like df1? What additional information do you get in the output? How can you suppress that extra detail?\n\nFrom time-to-time you encounter data frames with multiple list-columns with aligned values. For example, in the following data frame, the values of y and z are aligned (i.e. y and z will always have the same length within a row, and the first value of y corresponds to the first value of z). What happens if you apply two unnest_longer() calls to this data frame? How can you preserve the relationship between x and y? (Hint: carefully read the docs).\n\ndf4 <- tribble(\n  ~x, ~y, ~z,\n  \"a\", list(\"y-a-1\", \"y-a-2\"), list(\"z-a-1\", \"z-a-2\"),\n  \"b\", list(\"y-b-1\", \"y-b-2\", \"y-b-3\"), list(\"z-b-1\", \"z-b-2\", \"z-b-3\")\n)"
  },
  {
    "objectID": "rectangling.html#case-studies",
    "href": "rectangling.html#case-studies",
    "title": "24  Hierarchical data",
    "section": "\n24.4 Case studies",
    "text": "24.4 Case studies\nThe main difference between the simple examples we used above and real data is that real data typically contains multiple levels of nesting that require multiple calls to unnest_longer() and/or unnest_wider(). To show that in action, this section works through three real rectangling challenges using datasets from the repurrrsive package.\n\n24.4.1 Very wide data\nWe’ll start with gh_repos. This is a list that contains data about a collection of GitHub repositories retrieved using the GitHub API. It’s a very deeply nested list so it’s difficult to show the structure in this book; we recommend exploring a little on your own with View(gh_repos) before we continue.\ngh_repos is a list, but our tools work with list-columns, so we’ll begin by putting it into a tibble. We call this column json for reasons we’ll get to later.\n\nrepos <- tibble(json = gh_repos)\nrepos\n#> # A tibble: 6 × 1\n#>   json       \n#>   <list>     \n#> 1 <list [30]>\n#> 2 <list [30]>\n#> 3 <list [30]>\n#> 4 <list [26]>\n#> 5 <list [30]>\n#> 6 <list [30]>\n\nThis tibble contains 6 rows, one row for each child of gh_repos. Each row contains a unnamed list with either 26 or 30 rows. Since these are unnamed, we’ll start with unnest_longer() to put each child in its own row:\n\nrepos |> \n  unnest_longer(json)\n#> # A tibble: 176 × 1\n#>   json             \n#>   <list>           \n#> 1 <named list [68]>\n#> 2 <named list [68]>\n#> 3 <named list [68]>\n#> 4 <named list [68]>\n#> 5 <named list [68]>\n#> 6 <named list [68]>\n#> # ℹ 170 more rows\n\nAt first glance, it might seem like we haven’t improved the situation: while we have more rows (176 instead of 6) each element of json is still a list. However, there’s an important difference: now each element is a named list so we can use unnest_wider() to put each element into its own column:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) \n#> # A tibble: 176 × 68\n#>         id name        full_name         owner        private html_url       \n#>      <int> <chr>       <chr>             <list>       <lgl>   <chr>          \n#> 1 61160198 after       gaborcsardi/after <named list> FALSE   https://github…\n#> 2 40500181 argufy      gaborcsardi/argu… <named list> FALSE   https://github…\n#> 3 36442442 ask         gaborcsardi/ask   <named list> FALSE   https://github…\n#> 4 34924886 baseimports gaborcsardi/base… <named list> FALSE   https://github…\n#> 5 61620661 citest      gaborcsardi/cite… <named list> FALSE   https://github…\n#> 6 33907457 clisymbols  gaborcsardi/clis… <named list> FALSE   https://github…\n#> # ℹ 170 more rows\n#> # ℹ 62 more variables: description <chr>, fork <lgl>, url <chr>, …\n\nThis has worked but the result is a little overwhelming: there are so many columns that tibble doesn’t even print all of them! We can see them all with names(); and here we look at the first 10:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  names() |> \n  head(10)\n#>  [1] \"id\"          \"name\"        \"full_name\"   \"owner\"       \"private\"    \n#>  [6] \"html_url\"    \"description\" \"fork\"        \"url\"         \"forks_url\"\n\nLet’s pull out a few that look interesting:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description)\n#> # A tibble: 176 × 4\n#>         id full_name               owner             description             \n#>      <int> <chr>                   <list>            <chr>                   \n#> 1 61160198 gaborcsardi/after       <named list [17]> Run Code in the Backgro…\n#> 2 40500181 gaborcsardi/argufy      <named list [17]> Declarative function ar…\n#> 3 36442442 gaborcsardi/ask         <named list [17]> Friendly CLI interactio…\n#> 4 34924886 gaborcsardi/baseimports <named list [17]> Do we get warnings for …\n#> 5 61620661 gaborcsardi/citest      <named list [17]> Test R package and repo…\n#> 6 33907457 gaborcsardi/clisymbols  <named list [17]> Unicode symbols for CLI…\n#> # ℹ 170 more rows\n\nYou can use this to work back to understand how gh_repos was structured: each child was a GitHub user containing a list of up to 30 GitHub repositories that they created.\nowner is another list-column, and since it contains a named list, we can use unnest_wider() to get at the values:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description) |> \n  unnest_wider(owner)\n#> Error in `unnest_wider()`:\n#> ! Can't duplicate names between the affected columns and the original\n#>   data.\n#> ✖ These names are duplicated:\n#>   ℹ `id`, from `owner`.\n#> ℹ Use `names_sep` to disambiguate using the column name.\n#> ℹ Or use `names_repair` to specify a repair strategy.\n\nUh oh, this list column also contains an id column and we can’t have two id columns in the same data frame. As suggested, lets use names_sep to resolve the problem:\n\nrepos |> \n  unnest_longer(json) |> \n  unnest_wider(json) |> \n  select(id, full_name, owner, description) |> \n  unnest_wider(owner, names_sep = \"_\")\n#> # A tibble: 176 × 20\n#>         id full_name               owner_login owner_id owner_avatar_url     \n#>      <int> <chr>                   <chr>          <int> <chr>                \n#> 1 61160198 gaborcsardi/after       gaborcsardi   660288 https://avatars.gith…\n#> 2 40500181 gaborcsardi/argufy      gaborcsardi   660288 https://avatars.gith…\n#> 3 36442442 gaborcsardi/ask         gaborcsardi   660288 https://avatars.gith…\n#> 4 34924886 gaborcsardi/baseimports gaborcsardi   660288 https://avatars.gith…\n#> 5 61620661 gaborcsardi/citest      gaborcsardi   660288 https://avatars.gith…\n#> 6 33907457 gaborcsardi/clisymbols  gaborcsardi   660288 https://avatars.gith…\n#> # ℹ 170 more rows\n#> # ℹ 15 more variables: owner_gravatar_id <chr>, owner_url <chr>, …\n\nThis gives another wide dataset, but you can get the sense that owner appears to contain a lot of additional data about the person who “owns” the repository.\n\n24.4.2 Relational data\nNested data is sometimes used to represent data that we’d usually spread across multiple data frames. For example, take got_chars which contains data about characters that appear in the Game of Thrones books and TV series. Like gh_repos it’s a list, so we start by turning it into a list-column of a tibble:\n\nchars <- tibble(json = got_chars)\nchars\n#> # A tibble: 30 × 1\n#>   json             \n#>   <list>           \n#> 1 <named list [18]>\n#> 2 <named list [18]>\n#> 3 <named list [18]>\n#> 4 <named list [18]>\n#> 5 <named list [18]>\n#> 6 <named list [18]>\n#> # ℹ 24 more rows\n\nThe json column contains named elements, so we’ll start by widening it:\n\nchars |> \n  unnest_wider(json)\n#> # A tibble: 30 × 18\n#>   url                    id name            gender culture    born           \n#>   <chr>               <int> <chr>           <chr>  <chr>      <chr>          \n#> 1 https://www.anapio…  1022 Theon Greyjoy   Male   \"Ironborn\" \"In 278 AC or …\n#> 2 https://www.anapio…  1052 Tyrion Lannist… Male   \"\"         \"In 273 AC, at…\n#> 3 https://www.anapio…  1074 Victarion Grey… Male   \"Ironborn\" \"In 268 AC or …\n#> 4 https://www.anapio…  1109 Will            Male   \"\"         \"\"             \n#> 5 https://www.anapio…  1166 Areo Hotah      Male   \"Norvoshi\" \"In 257 AC or …\n#> 6 https://www.anapio…  1267 Chett           Male   \"\"         \"At Hag's Mire\"\n#> # ℹ 24 more rows\n#> # ℹ 12 more variables: died <chr>, alive <lgl>, titles <list>, …\n\nAnd selecting a few columns to make it easier to read:\n\ncharacters <- chars |> \n  unnest_wider(json) |> \n  select(id, name, gender, culture, born, died, alive)\ncharacters\n#> # A tibble: 30 × 7\n#>      id name              gender culture    born              died           \n#>   <int> <chr>             <chr>  <chr>      <chr>             <chr>          \n#> 1  1022 Theon Greyjoy     Male   \"Ironborn\" \"In 278 AC or 27… \"\"             \n#> 2  1052 Tyrion Lannister  Male   \"\"         \"In 273 AC, at C… \"\"             \n#> 3  1074 Victarion Greyjoy Male   \"Ironborn\" \"In 268 AC or be… \"\"             \n#> 4  1109 Will              Male   \"\"         \"\"                \"In 297 AC, at…\n#> 5  1166 Areo Hotah        Male   \"Norvoshi\" \"In 257 AC or be… \"\"             \n#> 6  1267 Chett             Male   \"\"         \"At Hag's Mire\"   \"In 299 AC, at…\n#> # ℹ 24 more rows\n#> # ℹ 1 more variable: alive <lgl>\n\nThis dataset contains also many list-columns:\n\nchars |> \n  unnest_wider(json) |> \n  select(id, where(is.list))\n#> # A tibble: 30 × 8\n#>      id titles    aliases    allegiances books     povBooks tvSeries playedBy\n#>   <int> <list>    <list>     <list>      <list>    <list>   <list>   <list>  \n#> 1  1022 <chr [2]> <chr [4]>  <chr [1]>   <chr [3]> <chr>    <chr>    <chr>   \n#> 2  1052 <chr [2]> <chr [11]> <chr [1]>   <chr [2]> <chr>    <chr>    <chr>   \n#> 3  1074 <chr [2]> <chr [1]>  <chr [1]>   <chr [3]> <chr>    <chr>    <chr>   \n#> 4  1109 <chr [1]> <chr [1]>  <NULL>      <chr [1]> <chr>    <chr>    <chr>   \n#> 5  1166 <chr [1]> <chr [1]>  <chr [1]>   <chr [3]> <chr>    <chr>    <chr>   \n#> 6  1267 <chr [1]> <chr [1]>  <NULL>      <chr [2]> <chr>    <chr>    <chr>   \n#> # ℹ 24 more rows\n\nLet’s explore the titles column. It’s an unnamed list-column, so we’ll unnest it into rows:\n\nchars |> \n  unnest_wider(json) |> \n  select(id, titles) |> \n  unnest_longer(titles)\n#> # A tibble: 59 × 2\n#>      id titles                                              \n#>   <int> <chr>                                               \n#> 1  1022 Prince of Winterfell                                \n#> 2  1022 Lord of the Iron Islands (by law of the green lands)\n#> 3  1052 Acting Hand of the King (former)                    \n#> 4  1052 Master of Coin (former)                             \n#> 5  1074 Lord Captain of the Iron Fleet                      \n#> 6  1074 Master of the Iron Victory                          \n#> # ℹ 53 more rows\n\nYou might expect to see this data in its own table because it would be easy to join to the characters data as needed. Let’s do that, which requires little cleaning: removing the rows containing empty strings and renaming titles to title since each row now only contains a single title.\n\ntitles <- chars |> \n  unnest_wider(json) |> \n  select(id, titles) |> \n  unnest_longer(titles) |> \n  filter(titles != \"\") |> \n  rename(title = titles)\ntitles\n#> # A tibble: 52 × 2\n#>      id title                                               \n#>   <int> <chr>                                               \n#> 1  1022 Prince of Winterfell                                \n#> 2  1022 Lord of the Iron Islands (by law of the green lands)\n#> 3  1052 Acting Hand of the King (former)                    \n#> 4  1052 Master of Coin (former)                             \n#> 5  1074 Lord Captain of the Iron Fleet                      \n#> 6  1074 Master of the Iron Victory                          \n#> # ℹ 46 more rows\n\nYou could imagine creating a table like this for each of the list-columns, then using joins to combine them with the character data as you need it.\n\n24.4.3 Deeply nested\nWe’ll finish off these case studies with a list-column that’s very deeply nested and requires repeated rounds of unnest_wider() and unnest_longer() to unravel: gmaps_cities. This is a two column tibble containing five city names and the results of using Google’s geocoding API to determine their location:\n\ngmaps_cities\n#> # A tibble: 5 × 2\n#>   city       json            \n#>   <chr>      <list>          \n#> 1 Houston    <named list [2]>\n#> 2 Washington <named list [2]>\n#> 3 New York   <named list [2]>\n#> 4 Chicago    <named list [2]>\n#> 5 Arlington  <named list [2]>\n\njson is a list-column with internal names, so we start with an unnest_wider():\n\ngmaps_cities |> \n  unnest_wider(json)\n#> # A tibble: 5 × 3\n#>   city       results    status\n#>   <chr>      <list>     <chr> \n#> 1 Houston    <list [1]> OK    \n#> 2 Washington <list [2]> OK    \n#> 3 New York   <list [1]> OK    \n#> 4 Chicago    <list [1]> OK    \n#> 5 Arlington  <list [2]> OK\n\nThis gives us the status and the results. We’ll drop the status column since they’re all OK; in a real analysis, you’d also want to capture all the rows where status != \"OK\" and figure out what went wrong. results is an unnamed list, with either one or two elements (we’ll see why shortly) so we’ll unnest it into rows:\n\ngmaps_cities |> \n  unnest_wider(json) |> \n  select(-status) |> \n  unnest_longer(results)\n#> # A tibble: 7 × 2\n#>   city       results         \n#>   <chr>      <list>          \n#> 1 Houston    <named list [5]>\n#> 2 Washington <named list [5]>\n#> 3 Washington <named list [5]>\n#> 4 New York   <named list [5]>\n#> 5 Chicago    <named list [5]>\n#> 6 Arlington  <named list [5]>\n#> # ℹ 1 more row\n\nNow results is a named list, so we’ll use unnest_wider():\n\nlocations <- gmaps_cities |> \n  unnest_wider(json) |> \n  select(-status) |> \n  unnest_longer(results) |> \n  unnest_wider(results)\nlocations\n#> # A tibble: 7 × 6\n#>   city       address_components formatted_address   geometry        \n#>   <chr>      <list>             <chr>               <list>          \n#> 1 Houston    <list [4]>         Houston, TX, USA    <named list [4]>\n#> 2 Washington <list [2]>         Washington, USA     <named list [4]>\n#> 3 Washington <list [4]>         Washington, DC, USA <named list [4]>\n#> 4 New York   <list [3]>         New York, NY, USA   <named list [4]>\n#> 5 Chicago    <list [4]>         Chicago, IL, USA    <named list [4]>\n#> 6 Arlington  <list [4]>         Arlington, TX, USA  <named list [4]>\n#> # ℹ 1 more row\n#> # ℹ 2 more variables: place_id <chr>, types <list>\n\nNow we can see why two cities got two results: Washington matched both Washington state and Washington, DC, and Arlington matched Arlington, Virginia and Arlington, Texas.\nThere are a few different places we could go from here. We might want to determine the exact location of the match, which is stored in the geometry list-column:\n\nlocations |> \n  select(city, formatted_address, geometry) |> \n  unnest_wider(geometry)\n#> # A tibble: 7 × 6\n#>   city       formatted_address   bounds           location     location_type\n#>   <chr>      <chr>               <list>           <list>       <chr>        \n#> 1 Houston    Houston, TX, USA    <named list [2]> <named list> APPROXIMATE  \n#> 2 Washington Washington, USA     <named list [2]> <named list> APPROXIMATE  \n#> 3 Washington Washington, DC, USA <named list [2]> <named list> APPROXIMATE  \n#> 4 New York   New York, NY, USA   <named list [2]> <named list> APPROXIMATE  \n#> 5 Chicago    Chicago, IL, USA    <named list [2]> <named list> APPROXIMATE  \n#> 6 Arlington  Arlington, TX, USA  <named list [2]> <named list> APPROXIMATE  \n#> # ℹ 1 more row\n#> # ℹ 1 more variable: viewport <list>\n\nThat gives us new bounds (a rectangular region) and location (a point). We can unnest location to see the latitude (lat) and longitude (lng):\n\nlocations |> \n  select(city, formatted_address, geometry) |> \n  unnest_wider(geometry) |> \n  unnest_wider(location)\n#> # A tibble: 7 × 7\n#>   city       formatted_address   bounds             lat    lng location_type\n#>   <chr>      <chr>               <list>           <dbl>  <dbl> <chr>        \n#> 1 Houston    Houston, TX, USA    <named list [2]>  29.8  -95.4 APPROXIMATE  \n#> 2 Washington Washington, USA     <named list [2]>  47.8 -121.  APPROXIMATE  \n#> 3 Washington Washington, DC, USA <named list [2]>  38.9  -77.0 APPROXIMATE  \n#> 4 New York   New York, NY, USA   <named list [2]>  40.7  -74.0 APPROXIMATE  \n#> 5 Chicago    Chicago, IL, USA    <named list [2]>  41.9  -87.6 APPROXIMATE  \n#> 6 Arlington  Arlington, TX, USA  <named list [2]>  32.7  -97.1 APPROXIMATE  \n#> # ℹ 1 more row\n#> # ℹ 1 more variable: viewport <list>\n\nExtracting the bounds requires a few more steps:\n\nlocations |> \n  select(city, formatted_address, geometry) |> \n  unnest_wider(geometry) |> \n  # focus on the variables of interest\n  select(!location:viewport) |>\n  unnest_wider(bounds)\n#> # A tibble: 7 × 4\n#>   city       formatted_address   northeast        southwest       \n#>   <chr>      <chr>               <list>           <list>          \n#> 1 Houston    Houston, TX, USA    <named list [2]> <named list [2]>\n#> 2 Washington Washington, USA     <named list [2]> <named list [2]>\n#> 3 Washington Washington, DC, USA <named list [2]> <named list [2]>\n#> 4 New York   New York, NY, USA   <named list [2]> <named list [2]>\n#> 5 Chicago    Chicago, IL, USA    <named list [2]> <named list [2]>\n#> 6 Arlington  Arlington, TX, USA  <named list [2]> <named list [2]>\n#> # ℹ 1 more row\n\nWe then rename southwest and northeast (the corners of the rectangle) so we can use names_sep to create short but evocative names:\n\nlocations |> \n  select(city, formatted_address, geometry) |> \n  unnest_wider(geometry) |> \n  select(!location:viewport) |>\n  unnest_wider(bounds) |> \n  rename(ne = northeast, sw = southwest) |> \n  unnest_wider(c(ne, sw), names_sep = \"_\") \n#> # A tibble: 7 × 6\n#>   city       formatted_address   ne_lat ne_lng sw_lat sw_lng\n#>   <chr>      <chr>                <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 Houston    Houston, TX, USA      30.1  -95.0   29.5  -95.8\n#> 2 Washington Washington, USA       49.0 -117.    45.5 -125. \n#> 3 Washington Washington, DC, USA   39.0  -76.9   38.8  -77.1\n#> 4 New York   New York, NY, USA     40.9  -73.7   40.5  -74.3\n#> 5 Chicago    Chicago, IL, USA      42.0  -87.5   41.6  -87.9\n#> 6 Arlington  Arlington, TX, USA    32.8  -97.0   32.6  -97.2\n#> # ℹ 1 more row\n\nNote how we unnest two columns simultaneously by supplying a vector of variable names to unnest_wider().\nOnce you’ve discovered the path to get to the components you’re interested in, you can extract them directly using another tidyr function, hoist():\n\nlocations |> \n  select(city, formatted_address, geometry) |> \n  hoist(\n    geometry,\n    ne_lat = c(\"bounds\", \"northeast\", \"lat\"),\n    sw_lat = c(\"bounds\", \"southwest\", \"lat\"),\n    ne_lng = c(\"bounds\", \"northeast\", \"lng\"),\n    sw_lng = c(\"bounds\", \"southwest\", \"lng\"),\n  )\n\nIf these case studies have whetted your appetite for more real-life rectangling, you can see a few more examples in vignette(\"rectangling\", package = \"tidyr\").\n\n24.4.4 Exercises\n\nRoughly estimate when gh_repos was created. Why can you only roughly estimate the date?\nThe owner column of gh_repo contains a lot of duplicated information because each owner can have many repos. Can you construct an owners data frame that contains one row for each owner? (Hint: does distinct() work with list-cols?)\nFollow the steps used for titles to create similar tables for the aliases, allegiances, books, and TV series for the Game of Thrones characters.\n\nExplain the following code line-by-line. Why is it interesting? Why does it work for got_chars but might not work in general?\n\ntibble(json = got_chars) |> \n  unnest_wider(json) |> \n  select(id, where(is.list)) |> \n  pivot_longer(\n    where(is.list), \n    names_to = \"name\", \n    values_to = \"value\"\n  ) |>  \n  unnest_longer(value)\n\n\nIn gmaps_cities, what does address_components contain? Why does the length vary between rows? Unnest it appropriately to figure it out. (Hint: types always appears to contain two elements. Does unnest_wider() make it easier to work with than unnest_longer()?) ."
  },
  {
    "objectID": "rectangling.html#json",
    "href": "rectangling.html#json",
    "title": "24  Hierarchical data",
    "section": "\n24.5 JSON",
    "text": "24.5 JSON\nAll of the case studies in the previous section were sourced from wild-caught JSON. JSON is short for javascript object notation and is the way that most web APIs return data. It’s important to understand it because while JSON and R’s data types are pretty similar, there isn’t a perfect 1-to-1 mapping, so it’s good to understand a bit about JSON if things go wrong.\n\n24.5.1 Data types\nJSON is a simple format designed to be easily read and written by machines, not humans. It has six key data types. Four of them are scalars:\n\nThe simplest type is a null (null) which plays the same role as NA in R. It represents the absence of data.\nA string is much like a string in R, but must always use double quotes.\nA number is similar to R’s numbers: they can use integer (e.g., 123), decimal (e.g., 123.45), or scientific (e.g., 1.23e3) notation. JSON doesn’t support Inf, -Inf, or NaN.\nA boolean is similar to R’s TRUE and FALSE, but uses lowercase true and false.\n\nJSON’s strings, numbers, and booleans are pretty similar to R’s character, numeric, and logical vectors. The main difference is that JSON’s scalars can only represent a single value. To represent multiple values you need to use one of the two remaining types: arrays and objects.\nBoth arrays and objects are similar to lists in R; the difference is whether or not they’re named. An array is like an unnamed list, and is written with []. For example [1, 2, 3] is an array containing 3 numbers, and [null, 1, \"string\", false] is an array that contains a null, a number, a string, and a boolean. An object is like a named list, and is written with {}. The names (keys in JSON terminology) are strings, so must be surrounded by quotes. For example, {\"x\": 1, \"y\": 2} is an object that maps x to 1 and y to 2.\nNote that JSON doesn’t have any native way to represent dates or date-times, so they’re often stored as strings, and you’ll need to use readr::parse_date() or readr::parse_datetime() to turn them into the correct data structure. Similarly, JSON’s rules for representing floating point numbers in JSON are a little imprecise, so you’ll also sometimes find numbers stored in strings. Apply readr::parse_double() as needed to get the correct variable type.\n\n24.5.2 jsonlite\nTo convert JSON into R data structures, we recommend the jsonlite package, by Jeroen Ooms. We’ll use only two jsonlite functions: read_json() and parse_json(). In real life, you’ll use read_json() to read a JSON file from disk. For example, the repurrsive package also provides the source for gh_user as a JSON file and you can read it with read_json():\n\n# A path to a json file inside the package:\ngh_users_json()\n#> [1] \"D:/1.study/R/R-4.2.0/library/repurrrsive/extdata/gh_users.json\"\n\n# Read it with read_json()\ngh_users2 <- read_json(gh_users_json())\n\n# Check it's the same as the data we were using previously\nidentical(gh_users, gh_users2)\n#> [1] TRUE\n\nIn this book, we’ll also use parse_json(), since it takes a string containing JSON, which makes it good for generating simple examples. To get started, here are three simple JSON datasets, starting with a number, then putting a few numbers in an array, then putting that array in an object:\n\nstr(parse_json('1'))\n#>  int 1\nstr(parse_json('[1, 2, 3]'))\n#> List of 3\n#>  $ : int 1\n#>  $ : int 2\n#>  $ : int 3\nstr(parse_json('{\"x\": [1, 2, 3]}'))\n#> List of 1\n#>  $ x:List of 3\n#>   ..$ : int 1\n#>   ..$ : int 2\n#>   ..$ : int 3\n\njsonlite has another important function called fromJSON(). We don’t use it here because it performs automatic simplification (simplifyVector = TRUE). This often works well, particularly in simple cases, but we think you’re better off doing the rectangling yourself so you know exactly what’s happening and can more easily handle the most complicated nested structures.\n\n24.5.3 Starting the rectangling process\nIn most cases, JSON files contain a single top-level array, because they’re designed to provide data about multiple “things”, e.g., multiple pages, or multiple records, or multiple results. In this case, you’ll start your rectangling with tibble(json) so that each element becomes a row:\n\njson <- '[\n  {\"name\": \"John\", \"age\": 34},\n  {\"name\": \"Susan\", \"age\": 27}\n]'\ndf <- tibble(json = parse_json(json))\ndf\n#> # A tibble: 2 × 1\n#>   json            \n#>   <list>          \n#> 1 <named list [2]>\n#> 2 <named list [2]>\n\ndf |> \n  unnest_wider(json)\n#> # A tibble: 2 × 2\n#>   name    age\n#>   <chr> <int>\n#> 1 John     34\n#> 2 Susan    27\n\nIn rarer cases, the JSON file consists of a single top-level JSON object, representing one “thing”. In this case, you’ll need to kick off the rectangling process by wrapping it in a list, before you put it in a tibble.\n\njson <- '{\n  \"status\": \"OK\", \n  \"results\": [\n    {\"name\": \"John\", \"age\": 34},\n    {\"name\": \"Susan\", \"age\": 27}\n ]\n}\n'\ndf <- tibble(json = list(parse_json(json)))\ndf\n#> # A tibble: 1 × 1\n#>   json            \n#>   <list>          \n#> 1 <named list [2]>\n\ndf |> \n  unnest_wider(json) |> \n  unnest_longer(results) |> \n  unnest_wider(results)\n#> # A tibble: 2 × 3\n#>   status name    age\n#>   <chr>  <chr> <int>\n#> 1 OK     John     34\n#> 2 OK     Susan    27\n\nAlternatively, you can reach inside the parsed JSON and start with the bit that you actually care about:\n\ndf <- tibble(results = parse_json(json)$results)\ndf |> \n  unnest_wider(results)\n#> # A tibble: 2 × 2\n#>   name    age\n#>   <chr> <int>\n#> 1 John     34\n#> 2 Susan    27\n\n\n24.5.4 Exercises\n\n\nRectangle the df_col and df_row below. They represent the two ways of encoding a data frame in JSON.\n\njson_col <- parse_json('\n  {\n    \"x\": [\"a\", \"x\", \"z\"],\n    \"y\": [10, null, 3]\n  }\n')\njson_row <- parse_json('\n  [\n    {\"x\": \"a\", \"y\": 10},\n    {\"x\": \"x\", \"y\": null},\n    {\"x\": \"z\", \"y\": 3}\n  ]\n')\n\ndf_col <- tibble(json = list(json_col)) \ndf_row <- tibble(json = json_row)"
  },
  {
    "objectID": "rectangling.html#summary",
    "href": "rectangling.html#summary",
    "title": "24  Hierarchical data",
    "section": "\n24.6 Summary",
    "text": "24.6 Summary\nIn this chapter, you learned what lists are, how you can generate them from JSON files, and how turn them into rectangular data frames. Surprisingly we only need two new functions: unnest_longer() to put list elements into rows and unnest_wider() to put list elements into columns. It doesn’t matter how deeply nested the list-column is; all you need to do is repeatedly call these two functions.\nJSON is the most common data format returned by web APIs. What happens if the website doesn’t have an API, but you can see data you want on the website? That’s the topic of the next chapter: web scraping, extracting data from HTML webpages."
  },
  {
    "objectID": "data-import.html#introduction",
    "href": "data-import.html#introduction",
    "title": "8  Data import",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\n使用 R 包提供的数据是学习数据科学工具的好方法，但您希望在某个时候将所学到的知识应用到自己的数据中。 在本章中，您将学习将数据文件读入 R 的基础知识。\n具体来说，本章将重点关注读取纯文本矩形文件。 我们将从处理列名称、类型和缺失数据等功能的实用建议开始。 然后，您将了解如何同时从多个文件读取数据以及将数据从 R 写入文件。 最后，您将学习如何在 R 中手工制作 data frames。\n\n8.1.1 Prerequisites\n在本章中，您将学习如何使用 readr 包在 R 中加载平面文件，该包是 tidyverse 核心的一部分。\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "data-import.html#reading-data-from-a-file",
    "href": "data-import.html#reading-data-from-a-file",
    "title": "8  Data import",
    "section": "\n8.2 Reading data from a file",
    "text": "8.2 Reading data from a file\n首先，我们将重点关注最常见的矩形数据文件类型：CSV，它是逗号分隔值（comma-separated values）的缩写。 简单的 CSV 文件如下所示。 第一行，通常称为标题行，给出列名称，接下来的六行提供数据。 各列由逗号分隔。\n\nStudent ID,Full Name,favourite.food,mealPlan,AGE\n1,Sunil Huffmann,Strawberry yoghurt,Lunch only,4\n2,Barclay Lynn,French fries,Lunch only,5\n3,Jayendra Lyne,N/A,Breakfast and lunch,7\n4,Leon Rossini,Anchovies,Lunch only,\n5,Chidiegwu Dunkel,Pizza,Breakfast and lunch,five\n6,Güvenç Attila,Ice cream,Lunch only,6\n\nTable 8.1 显示了与表格相同的数据的表示形式。\n\n\n\n\nTable 8.1: Data from the students.csv file as a table.\n\n\n\n\n\n\n\n\nStudent ID\nFull Name\nfavourite.food\nmealPlan\nAGE\n\n\n\n1\nSunil Huffmann\nStrawberry yoghurt\nLunch only\n4\n\n\n2\nBarclay Lynn\nFrench fries\nLunch only\n5\n\n\n3\nJayendra Lyne\nN/A\nBreakfast and lunch\n7\n\n\n4\nLeon Rossini\nAnchovies\nLunch only\nNA\n\n\n5\nChidiegwu Dunkel\nPizza\nBreakfast and lunch\nfive\n\n\n6\nGüvenç Attila\nIce cream\nLunch only\n6\n\n\n\n\n\n\n我们可以使用 read_csv() 将该文件读入 R。 第一个参数是最重要的：文件的路径。 您可以将路径视为文件的地址：该文件名为 students.csv，位于 data 文件夹中。\n\nstudents <- read_csv(\"data/students.csv\")\n#> Rows: 6 Columns: 5\n#> ── Column specification ─────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr (4): Full Name, favourite.food, mealPlan, AGE\n#> dbl (1): Student ID\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n如果项目的 data 文件夹中有 students.csv 文件，则上面的代码将起作用。 您可以从 https://pos.it/r4ds-students-csv 下载 students.csv 文件，也可以使用以下命令直接从该 URL 读取它：\n\nstudents <- read_csv(\"https://pos.it/r4ds-students-csv\")\n\n当您运行 read_csv() 时，它会打印一条消息，告诉您数据的行数和列数、使用的分隔符（delimiter）以及列规范（按列包含的数据类型组织的列名称）。 它还打印出一些有关检索完整列规范以及如何消除此消息的信息。 该消息是 readr 不可或缺的一部分，我们将在 Section 8.3 中返回它。\n\n8.2.1 Practical advice\n读入数据后，第一步通常涉及以某种方式对其进行转换，以便更容易在其余分析中使用。 考虑到这一点，让我们再看一下 students 数据。\n\nstudents\n#> # A tibble: 6 × 5\n#>   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#>          <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2            2 Barclay Lynn     French fries       Lunch only          5    \n#> 3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n#> 4            4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\n在 favourite.food 列中，有一堆食物项目，然后是字符串 N/A，这应该是真正的 NA，R 会识别为 “not available”。 我们可以使用 na 参数来解决这个问题。 默认情况下，read_csv() 仅将该数据集中的空字符串（\"\"）识别为 NAs，我们希望它也识别字符串 \"N/A\"。\n\nstudents <- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\nstudents\n#> # A tibble: 6 × 5\n#>   `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n#>          <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2            2 Barclay Lynn     French fries       Lunch only          5    \n#> 3            3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4            4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6            6 Güvenç Attila    Ice cream          Lunch only          6\n\n您可能还会注意到 Student ID 和 Full Name 列周围有反引号。 这是因为它们包含空格，违反了 R 变量名称的通常规则； 它们是非语法（non-syntactic）名称。 要引用这些变量，您需要用反引号将它们括起来，`：\n\nstudents |> \n  rename(\n    student_id = `Student ID`,\n    full_name = `Full Name`\n  )\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite.food     mealPlan            AGE  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n另一种方法是使用 janitor::clean_names() 来使用一些启发式方法将它们一次性全部变成 snake case1。\n\nstudents |> janitor::clean_names()\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <chr>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n读入数据后的另一个常见任务是考虑变量类型。 例如，meal_plan 是一个具有一组已知可能值的分类变量，在 R 中应表示为一个因子（factor）：\n\nstudents |>\n  janitor::clean_names() |>\n  mutate(meal_plan = factor(meal_plan))\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan           age  \n#>        <dbl> <chr>            <chr>              <fct>               <chr>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n#> 2          2 Barclay Lynn     French fries       Lunch only          5    \n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch 7    \n#> 4          4 Leon Rossini     Anchovies          Lunch only          <NA> \n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n#> 6          6 Güvenç Attila    Ice cream          Lunch only          6\n\n请注意，meal_plan 变量中的值保持不变，但变量名称下方表示的变量类型已从 character (<chr>) 更改为 factor ()。 您将在 Chapter 17 中了解有关 factors 的更多信息。\n在分析这些数据之前，您可能需要修复 age 和 id 列。 目前，age 是一个 character 变量，因为其中一个观测结果被输入为 five，而不是数字 5。 我们将在 Chapter 21 中讨论解决此问题的详细信息。\n\nstudents <- students |>\n  janitor::clean_names() |>\n  mutate(\n    meal_plan = factor(meal_plan),\n    age = parse_number(if_else(age == \"five\", \"5\", age))\n  )\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <fct>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n这里的一个新函数是 if_else()，它有三个参数。 第一个参数 test 应该是一个逻辑向量。 当 test 为 TRUE 时，结果将包含第二个参数 yes 的值；当 test 为 FALSE 时，结果将包含第三个参数 no 的值。 这里我们说如果 age 是字符串 \"five\"，则将其设为 \"5\"，如果不是则将其保留为 age。 您将在 Chapter 13 中了解有关 if_else() 和逻辑向量的更多信息。\n\n8.2.2 Other arguments\n我们还需要提及其他几个重要的参数，如果我们首先向您展示一个方便的技巧，它们将更容易演示：read_csv() 可以读取您创建的文本字符串并像 CSV 文件一样格式化：\n\nread_csv(\n  \"a,b,c\n  1,2,3\n  4,5,6\"\n)\n#> # A tibble: 2 × 3\n#>       a     b     c\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\n通常，read_csv() 使用数据的第一行作为列名，这是一个非常常见的约定。 但文件顶部包含几行 metadata 的情况并不少见。 您可以使用 skip = n 跳过前 n 行，或使用 comment = \"#\" 删除所有以（例如）# 开头的行：\n\nread_csv(\n  \"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2\n)\n#> # A tibble: 1 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\nread_csv(\n  \"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\"\n)\n#> # A tibble: 1 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\n在其他情况下，数据可能没有列名称。 您可以使用 col_names = FALSE 告诉 read_csv() 不要将第一行视为标题，而是从 X1 到 Xn 按顺序标记它们：\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = FALSE\n)\n#> # A tibble: 2 × 3\n#>      X1    X2    X3\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\n或者，您可以向 col_names 传递一个字符向量，该向量将用作列名称：\n\nread_csv(\n  \"1,2,3\n  4,5,6\",\n  col_names = c(\"x\", \"y\", \"z\")\n)\n#> # A tibble: 2 × 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n\n阅读实践中遇到的大多数 CSV 文件时，您只需要了解这些参数即可。 （对于其余的，您需要仔细检查 .csv 文件并阅读 read_csv() 的许多其他参数的文档。）\n\n8.2.3 Other file types\n一旦掌握了 read_csv()，使用 readr 的其他函数就很简单了；只需要知道要使用哪个函数即可：\n\nread_csv2() 读取以分号分隔的（semicolon-separated）文件。 这些使用 ; 而不是 , 来分隔字段，并且在使用 , 作为小数点标记的国家/地区很常见。\nread_tsv() 读取制表符分隔的（tab-delimited）文件。\nread_delim() 读取带有任何分隔符的文件，如果您未指定分隔符，则会尝试自动猜测分隔符。\nread_fwf() 读取固定宽度的文件。 您可以使用 fwf_widths() 按宽度指定字段，或使用 fwf_positions() 按字段位置指定字段。\nread_table() 读取固定宽度文件的常见变体，其中列由空格分隔。\nread_log() 读取 Apache 风格的日志文件。\n\n8.2.4 Exercises\n\n您将使用什么函数来读取字段以 “|” 分隔的文件？\n除了 file、skip 和 comment 之外，read_csv() 和 read_tsv() 还有哪些其他参数的共同点？\nread_fwf() 最重要的参数是什么？\n\n有时 CSV 文件中的字符串包含逗号。 为了防止它们引起问题，它们需要被引号字符包围，例如 \" 或 '。默认情况下，read_csv() 假设引号字符为 \"。 要将以下文本读入数据框中，您需要为 read_csv() 指定什么参数？\n\n\"x,y\\n1,'a,b'\"\n\n\n\n确定以下每个内联 CSV 文件的问题所在。 运行代码时会发生什么？\n\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\nread_csv(\"a,b\\n\\\"1\")\nread_csv(\"a,b\\n1,2\\na,b\")\nread_csv(\"a;b\\n1;3\")\n\n\n\n通过以下方式练习引用以下数据框中的非语法名称：\n\n提取名为 1 的变量。\n绘制 1 与 2 的散点图。\n创建一个名为 3 的新列，即 2 除以 1。\n将列重命名为 one、two 和 three。\n\n\nannoying <- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)"
  },
  {
    "objectID": "data-import.html#sec-col-types",
    "href": "data-import.html#sec-col-types",
    "title": "8  Data import",
    "section": "\n8.3 Controlling column types",
    "text": "8.3 Controlling column types\nCSV 文件不包含有关每个变量类型的任何信息（i.e. whether it’s a logical, number, string, etc.），因此 readr 会尝试猜测类型。 本节介绍猜测过程的工作原理、如何解决导致其失败的一些常见问题，以及（如果需要）如何自行提供列类型。 最后，我们将提到一些通用策略，如果 readr 发生灾难性失败并且您需要更深入地了解文件结构，这些策略将非常有用。\n\n8.3.1 Guessing types\nreadr 使用启发式方法来确定列类型。 对于每一列，它会提取从第一行到最后一行均匀间隔的 1,0002 行的值，忽略缺失值。 然后它会回答以下问题：\n\n它仅包含 F、T、FALSE 或 TRUE（忽略大小写）吗？如果是这样，那就合乎逻辑了。\n它是否仅包含数字（e.g., 1, -4.5, 5e6, Inf）？如果是这样，它就是一个 number。\n是否符合 ISO8601 标准？如果是，则它是一个 date 或 date-time。（我们将在 ?sec-creating-datetimes 中更详细地返回 date-times）。\n否则，它必须是一个 string。\n\n您可以在这个简单的示例中看到该行为的实际效果：\n\nread_csv(\"\n  logical,numeric,date,string\n  TRUE,1,2021-01-15,abc\n  false,4.5,2021-02-15,def\n  T,Inf,2021-02-16,ghi\n\")\n#> # A tibble: 3 × 4\n#>   logical numeric date       string\n#>   <lgl>     <dbl> <date>     <chr> \n#> 1 TRUE        1   2021-01-15 abc   \n#> 2 FALSE       4.5 2021-02-15 def   \n#> 3 TRUE      Inf   2021-02-16 ghi\n\n如果你有一个干净的数据集，这种启发式方法会很有效，但在现实生活中，你会遇到一系列奇怪而美丽的失败。\n\n8.3.2 Missing values, column types, and problems\n列检测失败的最常见方式是列包含意外值，并且您得到的是字符列而不是更具体的类型。 造成这种情况的最常见原因之一是缺失值，该值是使用 readr 期望的 NA 之外的其他内容记录的。\n以这个简单的 1 列 CSV 文件为例：\n\nsimple_csv <- \"\n  x\n  10\n  .\n  20\n  30\"\n\n如果我们在没有任何附加参数的情况下读取它，x 就会成为一个 character 列：\n\nread_csv(simple_csv)\n#> # A tibble: 4 × 1\n#>   x    \n#>   <chr>\n#> 1 10   \n#> 2 .    \n#> 3 20   \n#> 4 30\n\n在这个非常小的情况下，您可以轻松地看到缺失值 .。 但是，如果您有数千行，其中只有少数由 .s 表示的缺失值，会发生什么情况？ 一种方法是告诉 readr x 是一个 numeric 列，然后查看它在哪里失败。 您可以使用 col_types 参数来执行此操作，该参数采用命名列表，其中名称与 CSV 文件中的列名称匹配：\n\ndf <- read_csv(\n  simple_csv, \n  col_types = list(x = col_double())\n)\n#> Warning: One or more parsing issues, call `problems()` on your data frame for\n#> details, e.g.:\n#>   dat <- vroom(...)\n#>   problems(dat)\n\n现在 read_csv() 报告存在问题，并告诉我们可以通过 problems() 找到更多信息：\n\nproblems(df)\n#> # A tibble: 1 × 5\n#>     row   col expected actual file                                           \n#>   <int> <int> <chr>    <chr>  <chr>                                          \n#> 1     3     1 a double .      C:/Users/13081/AppData/Local/Temp/RtmpmMITeQ/f…\n\n这告诉我们，第 3 行第 1 列存在问题，其中 readr 期望为 double，但得到的是 .. 这表明该数据集使用 . 对于缺失值。 然后我们设置 na = \".\"，自动猜测成功，得到我们想要的 numeric 列：\n\nread_csv(simple_csv, na = \".\")\n#> # A tibble: 4 × 1\n#>       x\n#>   <dbl>\n#> 1    10\n#> 2    NA\n#> 3    20\n#> 4    30\n\n\n8.3.3 Column types\nreadr 总共提供了九种列类型供您使用：\n\n\ncol_logical() 和 col_double() 读取逻辑数和实数。它们相对很少需要（除了上述情况），因为 readr 通常会为您猜测它们。\n\ncol_integer() 读取整数。在本书中，我们很少区分整数（integers）和双精度数（doubles），因为它们在功能上是等效的，但显式读取整数有时会很有用，因为它们占用双精度数一半的内存。\n\ncol_character() 读取字符串（strings）。当您有一列是数字标识符（即标识对象的一长串数字，但对其应用数学运算没有意义）时，这对于显式指定非常有用。示例包括电话号码、社会安全号码、信用卡号码等。\n\ncol_factor()、col_date() 和 col_datetime() 分别创建因子（factors）、日期（dates）和日期时间（date-times）；当我们在 Chapter 17 和 ?sec-dates-and-times 中介绍这些数据类型时，您将了解更多信息。\n\ncol_number() 是一个宽松的数字解析器，它将忽略非数字组件，对于货币特别有用。您将在 Chapter 14 中了解更多信息。\n\ncol_skip() 会跳过一列，因此它不会包含在结果中，如果您有一个大型 CSV 文件并且只想使用某些列，这对于加快读取数据的速度很有用。\n\n还可以通过从 list() 切换到 cols() 并指定 .default 来覆盖默认列：\n\nanother_csv <- \"\nx,y,z\n1,2,3\"\n\nread_csv(\n  another_csv, \n  col_types = cols(.default = col_character())\n)\n#> # A tibble: 1 × 3\n#>   x     y     z    \n#>   <chr> <chr> <chr>\n#> 1 1     2     3\n\n另一个有用的帮助器是 cols_only() ，它只会读取您指定的列：\n\nread_csv(\n  another_csv,\n  col_types = cols_only(x = col_character())\n)\n#> # A tibble: 1 × 1\n#>   x    \n#>   <chr>\n#> 1 1"
  },
  {
    "objectID": "data-import.html#sec-readr-directory",
    "href": "data-import.html#sec-readr-directory",
    "title": "8  Data import",
    "section": "\n8.4 Reading data from multiple files",
    "text": "8.4 Reading data from multiple files\n有时，您的数据会分割在多个文件中，而不是包含在单个文件中。 例如，您可能有多个月的销售数据，每个月的数据都位于单独的文件中：01-sales.csv for January, 02-sales.csv for February, and 03-sales.csv for March。 使用 read_csv()，您可以立即读取这些数据并将它们堆叠在单个数据框中。\n\nsales_files <- c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\nread_csv(sales_files, id = \"file\")\n#> # A tibble: 19 × 6\n#>   file              month    year brand  item     n\n#>   <chr>             <chr>   <dbl> <dbl> <dbl> <dbl>\n#> 1 data/01-sales.csv January  2019     1  1234     3\n#> 2 data/01-sales.csv January  2019     1  8721     9\n#> 3 data/01-sales.csv January  2019     1  1822     2\n#> 4 data/01-sales.csv January  2019     2  3333     1\n#> 5 data/01-sales.csv January  2019     2  2156     9\n#> 6 data/01-sales.csv January  2019     2  3987     6\n#> # ℹ 13 more rows\n\n再次强调，如果项目的 data 文件夹中有 CSV 文件，则上面的代码将起作用。 您可以从 https://pos.it/r4ds-01-sales, https://pos.it/r4ds-02-sales, and https://pos.it/r4ds-03-sales 下载这些文件，或者您可以直接读取它们：\n\nsales_files <- c(\n  \"https://pos.it/r4ds-01-sales\",\n  \"https://pos.it/r4ds-02-sales\",\n  \"https://pos.it/r4ds-03-sales\"\n)\nread_csv(sales_files, id = \"file\")\n\nid 参数将一个名为 file 的新列添加到结果数据框中，用于标识数据来自的文件。 当您正在阅读的文件没有可帮助您将观察结果追溯到其原始来源的识别列的情况下，这尤其有用。\n如果您想要读入许多文件，则将它们的名称写为列表可能会很麻烦。 相反，您可以使用基本的 list.files() 函数通过匹配文件名中的模式来查找文件。 您将在 Chapter 16 中了解有关这些模式的更多信息。\n\nsales_files <- list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\nsales_files\n#> [1] \"data/01-sales.csv\" \"data/02-sales.csv\" \"data/03-sales.csv\""
  },
  {
    "objectID": "data-import.html#sec-writing-to-a-file",
    "href": "data-import.html#sec-writing-to-a-file",
    "title": "8  Data import",
    "section": "\n8.5 Writing to a file",
    "text": "8.5 Writing to a file\nreadr 还附带两个有用的函数，用于将数据写回磁盘：write_csv() 和 write_tsv()。 这些函数最重要的参数是 x（要保存的数据框）和 file（保存数据的位置）。 您还可以指定如何使用 na 写入缺失值，以及是否要 append 到现有文件。\n\nwrite_csv(students, \"students.csv\")\n\n现在让我们重新读取该 csv 文件。 请注意，保存到 CSV 时，您刚刚设置的变量类型信息会丢失，因为您要再次从纯文本文件中读取：\n\nstudents\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <fct>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\nwrite_csv(students, \"students-2.csv\")\nread_csv(\"students-2.csv\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <chr>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n这使得 CSVs 对于缓存临时结果有点不可靠 — 每次加载时都需要重新创建列规范。 有两个主要替代方案：\n\n\nwrite_rds() 和 read_rds() 是基本函数 readRDS() 和 saveRDS() 的统一包装器。 它们以 R 的自定义二进制格式（称为 RDS）存储数据。 这意味着当您重新加载对象时，您将加载与存储的完全相同的 R 对象。\n\nwrite_rds(students, \"students.rds\")\nread_rds(\"students.rds\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <fct>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    <NA>               Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\narrow 包允许您读取和写入 parquet 文件，这是一种可以跨编程语言共享的快速二进制文件格式。 我们将在 ?sec-arrow 中更深入地回到 arrow。\n\nlibrary(arrow)\nwrite_parquet(students, \"students.parquet\")\nread_parquet(\"students.parquet\")\n#> # A tibble: 6 × 5\n#>   student_id full_name        favourite_food     meal_plan             age\n#>        <dbl> <chr>            <chr>              <fct>               <dbl>\n#> 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4\n#> 2          2 Barclay Lynn     French fries       Lunch only              5\n#> 3          3 Jayendra Lyne    NA                 Breakfast and lunch     7\n#> 4          4 Leon Rossini     Anchovies          Lunch only             NA\n#> 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5\n#> 6          6 Güvenç Attila    Ice cream          Lunch only              6\n\n\n\nParquet 往往比 RDS 快得多，并且可以在 R 之外使用，但需要 arrow 包。"
  },
  {
    "objectID": "data-import.html#data-entry",
    "href": "data-import.html#data-entry",
    "title": "8  Data import",
    "section": "\n8.6 Data entry",
    "text": "8.6 Data entry\n有时，您需要在 R 脚本中输入一些数据，“手动”组装 tibble。 有两个有用的函数可以帮助您执行此操作，这两个函数的不同之处在于您是按列还是按行布局 tibble。 tibble() 按列工作：\n\ntibble(\n  x = c(1, 2, 5), \n  y = c(\"h\", \"m\", \"g\"),\n  z = c(0.08, 0.83, 0.60)\n)\n#> # A tibble: 3 × 3\n#>       x y         z\n#>   <dbl> <chr> <dbl>\n#> 1     1 h      0.08\n#> 2     2 m      0.83\n#> 3     5 g      0.6\n\n按列布局数据可能会让人很难看出行之间的关系，因此另一种选择是 tribble()，它是 transposed tibble 的缩写，它可以让您逐行布局数据。 tribble() 是为代码中的数据输入而定制的：列标题以 ~ 开头，条目之间用逗号分隔。 这使得以易于阅读的形式布置少量数据成为可能：\n\ntribble(\n  ~x, ~y, ~z,\n  1, \"h\", 0.08,\n  2, \"m\", 0.83,\n  5, \"g\", 0.60\n)\n#> # A tibble: 3 × 3\n#>       x y         z\n#>   <dbl> <chr> <dbl>\n#> 1     1 h      0.08\n#> 2     2 m      0.83\n#> 3     5 g      0.6"
  },
  {
    "objectID": "data-import.html#summary",
    "href": "data-import.html#summary",
    "title": "8  Data import",
    "section": "\n8.7 Summary",
    "text": "8.7 Summary\n在本章中，您学习了如何使用 read_csv() 加载 CSV 文件以及如何使用 tibble() 和 tribble() 进行自己的数据输入。 您已经了解了 csv 文件的工作原理、可能遇到的一些问题以及如何克服这些问题。 我们将在本书中多次介绍数据导入：Chapter 21 来自 Excel 和 Google Sheets，?sec-import-databases 将向您展示如何从数据库加载数据，?sec-arrow 来自 parquet 文件，Chapter 24 来自 JSON ，Chapter 25 来自网站。\n我们即将读到本书这一部分的结尾，但最后还有一个重要的主题要讨论：如何获得帮助。 因此，在下一章中，您将了解一些寻求帮助的好地方、如何创建 reprex 以最大限度地提高获得良好帮助的机会，以及一些与 R 世界保持同步的一般建议。"
  },
  {
    "objectID": "data-visualize.html#introduction",
    "href": "data-visualize.html#introduction",
    "title": "2  Data visualization",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR 有几种用于制作图形的系统，但 ggplot2 是最优雅和最多功能的之一。 ggplot2 实现了图形语法（grammar of graphics），这是一套一致的描述和构建图形的系统。 通过 ggplot2，你可以更多更快地学习一个系统，并在许多地方应用。\n本章将教您如何使用 ggplot2 可视化数据。 我们将从创建一个简单的散点图（scatterplot）开始，并使用它来介绍美学映射（aesthetic mappings）和几何对象（geometric objects）– ggplot2 的基本构建块。 然后，我们将向您展示如何可视化单个变量的分布以及可视化两个或多个变量之间的关系。 最后，我们将介绍如何保存您的图形并提供故障排除提示。\n\n2.1.1 Prerequisites\n本章重点介绍 ggplot2，它是 tidyverse 中的核心包之一。 要访问本章中使用的数据集、帮助页面和函数，请运行以下命令加载 tidyverse：\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\n#> ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n这一行代码加载了核心的 tidyverse，这些包在几乎每个数据分析中都会使用。 它还会告诉您 tidyverse 中的哪些函数与 base R（或其他已加载的包）中的函数存在冲突1。\n如果您运行此代码并收到 there is no package called 'tidyverse' 的错误消息，则需要先安装它，然后再次运行 library()。\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n您只需要安装一个包一次，但每次开始新会话时都需要加载（load）它。\n除了 tidyverse 之外，我们还将使用 palmerpenguins 包，其中包含了 penguins 数据集，该数据集包含了帕尔默群岛上三个岛屿上企鹅的身体测量数据。还 有 ggthemes 包，它提供了一个适用于色盲的安全调色板。\n\nlibrary(palmerpenguins)\n#> Warning: package 'palmerpenguins' was built under R version 4.2.2\nlibrary(ggthemes)"
  },
  {
    "objectID": "data-visualize.html#first-steps",
    "href": "data-visualize.html#first-steps",
    "title": "2  Data visualization",
    "section": "\n2.2 First steps",
    "text": "2.2 First steps\n企鹅的翼展较长的是否比翼展较短的体重更重还是更轻？ 您可能已经有了答案，但请尽量给出精确的回答。 翼展长度和体重之间的关系是怎样的？ 是正相关的吗？ 还是负相关的？ 是线性的吗？ 还是非线性的？ 这种关系是否因企鹅的物种而异？ 岛屿的差异是否会对关系产生影响？ 让我们创建可视化图表来回答这些问题。\n\n2.2.1 The penguins data frame\n您可以使用 palmerpenguins 包中的 penguins 数据框（data frame）来测试您对这些问题的回答（即 palmerpenguins::penguins）。 data frame 是一种由变量（列）和观测（行）组成的矩形集合。 penguins 包含 344 个观测值，由 Dr. Kristen Gorman 和 Palmer Station, Antarctica LTER 搜集提供2。\n为了方便讨论，让我们定义一些术语： - A variable 是可以进行测量的数量、特性或属性。\n\nA value 是在测量时 variable 所处的状态。 variable 的 value 可能会在每次测量时发生变化。\nAn observation 是在类似条件下进行的一组测量（通常在同一时间和同一对象上进行所有测量）。 一个 observation 会包含多个 values，每个 value 与不同的 variable 相关联。 我们有时将一个 observation 称为一个数据点。\nTabular data 是一组 values，每个 value 与一个 variable 和一个 observation 相关联。 如果每个 value 都放置在自己的”单元格”中，每个 variable 都在自己的列中，每个 observation 都在自己的行中，那么 Tabular data 就是整洁（tidy）的。\n\n在这个案例中，variable 指的是所有企鹅的属性，observation 指的是单个企鹅的所有属性。\n在控制台中键入 data frame 的名称，R 将打印出其内容的预览。 请注意，预览的顶部显示着 tibble。 在 tidyverse 中，我们使用特殊的 data frames 称为 tibbles，您很快将学到更多关于它的知识。\n\npenguins\n#> # A tibble: 344 × 8\n#>   species island    bill_length_mm bill_depth_mm flipper_length_mm\n#>   <fct>   <fct>              <dbl>         <dbl>             <int>\n#> 1 Adelie  Torgersen           39.1          18.7               181\n#> 2 Adelie  Torgersen           39.5          17.4               186\n#> 3 Adelie  Torgersen           40.3          18                 195\n#> 4 Adelie  Torgersen           NA            NA                  NA\n#> 5 Adelie  Torgersen           36.7          19.3               193\n#> 6 Adelie  Torgersen           39.3          20.6               190\n#> # ℹ 338 more rows\n#> # ℹ 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\n该 data frame 包含 8 列。 如果想要以另一种视图查看所有变量和每个变量的前几个观察值，请使用 glimpse() 函数。 或者，如果您在 RStudio 中运行，请使用 View(penguins) 打开一个交互式数据查看器。\n\nglimpse(penguins)\n#> Rows: 344\n#> Columns: 8\n#> $ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, A…\n#> $ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torge…\n#> $ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.…\n#> $ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.…\n#> $ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, …\n#> $ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 347…\n#> $ sex               <fct> male, female, female, NA, female, male, female, m…\n#> $ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2…\n\npenguins 中的变量包括：\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\nbody_mass_g: body mass of a penguin, in grams.\n\n要了解更多关于 penguins 的信息，请运行 ?penguins 打开其帮助页面。\n\n2.2.2 Ultimate goal\n在本章中，我们的最终目标（ultimate goal）是重新创建以下可视化图表，显示企鹅的翼展长度（flipper lengths）和体重（body masses）之间的关系，并考虑企鹅的物种（species）差异。\n\n\n\n\n\n\n2.2.3 Creating a ggplot\n让我们逐步重新创建这个图表。\n在 ggplot2 中，你可以使用函数 ggplot() 开始一个绘图过程，定义一个绘图对象，然后向其添加图层（layers）。 ggplot() 的第一个参数是要在图表中使用的数据集（dataset），因此 ggplot(data = penguins) 创建了一个空图表（empty graph），准备展示 penguins 数据集，但由于我们尚未告诉它如何进行可视化，所以目前它是空的。 这并不是一个非常令人兴奋的图表，但你可以将其看作是一个空白的画布，你将在其上绘制剩下的图层。\n\nggplot(data = penguins)\n\n\n\n\n接下来，我们需要告诉 ggplot() 如何将数据的信息进行可视化表示。 ggplot() 函数的 mapping 参数定义了数据集中的变量如何映射到图表的视觉属性（aesthetics）。 mapping 参数总是在 aes() 函数中定义，aes() 函数的 x 和 y 参数指定要映射到 x 轴和 y 轴的变量。 目前，我们仅将翼展长度（flipper length）映射到 x aesthetic，将体重（body mass）映射到 y aesthetic。 ggplot2 会在 data 参数中寻找映射的变量，此处为 penguins 数据集。\n下面的图展示了添加这些映射后的结果。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n我们的空白画布现在具有了更多的结构 – 可以清楚地看到翼展长度（flipper lengths）将显示在 x-axis 上，体重（body masses）将显示在 y-axis 上。 但是企鹅的观测值还没有显示在图表上。 这是因为我们在代码中尚未明确指定如何在图表上表示数据框中的观测值。\n为了实现这一点，我们需要定义一个几何对象（geom）：用于表示数据的图表中的几何对象。 在 ggplot2 中，可以使用以 geom_ 开头的函数来获得这些几何对象。 人们通常通过图表使用的几何对象类型来描述图表。 例如，柱状图使用柱状几何对象（geom_bar()），折线图使用线条几何对象（geom_line()），箱线图使用箱线几何对象（geom_boxplot()），散点图使用点几何对象（geom_point()），等等。\n函数 geom_point() 将一层点添加到您的图表中，从而创建一个散点图。 ggplot2 提供了许多不同类型的几何函数，每个函数都可以向图表添加不同类型的图层（layer）。 在本书中，您将学习到许多不同的几何函数，特别是在 Chapter 10 中。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n#> Warning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n现在我们有了一个看起来像是”散点图”的图表。 它还不完全符合我们的”最终目标”图表，但使用这个图表，我们可以开始回答我们探索的问题：“翼展长度（flipper length）和体重（body mass）之间的关系是什么样的？” 这个关系似乎是正向的（随着翼展长度（flipper length）的增加，体重（body mass）也增加），相当线性（点围绕在一条线附近而不是曲线上），并且中等强度（这条线附近没有太多的散点）。 翼展较长的企鹅通常在体重上也较大。\n在我们为这个图表添加更多图层之前，让我们暂停一下并回顾一下我们收到的警告信息（warning message）：\n\nRemoved 2 rows containing missing values (geom_point()).\n\n我们看到这个警告信息是因为我们的数据集中有两个企鹅的体重和/或翼展长度值缺失，而 ggplot2 没有办法在图表上表示它们，因为需要同时具备这两个值。 与 R 一样，ggplot2 遵循这样的理念：缺失值永远不应该悄悄地丢失。 这种警告通常是您在处理实际数据时最常见的警告之一 – 缺失值是一个非常常见的问题，在本书中您将在 Chapter 19 中学到更多相关知识。 在本章的其余图表中，我们将禁止显示这个警告信息，以免在每个图表旁边都打印出来。\n\n2.2.4 Adding aesthetics and layers\n散点图对于显示两个数值变量之间的关系非常有用，但是对于两个变量之间的任何明显关系，保持怀疑态度并询问是否存在其他变量来解释或改变这种明显关系的性质总是一个好主意。 例如，翼展长度（flipper length）和体重（body mass）之间的关系是否因物种（species）而异？ 让我们将物种（species）信息加入到我们的图表中，看看这是否揭示了这些变量之间明显关系的其他洞察。 我们将使用不同颜色的点来表示不同的物种（species）。\n为了实现这一点，我们需要修改 aesthetic 或 geom 部分吗？ 如果你猜到了”in the aesthetic mapping, inside of aes()“，那么你已经开始掌握使用 ggplot2 创建数据可视化的方法了！ 如果没有，不用担心。 在本书中，你将制作更多的 ggplots，并有更多的机会在制作图表时检验你的直觉。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n当将一个分类变量映射到一个 aesthetic 时，ggplot2 会自动为每个唯一的变量水平（每个物种）分配一个唯一的 aesthetic 值（这里是唯一的 color），这个过程被称为缩放（scaling）。 ggplot2 还会添加一个图例（legend），解释哪些值对应于哪些水平。\n现在让我们添加一个额外的图层：一个显示体重（body mass）和翼展长度（flipper length）之间关系的平滑曲线。 在继续之前，请回顾上面的代码，并思考如何将其添加到我们现有的图表中。\n由于这是一个表示数据的新几何对象，我们将在我们的 point geom 之上添加一个新的几何层：geom_smooth()。 我们将通过 method = \"lm\" 指定使用线性模型（linear model）来绘制最佳拟合线。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n我们成功地添加了线条，但是这个图形看起来与 Section 2.2.2 提供的图形不同，Section 2.2.2 的图形只有一条线表示整个数据集，而不是每个企鹅物种都有独立的线条。\n当在 ggplot() 中定义 aesthetic mappings 时，在全局级别（global level）上，它们会传递给绘图的每个后续几何层（geom layers）。 然而，ggplot2 中的每个几何函数也可以接受一个 mapping 参数，该参数允许在局部级别（local level）上进行 aesthetic mappings，并将其添加到从全局级别继承的映射中。 由于我们希望点的颜色根据物种进行着色，但不希望将线条分开显示，所以我们应该仅对 geom_point() 指定 color = species。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n太棒了！ 我们已经接近我们的最终目标，尽管还不完美。 我们仍然需要为每个企鹅物种使用不同的形状，并改进标签。\n在绘图中仅使用颜色来表示信息通常不是一个好主意，因为由于色盲或其他色觉差异，人们对颜色的感知有所不同。 因此，除了颜色之外，我们还可以将 species 映射到 shape aesthetic 上。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n请注意，图例（legend）会自动更新以反映点的不同形状。\n最后，我们可以使用 labs() 函数在新的图层中改进我们绘图的标签。 labs() 的一些参数可能是不言自明的：title 添加标题，subtitle 添加副标题到绘图中。 其他参数与美学映射相匹配，x 是 x 轴标签，y 是 y 轴标签，color 和 shape 定义图例的标签。 此外，我们可以使用 ggthemes 包中的 scale_color_colorblind() 函数改进颜色调色板，使其适合色盲人士使用。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n我们最终有了一个完全符合我们”最终目标”的图！\n\n2.2.5 Exercises\n\npenguins 有多少行（rows）？ 有多少列（columns）？\npenguins 数据框中的 bill_depth_mm 变量描述了什么？ 请阅读 ?penguins 的帮助文档来获取答案。\n创建一个 bill_depth_mm vs. bill_length_mm 的散点图。 也就是说，在 y-axis 上绘制 bill_depth_mm，在 x-axis 上绘制 bill_length_mm。 描述这两个变量之间的关系。\n如果你绘制 species vs. bill_depth_mm 的散点图，会发生什么？ 选择什么样的几何图形可能更好？\n\n为什么下面的代码会出错，如何修复它？\n\nggplot(data = penguins) + \n  geom_point()\n\n\n在 geom_point() 中，na.rm 参数的作用是什么？ 这个参数的默认值是什么？ 创建一个散点图，并成功地将该参数设置为 TRUE。\n在前面绘制的图中添加以下说明：“Data come from the palmerpenguins package.” 提示：查看 labs() 的文档。\n\n重新创建以下可视化图形。 bill_depth_mm 应该映射到哪个美学属性？ 这个映射是应该在全局级别（global level）还是几何级别（geom level）上完成？\n\n\n\n\n\n\n\n在脑海中运行此代码并预测输出结果。 然后，在 R 中运行代码并检查您的预测。\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = island)\n) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n这两张图看起来会不一样吗？为 什么/为什么不？\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point() +\n  geom_smooth()\n\nggplot() +\n  geom_point(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  ) +\n  geom_smooth(\n    data = penguins,\n    mapping = aes(x = flipper_length_mm, y = body_mass_g)\n  )"
  },
  {
    "objectID": "data-visualize.html#sec-ggplot2-calls",
    "href": "data-visualize.html#sec-ggplot2-calls",
    "title": "2  Data visualization",
    "section": "\n2.3 ggplot2 calls",
    "text": "2.3 ggplot2 calls\n随着我们从这些介绍性部分继续前进，我们将过渡到 ggplot2 代码的更简洁的表达。 到目前为止，我们一直非常明确，这在学习过程中是很有帮助的：\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n通常，函数的前一个或两个参数非常重要，你应该熟记于心。 ggplot() 函数的前两个参数是 data 和 mapping，在本书的其余部分，我们不再提供这些参数的名称。 这样做可以节省输入的工作量，并通过减少额外文本的数量，更容易看出绘图之间的区别。 这是一个非常重要的编程问题，在 Chapter 26 中我们会再次涉及到这个问题。\n对先前的绘图进行更简洁的重写可以得到：\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n在将来，您还将学习使用管道操作符 |> 来创建该绘图，如下所示：\n\npenguins |> \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "data-visualize.html#visualizing-distributions",
    "href": "data-visualize.html#visualizing-distributions",
    "title": "2  Data visualization",
    "section": "\n2.4 Visualizing distributions",
    "text": "2.4 Visualizing distributions\n可视化变量的分布方式取决于变量的类型：分类变量或数值变量。\n\n2.4.1 A categorical variable\n如果一个变量只能取一小组值中的一个，则该变量是分类变量（categorical）。 要检查分类变量的分布情况，可以使用条形图。 条形图的高度显示了每个 x 值出现的观测次数。\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n在具有无序级别的分类变量的条形图中，例如上面的企鹅物种（species），通常最好根据它们的频率重新排序条形。 为此，需要将变量转换为因子（factor）（R 如何处理分类数据），然后重新排序该因子的级别（levels）。\n\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\n\n\n您将在 Chapter 17 中进一步了解有关因子（factors）和处理因子的函数（如上面所示的 fct_infreq()）的知识。\n\n2.4.2 A numerical variable\n如果一个变量可以取一系列广泛的数值，并且可以对这些数值进行加减或求平均，那么该变量就是数值变量（numerical）。 数值变量可以是连续的（continuous）或离散的（discrete）。\n直方图（histogram）是一种常用的连续变量分布可视化方法。\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n直方图将 x-axis 均匀地分成多个区间（bins），并使用条的高度显示落入每个区间的观测次数。 在上面的图中，最高的条表示有 39 个观测值的 body_mass_g 值介于 3,500 到 3,700 克之间，这个区间是条的左右边缘。\n您可以使用 binwidth 参数设置直方图中的区间宽度，该参数以 x 变量的单位进行测量。 在使用直方图时，应该尝试不同的区间宽度，因为不同的区间宽度可以展现不同的模式。 在下面的图中，区间宽度为 20 太窄了，导致有太多的条，使得难以确定分布的形状。 类似地，区间宽度为 2,000 太大了，导致所有的数据都被分到了只有三个条中，也难以确定分布的形状。 区间宽度为 200 提供了一个合理的平衡点。\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\n\n\n\n\n\n\n一种用于可视化数值变量分布的替代方法是密度图（density plot）。 密度图是直方图的平滑版本，特别适用于连续数据，这些数据来自于一个平滑分布。 我们不会详细讨论 geom_density() 如何估计密度（您可以在函数文档中阅读更多相关信息），但我们可以通过类比来解释绘制密度曲线的过程。 想象一个由木块组成的直方图。 然后，想象一根煮熟的意大利面条放在上面。 面条垂挂在木块上的形状可以被视为密度曲线的形状。 它展示的细节比直方图少，但可以更容易地快速了解分布的形状，特别是关于峰值和偏斜度方面的特征。\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n#> Warning: Removed 2 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n2.4.3 Exercises\n\n创建一个关于 penguins 数据集中的 species 变量的条形图（bar plot），将 species 分配给 y aesthetic。 这个图与之前的图有何不同？\n\n下面两个图形有何不同？ color 和 fill 这两个美学映射中，哪一个更适合改变条形图的颜色？\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(color = \"red\")\n\nggplot(penguins, aes(x = species)) +\n  geom_bar(fill = \"red\")\n\n\ngeom_histogram() 中的 bins 参数有什么作用？\n在加载 tidyverse 包时，可以使用 diamonds 数据集中的 carat 变量创建一个直方图（histogram）。 尝试使用不同的区间宽度（binwidths）来观察结果。 哪个区间宽度（binwidths）可以显示出最有趣的模式？"
  },
  {
    "objectID": "data-visualize.html#visualizing-relationships",
    "href": "data-visualize.html#visualizing-relationships",
    "title": "2  Data visualization",
    "section": "\n2.5 Visualizing relationships",
    "text": "2.5 Visualizing relationships\n要可视化一个关系（relationship），我们至少需要将两个变量映射到绘图的 aesthetics 中。 在接下来的章节中，您将学习常用的用于可视化两个或多个变量之间关系的图表以及用于创建这些图表的几何对象（geoms）。\n\n2.5.1 A numerical and a categorical variable\n要可视化数值变量和分类变量之间的关系，我们可以使用并列箱线图。 箱线图（boxplot）是一种用于描述分布位置（百分位数）的视觉工具。 它还可以用于识别潜在的异常值（outliers）。 如 Figure 2.1 所示，每个箱线图由以下几部分组成：\n\n一个 box，用于表示数据的中间一半的范围，也就是四分位距（IQR），从分布的第 25 个百分位数延伸到第 75 个百分位数。 box 的中间有一条线，显示分布的中位数，即第 50 个百分位数。 这三条线可以让您了解分布的扩展程度以及分布是否关于中位数对称或倾斜于一侧。\n用于显示位于 box 边缘 1.5 倍 IQR 之外的观测值的可视化点。 这些异常点是不寻常的，因此会单独绘制出来。\n从 box 的每一端延伸出一条线（或者称为 whisker），并延伸到分布中最远的非异常点。\n\n\n\n\n\nFigure 2.1: Diagram depicting how a boxplot is created.\n\n\n\n\n让我们使用 geom_boxplot() 来查看不同物种（species）的体重（body mass）分布：\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\n\n\n或者，我们可以使用 geom_density() 创建密度图（density plots）。\n\nggplot(penguins, aes(x = body_mass_g, color = species)) +\n  geom_density(linewidth = 0.75)\n\n\n\n\n我们还使用 linewidth 参数自定义了线条的粗细，以使其在背景中更加突出。\n此外，我们可以将 species 映射到 color 和 fill aesthetics，并使用 alpha aesthetic 为填充的密度曲线添加透明度。 这个 aesthetic 的取值范围在 0（完全透明）和 1（完全不透明）之间。 在下面的图中，它被设置为 0.5。\n\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\n\n请注意我们在这里使用的术语：\n\n如果我们希望由变量的值决定该美学属性的视觉特征的变化，我们会将变量映射（map）到美学属性。\n否则，我们会设置（set）美学属性的值。\n\n2.5.2 Two categorical variables\n我们可以使用堆叠条形图（stacked bar plots）来可视化两个分类变量之间的关系。 例如，以下两个堆叠条形图都显示了 island 和 species 之间的关系，具体而言，可视化了每个岛上 species 的分布。\n第一个图显示了每个岛上各种企鹅物种的频率。 频率图显示在每个岛上，Adelie 的数量是相等的。 但我们无法很好地了解每个岛内的百分比分布情况。\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar()\n\n\n\n\n第二个图是一个相对频率图，通过在 geom 中设置 position = \"fill\" 创建，它对比较跨岛屿的物种分布更有用，因为它不受岛屿上企鹅数量不均衡的影响。 使用这个图表，我们可以看到 Gentoo 企鹅全部生活在 Biscoe 岛上，并占该岛企鹅总数的大约 75％，Chinstrap 企鹅全部生活在 Dream 岛上，并占该岛企鹅总数的大约 50％，而 Adelie 企鹅分布在所有三个岛屿上，并占据 Torgersen 岛上所有的企鹅。\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n在创建这些条形图时，我们将要分隔成条的变量映射到 x aesthetic，将用于改变条内颜色的变量映射到 fill aesthetic。\n\n2.5.3 Two numerical variables\n到目前为止，你已经学习了使用 geom_point() 创建散点图和使用 geom_smooth() 创建平滑曲线来可视化两个数值变量之间的关系。 散点图可能是最常用的用于可视化两个数值变量之间关系的图表之一。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n2.5.4 Three or more variables\n正如我们在 Section 2.2.4 中所看到的，我们可以通过将它们 mapping 到其他 aesthetics 来将更多变量融入到图表中。 例如，在下面的散点图中，点的颜色代表物种（species），点的形状代表岛屿（islands）。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = island))\n\n\n\n\n然而，如果将过多的 aesthetic mappings 添加到图表中，会使其变得杂乱且难以理解。 另一种方法，特别适用于分类变量，是将图表分割为面板（facets），每个面板显示数据的一个子集。\n要通过单个变量进行面板化，可以使用 facet_wrap() 函数。 facet_wrap() 的第一个参数是一个公式3，可以使用 ~ 后跟一个变量名来创建。 传递给 facet_wrap() 的变量应该是分类变量。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\n\n\n您将在 Chapter 10 中学习到许多其他用于可视化变量分布和它们之间关系的几何对象（geoms）。\n\n2.5.5 Exercises\n\n根据 ggplot2 软件包中捆绑的 mpg 数据框，该数据框包含了美国环境保护署收集的 234 个观测数据，涵盖了 38 个汽车型号。 在 mpg 数据框中，哪些变量是分类变量？ 哪些变量是数值变量？ （提示：输入 ?mpg 来阅读数据集的文档。）当您运行 ?mpg 时，如何查看这些信息？\n使用 mpg 数据框创建一个 hwy vs. displ 的散点图。 然后，将第三个数值变量映射到 color、size、color 和 size 以及 shape。 这些 aesthetics 在分类变量和数值变量上的行为有何不同？\n在 hwy vs. displ 的散点图中，如果将第三个变量映射到 linewidth 会发生什么？\n如果将同一个变量映射到多个 aesthetics 会发生什么？\n创建一个 bill_depth_mm vs. bill_length_mm 的散点图，并按照 species 对点进行着色。 将点按物种（species）着色有关于这两个变量之间关系的什么信息？ 通过对物种（species）进行分面绘图（faceting）又会发现什么？\n\n为什么下面的代码会产生两个独立的图例（legends）？ 如何修复它以合并这两个图例？\n\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = bill_length_mm, y = bill_depth_mm, \n    color = species, shape = species\n  )\n) +\n  geom_point() +\n  labs(color = \"Species\")\n\n\n\n创建下面的两个堆叠条形图。 第一个图可以回答哪个问题？ 第二个图可以回答哪个问题？\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "data-visualize.html#sec-ggsave",
    "href": "data-visualize.html#sec-ggsave",
    "title": "2  Data visualization",
    "section": "\n2.6 Saving your plots",
    "text": "2.6 Saving your plots\n一旦你创建了一个图形，你可能希望将其保存为图像文件以在其他地方使用。 这时可以使用 ggsave() 函数来将最近创建的图形保存到磁盘中。\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\nggsave(filename = \"penguin-plot.png\")\n\n这将把你的图形保存到你的工作目录中，你会在 Chapter 7 中更详细地了解到这个概念。\n如果你没有指定 width 和 height 参数，它们将从当前绘图设备的尺寸中获取。 为了获得可重现的代码，建议你指定这些参数。 你可以在文档中了解更多关于 ggsave() 函数的信息。\n然而，通常我们建议使用 Quarto 来组织最终的报告。Q uarto 是一个可重复使用的创作系统，允许你将代码和文字交织在一起，并自动将图形包含在你的写作中。 你将在 Chapter 29 中了解更多关于 Quarto 的内容。\n\n2.6.1 Exercises\n\n\n运行以下代码行。 两个图中的哪一个保存为 mpg-plot.png？ 为什么？\n\nggplot(mpg, aes(x = class)) +\n  geom_bar()\nggplot(mpg, aes(x = cty, y = hwy)) +\n  geom_point()\nggsave(\"mpg-plot.png\")\n\n\n您需要在上面的代码中更改什么才能将绘图保存为 PDF 而不是 PNG？ 你如何找出哪些类型的图像文件可以在 ggsave() 中工作？"
  },
  {
    "objectID": "data-visualize.html#common-problems",
    "href": "data-visualize.html#common-problems",
    "title": "2  Data visualization",
    "section": "\n2.7 Common problems",
    "text": "2.7 Common problems\n当您开始运行 R 代码时，可能会遇到一些问题。不 要担心，这种情况对每个人都会发生。我 们多年来一直在编写 R 代码，但每天我们仍然会写出一开始就无法正常工作的代码！\n首先，仔细比较您正在运行的代码与书中的代码。 R 非常挑剔，一个错位的字符可能会产生完全不同的结果。 确保每个 ( 都与一个 ) 匹配，每个 \" 都与另一个 \" 配对。 有时候您运行代码后什么都没有发生。 检查您控制台的左侧：如果是 +，这意味着 R 认为您输入的不是完整的表达式，它正在等待您完成。 在这种情况下，通常通过按 ESCAPE 键中止处理当前命令，然后重新开始会比较容易。\n在创建 ggplot2 图形时，一个常见的问题是将 + 放在错误的位置：它必须放在行尾，而不是行首。换 句话说，请确保您没有意外地编写了以下这种代码：\n\nggplot(data = mpg) \n+ geom_point(mapping = aes(x = displ, y = hwy))\n\n如果您仍然遇到困难，请尝试获取帮助。 您可以在控制台中运行 ?function_name 来获取有关任何 R 函数的帮助，或者在 RStudio 中选中函数名并按下 F1 键。 如果帮助信息似乎不太有用，不要担心，可以跳到示例部分，寻找与您尝试做的事情相匹配的代码示例。\n如果这样还无法解决问题，请仔细阅读错误消息。 有时候答案就埋在错误消息中！ 但是，当您刚开始接触 R 时，即使答案在错误消息中，您可能还不知道如何理解它。 另一个很好的工具是 Google：尝试搜索错误消息，因为很可能有其他人遇到了同样的问题，并在网上寻求了帮助。"
  },
  {
    "objectID": "data-visualize.html#summary",
    "href": "data-visualize.html#summary",
    "title": "2  Data visualization",
    "section": "\n2.8 Summary",
    "text": "2.8 Summary\n在本章中，您学习了使用 ggplot2 进行数据可视化的基础知识。 我们从支持 ggplot2 的基本思想开始：可视化是从数据中的变量到 aesthetic（如 position、color、size、shape）的 mapping。 然后，您逐层学习了如何增加复杂性并改进绘图的呈现方式。 您还学习了用于可视化单个变量分布以及可视化两个或多个变量之间关系的常用图表，通过使用附加的 aesthetic mappings 和/或使用 faceting 将绘图分割为小多面体进行绘制。\n在本书的后续部分，我们将一次又一次地使用可视化，根据需要引入新的技术，并在 Chapter 10 到 ?sec-communication 中深入探讨如何使用 ggplot2 创建可视化。\n在掌握了可视化的基础知识之后，在下一章中，我们将转换一下思路，为您提供一些实际的工作流程建议。 我们将在本书的这一部分中穿插工作流程建议和数据科学工具，因为这将帮助您在编写越来越多的 R 代码时保持组织性。"
  },
  {
    "objectID": "data-visualize.html#footnotes",
    "href": "data-visualize.html#footnotes",
    "title": "2  Data visualization",
    "section": "",
    "text": "您可以使用 conflicted 包来消除该消息，并在需要时强制进行冲突解决。随 着加载更多的包，这变得更为重要。 您可以在 https://conflicted.r-lib.org 了解更多关于 conflicted 的信息。↩︎\nHere “formula” is the name of the thing created by ~, not a synonym for “equation”.↩︎\nHere “formula” is the name of the thing created by ~, not a synonym for “equation”.↩︎"
  },
  {
    "objectID": "data-transform.html#introduction",
    "href": "data-transform.html#introduction",
    "title": "4  Data transformation",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\n可视化是生成洞察力的重要工具，但很少有数据以完全符合你所需的形式提供，以制作你想要的图形。 通常情况下，你需要创建一些新的变量或摘要统计信息来回答你的问题，或者你可能只是想重命名变量或重新排序观测结果，以便更轻松地处理数据。 在本章中，你将学习如何使用 dplyr 包进行数据转换（data transformation），并使用关于 2013 年从纽约市出发的航班的新数据集来介绍这些内容，并且你还将学习更多相关的知识！\n本章的目标是为你提供转换数据框的所有关键工具的概述。 我们将从对数据框的行和列进行操作的函数开始，然后再回到管道操作符的讨论，这是一个重要的工具，用于组合操作。 然后，我们将介绍如何使用分组进行操作。 最后，我们将通过一个案例研究展示这些函数的实际应用，并在后续章节中更详细地讨论这些函数，以便深入研究特定类型的数据 (e.g., numbers, strings, dates)。\n\n4.1.1 Prerequisites\n在本章中，我们将关注 dplyr 包，它是 tidyverse 的另一个核心成员。 我们将使用 nycflights13 包中的数据说明关键思想，并使用 ggplot2 帮助我们理解数据。\n\nlibrary(nycflights13)\n#> Warning: package 'nycflights13' was built under R version 4.2.3\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\n#> ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n请仔细注意加载 tidyverse 时打印的冲突消息（conflicts message）。 它告诉你 dplyr 覆盖了 base R 中的一些函数。 如果你想在加载 dplyr 后使用这些函数的 base 版本，你需要使用它们的全名：stats::filter() 和 stats::lag()。 到目前为止，我们大多忽略了一个函数来自哪个包，因为大多数时候它并不重要。 但是，了解包可以帮助您找到帮助并找到相关函数，因此当我们需要准确了解某个函数来自哪个包时，我们将使用与 R 相同的语法：packagename::functionname()。\n\n4.1.2 nycflights13\n为了探索基本的 dplyr verbs，我们将使用 nycflights13::flights。 该数据集包含 2013 年从纽约市起飞的所有 336,776 航班。 数据来自美国 Bureau of Transportation Statistics，记录在 ?flights 中。\n\nflights\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\nflights 是一个 tibble，一种特殊类型的 data frame，tidyverse 使用它来避免一些常见的问题。 tibbles 和 data frame 之间最重要的区别是 tibbles 的打印方式； 它们是为大型数据集设计的，因此它们只显示前几行，并且只显示适合一个屏幕的列。 有几个选项可以查看所有内容。 如果您使用的是 RStudio，最方便的可能是 View(flights)，它将打开一个交互式的可滚动和可过滤的视图。 另外，您可以使用 print(flights, width = Inf) 来显示所有列，或使用 glimpse()：\n\nglimpse(flights)\n#> Rows: 336,776\n#> Columns: 19\n#> $ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n#> $ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 55…\n#> $ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 60…\n#> $ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2,…\n#> $ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 8…\n#> $ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 8…\n#> $ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7,…\n#> $ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\"…\n#> $ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301…\n#> $ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N…\n#> $ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LG…\n#> $ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IA…\n#> $ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149…\n#> $ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 73…\n#> $ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6…\n#> $ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59…\n#> $ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-0…\n\n在这两个视图中，变量名称后跟缩写，告诉您每个变量的类型：<int> 是整数（integer）的缩写，<dbl> 是双精度（double）（又名实数）的缩写，<chr> 用于字符（character）（也称为字符串），<dttm> 用于日期时间（date-time）。 这些很重要，因为您可以对列执行的操作在很大程度上取决于其”类型”。\n\n4.1.3 dplyr basics\n您将学习主要的 dplyr verbs（functions），这将使您能够解决绝大多数数据操作挑战。 但在我们讨论它们的个体差异之前，有必要先说明一下它们的共同点：\n\n第一个参数始终是一个 data frame。\n随后的参数通常使用变量名称（不带引号）描述要对哪些列进行操作。\n输出总是一个新的 data frame。\n\n因为每个 verb 都擅长做一件事，解决复杂问题通常需要组合多个 verbs，我们将使用竖线 |> 来实现。 我们将在 Section 4.4 中更多地讨论管道，但简而言之，管道将其左侧的东西传递给右侧的函数，因此 x |> f(y) 等价于 f(x, y)，而 x |> f(y) |> g(z) 等价于 g(f(x, y), z)。 管道的最简单发音是 “then”。 即使您尚未了解详细信息，也可以了解以下代码：\n\nflights |>\n  filter(dest == \"IAH\") |> \n  group_by(year, month, day) |> \n  summarize(\n    arr_delay = mean(arr_delay, na.rm = TRUE)\n  )\n\ndplyr 的 verbs 根据操作对象分为四组：rows, columns, groups, or tables。 在接下来的部分中，您将学习 rows、columns 和 groups 最重要的 verbs，然后我们将回到 Chapter 20 中用于 tables 的连接动词。 让我们开始吧！"
  },
  {
    "objectID": "data-transform.html#rows",
    "href": "data-transform.html#rows",
    "title": "4  Data transformation",
    "section": "\n4.2 Rows",
    "text": "4.2 Rows\n对数据集的行进行操作的最重要的 verbs 是 filter()，它改变了哪些行存在而不改变它们的顺序，以及 arrange()，它改变了行的顺序而不改变存在的行。 这两个函数只影响行，列保持不变。 我们还将讨论 distinct()，它可以找到具有唯一值的行，但与 arrange() 和 filter() 不同的是，它还可以选择性地修改列。\n\n4.2.1 filter()\n\nfilter() 允许您根据列的值保留行1。 第一个参数是 data frame。 第二个和后续参数是必须为真才能保留该行的条件。 例如，我们可以找到所有晚点超过 120 分钟（两小时）的航班：\n\nflights |> \n  filter(dep_delay > 120)\n#> # A tibble: 9,723 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      848           1835       853     1001           1950\n#> 2  2013     1     1      957            733       144     1056            853\n#> 3  2013     1     1     1114            900       134     1447           1222\n#> 4  2013     1     1     1540           1338       122     2020           1825\n#> 5  2013     1     1     1815           1325       290     2120           1542\n#> 6  2013     1     1     1842           1422       260     1958           1535\n#> # ℹ 9,717 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n除了 >（大于），您还可以使用 >=（大于或等于）、<（小于）、<=（小于或等于）、== （等于）和 !=（不等于）。 您还可以将条件与 & 或 , 结合起来以指示 “and”（检查两个条件）或与 | 结合以指示 “or”（检查任一条件）：\n\n# Flights that departed on January 1\nflights |> \n  filter(month == 1 & day == 1)\n#> # A tibble: 842 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 836 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n# Flights that departed in January or February\nflights |> \n  filter(month == 1 | month == 2)\n#> # A tibble: 51,955 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 51,949 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n组合 | 和 == 时有一个有用的快捷方式：%in%。 它保留变量等于右侧值之一的行：\n\n# A shorter way to select flights that departed in January or February\nflights |> \n  filter(month %in% c(1, 2))\n#> # A tibble: 51,955 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 51,949 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n我们将在 Chapter 13 中更详细地回到这些比较和逻辑运算符。\n当你运行 filter() 时，dplyr 执行过滤操作，创建一个新的数据框，然后打印它。 它不会修改现有的 flights 数据集，因为 dplyr 函数从不修改其输入。 要保存结果，您需要使用赋值运算符，<-：\n\njan1 <- flights |> \n  filter(month == 1 & day == 1)\n\n\n4.2.2 Common mistakes\n当您开始使用 R 时，最容易犯的错误是在测试相等性时使用 = 而不是 ==。 filter() 会在发生这种情况时通知您：\n\nflights |> \n  filter(month = 1)\n#> Error in `filter()`:\n#> ! We detected a named input.\n#> ℹ This usually means that you've used `=` instead of `==`.\n#> ℹ Did you mean `month == 1`?\n\n另一个错误是你像用英语那样写 “or” 语句：\n\nflights |> \n  filter(month == 1 | 2)\n\n这个能够工作，因为它不会抛出错误，但它不会得到你想要的结果，因为 | 首先检查条件 month == 1 然后检查条件 2，这不是一个明智的检查条件。 我们将在 Section 16.6.2 中详细了解这里发生的事情以及原因。\n\n4.2.3 arrange()\n\narrange() 根据列（columns）的值更改行（rows）的顺序。 它接受一个 data frame 和一组要按顺序排列的列名（column names）（或更复杂的表达式）。 如果提供多个列名（column names），每个额外的列将用于解决前面列的值相等的情况。 例如，下面的代码按照出发时间排序，该时间跨越了四列。 我们首先获取最早的年份（years），然后在一年内获取最早的月份（months），以此类推。\n\nflights |> \n  arrange(year, month, day, dep_time)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n在 arrange() 内部，您可以对列使用 desc() 函数，以便按降序（从大到小）重新排序数据框。 例如，以下代码按延误时间从最长到最短的顺序排序航班：\n\nflights |> \n  arrange(desc(dep_delay))\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     9      641            900      1301     1242           1530\n#> 2  2013     6    15     1432           1935      1137     1607           2120\n#> 3  2013     1    10     1121           1635      1126     1239           1810\n#> 4  2013     9    20     1139           1845      1014     1457           2210\n#> 5  2013     7    22      845           1600      1005     1044           1815\n#> 6  2013     4    10     1100           1900       960     1342           2211\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n请注意，行的数量没有改变 – 我们只是重新排列了数据，而没有对其进行筛选。\n\n4.2.4 distinct()\n\ndistinct() 函数用于在数据集中查找所有唯一的行，因此从技术上讲，它主要操作行。 然而，大多数情况下，您希望获取某些变量的唯一组合，因此您也可以选择性地提供列名：\n\n# Remove duplicate rows, if any\nflights |> \n  distinct()\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n# Find all unique origin and destination pairs\nflights |> \n  distinct(origin, dest)\n#> # A tibble: 224 × 2\n#>   origin dest \n#>   <chr>  <chr>\n#> 1 EWR    IAH  \n#> 2 LGA    IAH  \n#> 3 JFK    MIA  \n#> 4 JFK    BQN  \n#> 5 LGA    ATL  \n#> 6 EWR    ORD  \n#> # ℹ 218 more rows\n\n另外，如果您在筛选唯一行时想要保留其他列，可以使用 .keep_all = TRUE 选项。\n\nflights |> \n  distinct(origin, dest, .keep_all = TRUE)\n#> # A tibble: 224 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 218 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n并非巧合，所有这些唯一的航班都是在 1 月 1 日：distinct() 函数将找到数据集中唯一行的第一个出现，并丢弃其余的重复行。\n如果您想找到每个唯一行的出现次数，最好将 distinct() 替换为 count() 函数，并使用 sort = TRUE 参数按出现次数的降序排列它们。 您可以在 Section 14.3 中了解更多关于 count() 函数的内容。\n\nflights |>\n  count(origin, dest, sort = TRUE)\n#> # A tibble: 224 × 3\n#>   origin dest      n\n#>   <chr>  <chr> <int>\n#> 1 JFK    LAX   11262\n#> 2 LGA    ATL   10263\n#> 3 LGA    ORD    8857\n#> 4 JFK    SFO    8204\n#> 5 LGA    CLT    6168\n#> 6 EWR    ORD    6100\n#> # ℹ 218 more rows\n\n\n4.2.5 Exercises\n\n\n在每个条件的单个管道中，找到满足条件的所有航班（flights）：\n\nHad an arrival delay of two or more hours\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta\nDeparted in summer (July, August, and September)\nArrived more than two hours late, but didn’t leave late\nWere delayed by at least an hour, but made up over 30 minutes in flight\n\n\n对 flights 进行排序以查找起飞延误（departure delays）时间最长的航班。 查找早上最早起飞的航班。\n对 flights 进行排序以找到最快的航班。 （提示：尝试在你的函数中包含一个数学计算。）\n2013 年每天都有航班吗？\n哪些航班飞行距离最远？ 哪个航班飞行距离最短？\n如果同时使用 filter() 和 arrange()，那么使用它们的顺序有什么关系吗？ 为什么/为什么不？ 考虑结果以及功能必须完成的工作量。"
  },
  {
    "objectID": "data-transform.html#columns",
    "href": "data-transform.html#columns",
    "title": "4  Data transformation",
    "section": "\n4.3 Columns",
    "text": "4.3 Columns\n有四个重要的 verbs 可以在不更改行的情况下影响列：mutate() 创建从现有列派生的新列，select() 更改存在的列，rename() 更改列的名称， relocate() 更改列的位置。\n\n4.3.1 mutate()\n\nmutate() 的工作是添加根据现有列计算的新列。 在转换（transform）章节中，您将学习大量可用于操作不同类型变量的函数。 现在，我们将坚持使用基本代数，它允许我们计算 gain、延迟航班在空中补足的时间以及以英里/小时为单位的 speed：\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60\n  )\n#> # A tibble: 336,776 × 21\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 13 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n默认情况下，mutate() 会在数据集的右侧添加新列，这使得很难看出这里发生了什么。 我们可以使用 .before 参数将变量添加到左侧 2：\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .before = 1\n  )\n#> # A tibble: 336,776 × 21\n#>    gain speed  year month   day dep_time sched_dep_time dep_delay arr_time\n#>   <dbl> <dbl> <int> <int> <int>    <int>          <int>     <dbl>    <int>\n#> 1    -9  370.  2013     1     1      517            515         2      830\n#> 2   -16  374.  2013     1     1      533            529         4      850\n#> 3   -31  408.  2013     1     1      542            540         2      923\n#> 4    17  517.  2013     1     1      544            545        -1     1004\n#> 5    19  394.  2013     1     1      554            600        -6      812\n#> 6   -16  288.  2013     1     1      554            558        -4      740\n#> # ℹ 336,770 more rows\n#> # ℹ 12 more variables: sched_arr_time <int>, arr_delay <dbl>, …\n\n. 表示 .before 是函数的参数，而不是我们正在创建的第三个新变量的名称。 您还可以使用 .after 来在变量之后添加，并且在 .before 和 .after 中都可以使用变量名而不是位置。 例如，我们可以在 day 之后添加新变量：\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    speed = distance / air_time * 60,\n    .after = day\n  )\n\n或者，您可以使用 .keep 参数控制保留哪些变量。 一个特别有用的参数是 \"used\"，它指定我们只保留在 mutate() 步骤中涉及或创建的列。 例如，以下输出将仅包含变量 dep_delay, arr_delay, air_time, gain, hours, and gain_per_hour。\n\nflights |> \n  mutate(\n    gain = dep_delay - arr_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours,\n    .keep = \"used\"\n  )\n\n请注意，由于我们尚未将上述计算的结果分配回 flights，因此新变量 gain,、hours、gain_per_hour 只会被打印出来，而不会存储在数据框中。 如果我们希望它们在数据框中可用以供将来使用，我们应该仔细考虑是否要将结果分配回 flights，用更多变量覆盖原始数据框，或者分配给一个新对象 . 通常，正确的答案是一个新对象，该对象以信息性命名以指示其内容，例如 delay_gain，但您也可能有充分的理由覆盖 flights。\n\n4.3.2 select()\n\n获取包含数百甚至数千个变量的数据集并不少见。 在这种情况下，第一个挑战通常只是关注您感兴趣的变量。 select() 允许您使用基于变量名称的操作快速放大有用的子集：\n\n\n按名称选择列：\n\nflights |> \n  select(year, month, day)\n\n\n\n选择 year 和 day（含）之间的所有列：\n\nflights |> \n  select(year:day)\n\n\n\n选择除了 year 到 day（含）以外的所有列：\n\nflights |> \n  select(!year:day)\n\n您也可以使用 - 代替 !（您很可能会在野外看到它）；我们推荐 ! 因为它读起来像 “not”，并且与 & 和 | 结合得很好。\n\n\n选择所有 characters 列：\n\nflights |> \n  select(where(is.character))\n\n\n\n您可以在 select() 中使用许多辅助函数：\n\n\nstarts_with(\"abc\"): 匹配以 “abc” 开头的名称。\n\nends_with(\"xyz\"): 匹配以 “xyz” 结尾的名称。\n\ncontains(\"ijk\"): 匹配包含 “ijk” 的名称。\n\nnum_range(\"x\", 1:3): 匹配x1、x2 和 x3。\n\n有关详细信息，请参阅 ?select。 一但你了解正则表达式（the topic of Chapter 16），您还可以使用 matches() 来选择与模式匹配的变量。\n您可以在使用 = 在 select() 时重命名变量。 新名称出现在 = 的左侧，旧变量出现在右侧：\n\nflights |> \n  select(tail_num = tailnum)\n#> # A tibble: 336,776 × 1\n#>   tail_num\n#>   <chr>   \n#> 1 N14228  \n#> 2 N24211  \n#> 3 N619AA  \n#> 4 N804JB  \n#> 5 N668DN  \n#> 6 N39463  \n#> # ℹ 336,770 more rows\n\n\n4.3.3 rename()\n\n如果你想保留所有现有变量并且只想重命名一些变量，你可以使用 rename() 而不是 select()：\n\nflights |> \n  rename(tail_num = tailnum)\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n如果你有一堆命名不一致的列，并且手动修复它们会很痛苦，请查看 janitor::clean_names()，它提供了一些有用的自动清理。\n\n4.3.4 relocate()\n\n使用 relocate() 移动变量。 您可能希望将相关变量收集在一起或将重要变量移到前面。 默认情况下，relocate() 将变量移动到前面：\n\nflights |> \n  relocate(time_hour, air_time)\n#> # A tibble: 336,776 × 19\n#>   time_hour           air_time  year month   day dep_time sched_dep_time\n#>   <dttm>                 <dbl> <int> <int> <int>    <int>          <int>\n#> 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n#> 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n#> 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n#> 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n#> 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n#> 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n#> # ℹ 336,770 more rows\n#> # ℹ 12 more variables: dep_delay <dbl>, arr_time <int>, …\n\n您还可以使用 .before 和 .after 参数指定放置它们的位置，就像在 mutate() 中一样：\n\nflights |> \n  relocate(year:dep_time, .after = time_hour)\nflights |> \n  relocate(starts_with(\"arr\"), .before = dep_time)\n\n\n4.3.5 Exercises\n\n\n\n\n比较 dep_time, sched_dep_time, and dep_delay。 您认为这三个数字之间有何关联？\n尽可能多地集思广益，从 flights 中选择 dep_time、dep_delay、arr_time 和 arr_delay。\n如果您在 select() 调用中多次指定同一个变量的名称，会发生什么情况？\n\nany_of() 函数有什么作用？ 为什么将它与该载体结合使用会有所帮助？\n\nvariables <- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\n\n\n运行以下代码的结果是否让您感到惊讶？ 默认情况下，select helpers 如何处理大写和小写？ 你怎么能改变这个默认值？\n\nflights |> select(contains(\"TIME\"))\n\n\n将 air_time 重命名为 air_time_min 以指示测量单位并将其移至数据框的开头。\n\n为什么以下不起作用，错误是什么意思？\n\nflights |> \n  select(tailnum) |> \n  arrange(arr_delay)\n#> Error in `arrange()`:\n#> ℹ In argument: `..1 = arr_delay`.\n#> Caused by error:\n#> ! object 'arr_delay' not found"
  },
  {
    "objectID": "data-transform.html#sec-the-pipe",
    "href": "data-transform.html#sec-the-pipe",
    "title": "4  Data transformation",
    "section": "\n4.4 The pipe",
    "text": "4.4 The pipe\n我们已经在前面向您展示了管道（pipe）的简单示例，但当您开始组合多个 verbs 时，它的真正威力就会显现出来。 例如，假设您想找到飞往休斯顿 IAH 机场的快速航班：您需要结合使用 filter()、mutate()、select() 和 arrange()：\n\nflights |> \n  filter(dest == \"IAH\") |> \n  mutate(speed = distance / air_time * 60) |> \n  select(year:day, dep_time, carrier, flight, speed) |> \n  arrange(desc(speed))\n#> # A tibble: 7,198 × 7\n#>    year month   day dep_time carrier flight speed\n#>   <int> <int> <int>    <int> <chr>    <int> <dbl>\n#> 1  2013     7     9      707 UA         226  522.\n#> 2  2013     8    27     1850 UA        1128  521.\n#> 3  2013     8    28      902 UA        1711  519.\n#> 4  2013     8    28     2122 UA        1022  519.\n#> 5  2013     6    11     1628 UA        1178  515.\n#> 6  2013     8    27     1017 UA         333  515.\n#> # ℹ 7,192 more rows\n\n尽管这个管道有四个步骤，但它很容易浏览，因为 verbs 出现在每一行的开头：从 flights 数据开始，then filter, then mutate, then select, then arrange。\n如果我们没有管道会怎样？ 我们可以将每个函数调用嵌套在前一个调用中：\n\narrange(\n  select(\n    mutate(\n      filter(\n        flights, \n        dest == \"IAH\"\n      ),\n      speed = distance / air_time * 60\n    ),\n    year:day, dep_time, carrier, flight, speed\n  ),\n  desc(speed)\n)\n\n或者我们可以使用一堆中间对象：\n\nflights1 <- filter(flights, dest == \"IAH\")\nflights2 <- mutate(flights1, speed = distance / air_time * 60)\nflights3 <- select(flights2, year:day, dep_time, carrier, flight, speed)\narrange(flights3, desc(speed))\n\n虽然这两种形式都有它们的时间和地点，但管道通常会产生更易于编写和阅读的数据分析代码。\n要将管道添加到代码中，我们建议使用内置键盘快捷键 Ctrl/Cmd + Shift + M。 您需要对 RStudio 选项进行一项更改，以使用 |> 而不是 %>%，如 Figure 4.1 中所示；稍后将详细介绍 %>%。\n\n\n\n\nFigure 4.1: To insert |>, make sure the “Use native pipe operator” option is checked.\n\n\n\n\n\n\n\n\n\n\nmagrittr\n\n\n\n如果您已经使用 tidyverse 一段时间，您可能熟悉 magrittr 包提供的 %>% 管道。 magrittr 包包含在核心 tidyverse 中，因此您可以在加载 tidyverse 时使用 %>%：\n\nlibrary(tidyverse)\n\nmtcars %>% \n  group_by(cyl) %>%\n  summarize(n = n())\n\n对于简单的情况，|> 和 %>% 的行为相同。 那么为什么我们推荐 base pipe（|>）呢？ 首先，因为它是 base R 的一部分，所以它始终可供您使用，即使您不使用 tidyverse。 其次，|> 比 %>% 简单得多：从 2014 年发明 %>% 到 2021 年将 |> 包含在 R 4.1.0 之间，我们对管道有了更好地了解。 这允许基本实现放弃不常用和不太重要的功能。"
  },
  {
    "objectID": "data-transform.html#groups",
    "href": "data-transform.html#groups",
    "title": "4  Data transformation",
    "section": "\n4.5 Groups",
    "text": "4.5 Groups\n到目前为止，您已经了解了处理行和列的函数。 当您添加与组（groups）一起工作的能力时，dplyr 变得更加强大。 在本节中，我们将重点关注最重要的函数：group_by()、summarize() 和 slice 函数系列。\n\n4.5.1 group_by()\n\n使用 group_by() 将您的数据集分成对您的分析有意义的组：\n\nflights |> \n  group_by(month)\n#> # A tibble: 336,776 × 19\n#> # Groups:   month [12]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\ngroup_by() 不会更改数据，但是，如果您仔细查看输出，您会注意到输出表明它是按 month 分组的（Groups: month [12]）。 这意味着后续操作现在将按 month 进行。 group_by() 将这个分组特征（称为类）添加到数据框，这改变了应用于数据的后续 verbs 的行为。\n\n4.5.2 summarize()\n\n最重要的分组操作是 summary，如果用于计算单个汇总统计数据，则会将数据框减少为每个组只有一行。 在 dplyr 中，此操作由 summarize()3 执行，如下例所示，它按月计算平均出发延迟：\n\nflights |> \n  group_by(month) |> \n  summarize(\n    avg_delay = mean(dep_delay)\n  )\n#> # A tibble: 12 × 2\n#>   month avg_delay\n#>   <int>     <dbl>\n#> 1     1        NA\n#> 2     2        NA\n#> 3     3        NA\n#> 4     4        NA\n#> 5     5        NA\n#> 6     6        NA\n#> # ℹ 6 more rows\n\n呃！ 出了点问题，我们所有的结果都是 NA（发音为”N-A”），R 的缺失值符号。 发生这种情况是因为一些观察到的航班在延误列中缺少数据，因此当我们计算包括这些值的平均值时，我们得到了 NA 结果。 我们将在 Chapter 19 中回来详细讨论缺失值，但现在我们将告诉 mean() 函数通过将参数 na.rm 设置为 TRUE 来忽略所有缺失值：\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE)\n  )\n#> # A tibble: 12 × 2\n#>   month delay\n#>   <int> <dbl>\n#> 1     1  10.0\n#> 2     2  10.8\n#> 3     3  13.2\n#> 4     4  13.9\n#> 5     5  13.0\n#> 6     6  20.8\n#> # ℹ 6 more rows\n\n您可以在一次调用 summarize() 中创建任意数量的 summaries。 您将在接下来的章节中学习各种有用的 summaries，但一个非常有用的 summary 是 n()，它返回每组中的行数：\n\nflights |> \n  group_by(month) |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n#> # A tibble: 12 × 3\n#>   month delay     n\n#>   <int> <dbl> <int>\n#> 1     1  10.0 27004\n#> 2     2  10.8 24951\n#> 3     3  13.2 28834\n#> 4     4  13.9 28330\n#> 5     5  13.0 28796\n#> 6     6  20.8 28243\n#> # ℹ 6 more rows\n\nMeans 和 counts 可以让你在数据科学中走得更远！\n\n4.5.3 The slice_ functions\n有五个方便的函数可让您提取每个组中的特定行：\n\n\ndf |> slice_head(n = 1) 从每组中取出第一行。\n\ndf |> slice_tail(n = 1) 从每组中取出最后一行。\n\ndf |> slice_min(x, n = 1) 获取列 x 中具有最小值的行。\n\ndf |> slice_max(x, n = 1) 获取列 x 中具有最大值的行。\n\ndf |> slice_sample(n = 1) 取一个随机行。\n\n您可以改变 n 以选择多行，或者不使用 n =，您可以使用 prop = 0.1 来选择（例如）每组中 10% 的行。 例如，以下代码查找到达每个目的地时最晚延误的航班：\n\nflights |> \n  group_by(dest) |> \n  slice_max(arr_delay, n = 1) |>\n  relocate(dest)\n#> # A tibble: 108 × 19\n#> # Groups:   dest [105]\n#>   dest   year month   day dep_time sched_dep_time dep_delay arr_time\n#>   <chr> <int> <int> <int>    <int>          <int>     <dbl>    <int>\n#> 1 ABQ    2013     7    22     2145           2007        98      132\n#> 2 ACK    2013     7    23     1139            800       219     1250\n#> 3 ALB    2013     1    25      123           2000       323      229\n#> 4 ANC    2013     8    17     1740           1625        75     2042\n#> 5 ATL    2013     7    22     2257            759       898      121\n#> 6 AUS    2013     7    10     2056           1505       351     2347\n#> # ℹ 102 more rows\n#> # ℹ 11 more variables: sched_arr_time <int>, arr_delay <dbl>, …\n\n请注意，有 105 个目的地（destinations），但我们在这里得到 108 行。 这是怎么回事？ slice_min() 和 slice_max() 保持绑定值，所以 n = 1 意味着给我们所有具有最高值的行。 如果您只想每组一行，您可以设置 with_ties = FALSE。\n这类似于使用 summarize() 计算最大延迟，但是您得到的是整个对应行（如果有平局则为行）而不是单个汇总统计数据。\n\n4.5.4 Grouping by multiple variables\n您可以使用多个变量创建 groups。 例如，我们可以为每个日期创建一个 group。\n\ndaily <- flights |>  \n  group_by(year, month, day)\ndaily\n#> # A tibble: 336,776 × 19\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n当您 summarize 由多个变量分组的 tibble 时，每个 summary 都会剥离最后一组。 事后看来，这不是使该功能正常工作的好方法，但在不破坏现有代码的情况下很难进行更改。 为了使发生的事情一目了然，dplyr 会显示一条消息，告诉您如何更改此行为：\n\ndaily_flights <- daily |> \n  summarize(n = n())\n#> `summarise()` has grouped output by 'year', 'month'. You can override using\n#> the `.groups` argument.\n\n如果您对此行为感到满意，您可以明确请求它以抑制消息：\n\ndaily_flights <- daily |> \n  summarize(\n    n = n(), \n    .groups = \"drop_last\"\n  )\n\n或者，通过设置不同的值来更改默认行为，例如，\"drop\" 删除所有分组或 \"keep\" 保留相同的组。\n\n4.5.5 Ungrouping\n您可能还想在不使用 summarize() 的情况下从数据框中删除分组。 你可以用 ungroup() 来做到这一点。\n\ndaily |> \n  ungroup()\n#> # A tibble: 336,776 × 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # ℹ 336,770 more rows\n#> # ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>, …\n\n现在让我们看看当您汇总未分组的数据框时会发生什么。\n\ndaily |> \n  ungroup() |>\n  summarize(\n    avg_delay = mean(dep_delay, na.rm = TRUE), \n    flights = n()\n  )\n#> # A tibble: 1 × 2\n#>   avg_delay flights\n#>       <dbl>   <int>\n#> 1      12.6  336776\n\n您会返回一行，因为 dplyr 将未分组数据框中的所有行视为属于一个组。\n\n4.5.6 .by\n\ndplyr 1.1.0 包括一个新的、实验性的、用于每个操作分组的语法，即 .by 参数。 group_by() 和 ungroup() 不会消失，但您现在还可以使用 .by 参数在单个操作中进行分组：\n\nflights |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = month\n  )\n\n或者如果你想按多个变量分组：\n\nflights |> \n  summarize(\n    delay = mean(dep_delay, na.rm = TRUE), \n    n = n(),\n    .by = c(origin, dest)\n  )\n\n.by 适用于所有动词，优点是您不需要使用 .groups 参数来取消分组消息或在完成后使用 ungroup()。\n我们在本章中没有关注这个语法，因为在我们写这本书的时候它是非常新的。 我们确实想提及它，因为我们认为它有很多希望并且很可能很受欢迎。 您可以在 dplyr 1.1.0 博客文章 中了解更多信息。\n\n4.5.7 Exercises\n\n哪家航空公司的平均延误最严重？ 挑战：你能分清糟糕机场与糟糕承运人的影响吗？ 为什么/为什么不？ （提示：考虑 flights |> group_by(carrier, dest) |> summarize(n())）\n找出从每个目的地出发时延误最严重的航班。\n一天中的延迟如何变化。 用一个图说明你的答案。\n如果您向 slice_min() 和相关函数提供负数 n 会发生什么？\n根据您刚刚学习的 dplyr verbs 解释 count() 的作用。 count() 的 sort 参数有什么作用？\n\n假设我们有以下微型数据框：\n\ndf <- tibble(\n  x = 1:5,\n  y = c(\"a\", \"b\", \"a\", \"a\", \"b\"),\n  z = c(\"K\", \"K\", \"L\", \"L\", \"K\")\n)\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 group_by() 的作用。\n\ndf |>\n  group_by(y)\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 arrange() 的作用。 还要评论它与 (a) 部分中的 group_by() 有何不同？\n\ndf |>\n  arrange(y)\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 pipeline 的作用。\n\ndf |>\n  group_by(y) |>\n  summarize(mean_x = mean(x))\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 pipeline 的作用。 然后，评论消息的内容。\n\ndf |>\n  group_by(y, z) |>\n  summarize(mean_x = mean(x))\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 pipeline 的作用。 输出与 (d) 部分中的输出有何不同。\n\ndf |>\n  group_by(y, z) |>\n  summarize(mean_x = mean(x), .groups = \"drop\")\n\n\n\n写下您认为输出的样子，然后检查您是否正确，并描述 pipeline 的作用。 两条 pipelines 的输出有何不同？\n\ndf |>\n  group_by(y, z) |>\n  summarize(mean_x = mean(x))\n\ndf |>\n  group_by(y, z) |>\n  mutate(mean_x = mean(x))"
  },
  {
    "objectID": "data-transform.html#sec-sample-size",
    "href": "data-transform.html#sec-sample-size",
    "title": "4  Data transformation",
    "section": "\n4.6 Case study: aggregates and sample size",
    "text": "4.6 Case study: aggregates and sample size\n每当您进行任何聚合（aggregation）时，包含一个计数 (n()) 总是一个好主意。 这样，您就可以确保您不会根据非常少量的数据得出结论。 我们将使用 Lahman 包中的一些 baseball 数据来证明这一点。 具体来说，我们将比较球员击球次数 (H) 与他们尝试将球投入比赛的次数 (AB) 的比例：\n\nbatters <- Lahman::Batting |> \n  group_by(playerID) |> \n  summarize(\n    performance = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    n = sum(AB, na.rm = TRUE)\n  )\nbatters\n#> # A tibble: 20,469 × 3\n#>   playerID  performance     n\n#>   <chr>           <dbl> <int>\n#> 1 aardsda01      0          4\n#> 2 aaronha01      0.305  12364\n#> 3 aaronto01      0.229    944\n#> 4 aasedo01       0          5\n#> 5 abadan01       0.0952    21\n#> 6 abadfe01       0.111      9\n#> # ℹ 20,463 more rows\n\n当我们绘制击球手的技能（以击球率 performance 衡量）与击球机会的数量（以击球次数 n 衡量）之间的关系时，您会看到两种模式：\n\n在击球次数较少的球员中，performance 的差异更大。 该图的形状非常有特色：每当您绘制平均值（或其他汇总统计数据）与组大小的关系时，您都会看到变异随着样本大小的增加而减小4。\n技能（performance）和击球机会 (n) 之间存在正相关关系，因为球队希望给最好的击球手最多的击球机会。\n\n\nbatters |> \n  filter(n > 100) |> \n  ggplot(aes(x = n, y = performance)) +\n  geom_point(alpha = 1 / 10) + \n  geom_smooth(se = FALSE)\n\n\n\n\n请注意组合 ggplot2 和 dplyr 的便利模式。 您只需要记住从用于数据集处理的 |> 切换到用于向绘图添加图层的 + 。\n这对排名也有重要影响。 如果你天真地按 desc(performance) 排序，平均击球率最好的人显然是那些尝试将球打入比赛并碰巧被击中的人，他们不一定是技术最好的球员 :\n\nbatters |> \n  arrange(desc(performance))\n#> # A tibble: 20,469 × 3\n#>   playerID  performance     n\n#>   <chr>           <dbl> <int>\n#> 1 abramge01           1     1\n#> 2 alberan01           1     1\n#> 3 banisje01           1     1\n#> 4 bartocl01           1     1\n#> 5 bassdo01            1     1\n#> 6 birasst01           1     2\n#> # ℹ 20,463 more rows\n\n您可以在 http://varianceexplained.org/r/empirical_bayes_baseball/ 和 https://www.evanmiller.org/how-not-to-sort-by-average-rating.html 找到这个问题的一个很好的解释以及如何克服它。"
  },
  {
    "objectID": "data-transform.html#summary",
    "href": "data-transform.html#summary",
    "title": "4  Data transformation",
    "section": "\n4.7 Summary",
    "text": "4.7 Summary\n在本章中，您学习了 dplyr 提供的用于处理 data frames 的工具。 这些工具大致分为三类：操作行的工具（如 filter() 和 arrange()），操作列的工具（如 select() 和 mutate()），以及那些 操纵组的工具（如 group_by() 和 summarize()）。 在本章中，我们重点介绍了这些 “whole data frame” 工具，但您还没有学到很多关于可以使用单个变量做什么的知识。 我们将在本书的 Transform 部分回到这一点，每一章都会为您提供用于特定类型变量的工具。\n在下一章中，我们将回到工作流来讨论代码风格的重要性，让您的代码井井有条，以便您和其他人轻松阅读和理解您的代码。"
  },
  {
    "objectID": "data-tidy.html#introduction",
    "href": "data-tidy.html#introduction",
    "title": "6  Data tidying",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.”\n— Leo Tolstoy\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.”\n— Hadley Wickham\n\n在这一章中，您将学习使用一种称为 tidy data 的系统，在 R 中以一种一致的方式组织您的数据。 将数据转换成这种格式需要一些初始工作，但这种工作在长期来看是值得的。 一旦您拥有 tidy data 和 tidyverse 包中提供的 tidy tools，您将花费更少的时间将数据从一种表示转换为另一种表示，从而能够更多地专注于您关心的数据问题。\n在这一章中，您首先将首先学习 tidy data 的定义，并将其应用于一个简单的示例数据集。 然后，我们将深入探讨用于整理数据的主要工具：数据透视（pivoting）。 pivoting 使您可以在不改变任何值的情况下改变数据的形式。\n\n6.1.1 Prerequisites\n在本章中，我们将专注于 tidyr，这是一个提供了一系列工具来帮助整理混乱数据集的包。 tidyr 是 core tidyverse 的成员之一。\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3\n\n从本章开始，我们将抑制来自 library(tidyverse) 的加载消息。"
  },
  {
    "objectID": "data-tidy.html#sec-tidy-data",
    "href": "data-tidy.html#sec-tidy-data",
    "title": "6  Data tidying",
    "section": "\n6.2 Tidy data",
    "text": "6.2 Tidy data\n可以用多种方式表示相同的基础数据。 下面的示例展示了相同的数据以三种不同的方式组织。 每个数据集都显示了四个变量的相同值：country、year、population 和结核病（TB）的记录 cases，但是每个数据集以不同的方式组织这些值。\n\ntable1\n#> # A tibble: 6 × 4\n#>   country      year  cases population\n#>   <chr>       <dbl>  <dbl>      <dbl>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\n\ntable2\n#> # A tibble: 12 × 4\n#>   country      year type           count\n#>   <chr>       <dbl> <chr>          <dbl>\n#> 1 Afghanistan  1999 cases            745\n#> 2 Afghanistan  1999 population  19987071\n#> 3 Afghanistan  2000 cases           2666\n#> 4 Afghanistan  2000 population  20595360\n#> 5 Brazil       1999 cases          37737\n#> 6 Brazil       1999 population 172006362\n#> # ℹ 6 more rows\n\ntable3\n#> # A tibble: 6 × 3\n#>   country      year rate             \n#>   <chr>       <dbl> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583\n\n这些都是相同基础数据的表示方式，但它们在使用上并不同样方便。 其中一个数据集，table1，因为它是整洁的（tidy），所以在 tidyverse 内部使用起来会更加方便。\n有三个相互关联的规则定义了一个整洁的数据集：\n\n每个变量是一列，每列是一个变量。\n每个样本是一行，每行是一个样本。\n每个值是一个单元格，每个单元格是一个单一的值。\n\nFigure 6.1 可视化地展示了这些规则。\n\n\n\n\nFigure 6.1: The following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells.\n\n\n\n\n为什么要确保你的数据是整洁的（tidy）？ 有两个主要的优势：\n\n选择一种一致的数据存储方式有一个普遍的优势。 如果您拥有一种一致的数据结构，学习与之配套的工具会更容易，因为它们具有基本的统一性。\n将变量（variables）放置在列（columns）中具有特定的优势，因为这可以展现出 R 的向量化（vectorized）特性。 正如您在 ?sec-mutate 和 ?sec-summarize 中学到的，大多数内置的 R 函数都可以处理值的向量（vectors）。 这使得转换整洁数据（tidy data）感觉特别自然。\n\ndplyr、ggplot2 和 tidyverse 中的其他所有包都被设计用于处理整洁数据（tidy data）。 以下是一些小例子，展示了如何处理 table1。\n\n# Compute rate per 10,000\ntable1 |>\n  mutate(rate = cases / population * 10000)\n#> # A tibble: 6 × 5\n#>   country      year  cases population  rate\n#>   <chr>       <dbl>  <dbl>      <dbl> <dbl>\n#> 1 Afghanistan  1999    745   19987071 0.373\n#> 2 Afghanistan  2000   2666   20595360 1.29 \n#> 3 Brazil       1999  37737  172006362 2.19 \n#> 4 Brazil       2000  80488  174504898 4.61 \n#> 5 China        1999 212258 1272915272 1.67 \n#> 6 China        2000 213766 1280428583 1.67\n\n# Compute total cases per year\ntable1 |> \n  group_by(year) |> \n  summarize(total_cases = sum(cases))\n#> # A tibble: 2 × 2\n#>    year total_cases\n#>   <dbl>       <dbl>\n#> 1  1999      250740\n#> 2  2000      296920\n\n# Visualize changes over time\nggplot(table1, aes(x = year, y = cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000)) # x-axis breaks at 1999 and 2000\n\n\n\n\n\n6.2.1 Exercises\n\n对于每个示例表格，描述每个观测（observation）和每列代表的内容。\n\n描绘出计算 table2 和 table3中的 rate 所使用的过程。 您需要执行四个操作：\n\n提取每个国家每年的结核病病例数（cases）。\n提取每个国家每年的相应人口（population）。\n将病例数除以人口，并乘以 10000。\n存储在适当的位置。\n\n您还没有学习到实际执行这些操作所需的所有函数，但您应该能够思考所需的转换过程。"
  },
  {
    "objectID": "data-tidy.html#sec-pivoting",
    "href": "data-tidy.html#sec-pivoting",
    "title": "6  Data tidying",
    "section": "\n6.3 Lengthening data",
    "text": "6.3 Lengthening data\n整洁数据（tidy data）的原则可能看起来如此显而易见，以至于您会想知道是否会遇到不整洁的数据集。 然而，不幸的是，大多数真实数据都是不整洁的。 这主要有两个原因：\n\n数据通常被组织成为实现除了分析之外的某个目标。 例如，常见的情况是为了简化数据输入而结构化数据，而不是为了方便分析。\n大多数人不熟悉整洁数据（tidy data）的原则，除非您花费大量时间处理数据，否则很难自己推导出这些原则。\n\n这意味着大多数实际分析都需要进行一些整理工作。 首先，您需要确定基础变量（variables）和观测（observations）是什么。 有时这很容易；其他时候，您可能需要与最初生成数据的人进行咨询。 接下来，您将 pivot 您的数据为整洁的形式，其中变量（variables）位于列（columns）中，观测（observations）位于行（rows）中。\ntidyr 提供了两个用于数据透视（pivoting）的函数：pivot_longer() 和 pivot_wider()。 我们首先从 pivot_longer() 开始，因为它是最常见的情况。 让我们深入一些示例。\n\n6.3.1 Data in column names\nbillboard 数据集记录了 2000 年歌曲的 billboard 排名：\n\nbillboard\n#> # A tibble: 317 × 79\n#>   artist       track               date.entered   wk1   wk2   wk3   wk4   wk5\n#>   <chr>        <chr>               <date>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1 2 Pac        Baby Don't Cry (Ke… 2000-02-26      87    82    72    77    87\n#> 2 2Ge+her      The Hardest Part O… 2000-09-02      91    87    92    NA    NA\n#> 3 3 Doors Down Kryptonite          2000-04-08      81    70    68    67    66\n#> 4 3 Doors Down Loser               2000-10-21      76    76    72    69    67\n#> 5 504 Boyz     Wobble Wobble       2000-04-15      57    34    25    17    17\n#> 6 98^0         Give Me Just One N… 2000-08-19      51    39    34    26    26\n#> # ℹ 311 more rows\n#> # ℹ 71 more variables: wk6 <dbl>, wk7 <dbl>, wk8 <dbl>, wk9 <dbl>, …\n\n在这个数据集中，每个观测（observation）都是一首歌曲。 前三列（artist, track and date.entered）是描述歌曲的变量（variables）。 然后我们有 76 列（wk1-wk76），描述了歌曲在每周的排名1。 这里，列名是一个变量（week），单元格的值是另一个变量（rank）。\n为了整理这个数据，我们将使用 pivot_longer() 函数：\n\nbillboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n#> # A tibble: 24,092 × 5\n#>    artist track                   date.entered week   rank\n#>    <chr>  <chr>                   <date>       <chr> <dbl>\n#>  1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#>  2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#>  3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#>  4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#>  5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#>  6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#>  7 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk7      99\n#>  8 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk8      NA\n#>  9 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk9      NA\n#> 10 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk10     NA\n#> # ℹ 24,082 more rows\n\n数据之后，还有三个关键论点：\n\n\ncols 指定哪些列需要被 pivoted，即哪些列不是变量。此参数使用与 select() 相同的语法，因此这里我们可以使用 !c(artist, track, date.entered) 或 starts_with(\"wk\")。\n\nnames_to 命名存储在 column names 中的变量，我们将该变量命名为 week。\n\nvalues_to 命名存储在 cell values 中的变量，我们将该变量命名为 rank。\n\n请注意，在代码中引用了 \"week\" 和 \"rank\" ，因为这些是我们正在创建的新变量，当我们运行 pivot_longer() 调用时，它们还不存在于数据中。\n现在让我们将注意力转向结果，longer data frame。 如果一首歌进入前 100 名的时间少于 76 周，会发生什么情况？ 以 2 Pac 的 “Baby Don’t Cry” 为例。 上面的输出表明它只在前 100 名中停留了 7 周，其余所有周都用缺失值填充。 这些 NAs 并不真正代表未知的观察结果；它们是由 dataset2 的结构强制存在的，因此我们可以要求 pivot_longer() 通过设置 values_drop_na = TRUE 来删除它们：\n\nbillboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  )\n#> # A tibble: 5,307 × 5\n#>   artist track                   date.entered week   rank\n#>   <chr>  <chr>                   <date>       <chr> <dbl>\n#> 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk1      87\n#> 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk2      82\n#> 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk3      72\n#> 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk4      77\n#> 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk5      87\n#> 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26   wk6      94\n#> # ℹ 5,301 more rows\n\n行数现在少得多，表明许多具有 NAs 的行已被删除。\n您可能还想知道如果一首歌进入前 100 名超过 76 周会发生什么？ 我们无法从这些数据中看出，但您可能会猜测额外的列 wk77、wk78, … 将添加到数据集中。\n这些数据现在很整洁，但我们可以通过使用 mutate() 和 readr::parse_number() 将 week 值从字符串（character strings）转换为数字（numbers），从而使将来的计算变得更容易。 parse_number() 是一个方便的函数，它将从字符串中提取第一个数字，忽略所有其他文本。\n\nbillboard_longer <- billboard |> \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |> \n  mutate(\n    week = parse_number(week)\n  )\nbillboard_longer\n#> # A tibble: 5,307 × 5\n#>   artist track                   date.entered  week  rank\n#>   <chr>  <chr>                   <date>       <dbl> <dbl>\n#> 1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n#> 2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n#> 3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n#> 4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n#> 5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n#> 6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n#> # ℹ 5,301 more rows\n\n现在我们在一个变量中拥有所有 week 数值，在另一个变量中拥有所有 rank 值，我们可以很好地可视化歌曲排名如何随时间变化。 代码如下所示，结果如 Figure 6.2 所示。 我们可以看到，很少有歌曲能在前 100 名中保持超过 20 周的时间。\n\nbillboard_longer |> \n  ggplot(aes(x = week, y = rank, group = track)) + \n  geom_line(alpha = 0.25) + \n  scale_y_reverse()\n\n\n\nFigure 6.2: A line plot showing how the rank of a song changes over time.\n\n\n\n\n\n6.3.2 How does pivoting work?\n现在您已经了解了如何使用 pivoting 来重塑数据，让我们花一点时间来直观地了解 pivoting 对数据的作用。 让我们从一个非常简单的数据集开始，以便更容易地了解正在发生的情况。 假设我们有 3 位 id 为 A、B、C 的患者（patients），我们对每位患者（patients）进行两次血压（blood pressure）测量。 我们将使用 tribble() 创建数据，这是一个手动构建小 tibbles 的便捷函数：\n\ndf <- tribble(\n  ~id,  ~bp1, ~bp2,\n   \"A\",  100,  120,\n   \"B\",  140,  115,\n   \"C\",  120,  125\n)\n\n我们希望我们的新数据集具有三个变量：id（已存在）、measurement（列名称）和 value（单元格值）。 为了实现这一点，我们需要 pivot df longer：\n\ndf |> \n  pivot_longer(\n    cols = bp1:bp2,\n    names_to = \"measurement\",\n    values_to = \"value\"\n  )\n#> # A tibble: 6 × 3\n#>   id    measurement value\n#>   <chr> <chr>       <dbl>\n#> 1 A     bp1           100\n#> 2 A     bp2           120\n#> 3 B     bp1           140\n#> 4 B     bp2           115\n#> 5 C     bp1           120\n#> 6 C     bp2           125\n\n重塑是如何进行的？ 如果我们逐列思考就更容易看出。 如 Figure 6.3 所示，对于原始数据集（id）中已经是变量的列中的值需要重复，对于每个被 pivoted 的列重复一次。\n\n\n\n\nFigure 6.3: 已经是变量的列需要重复，每个 pivotted 的列重复一次。\n\n\n\n\ncolumn names 成为新变量中的值，其名称由 names_to 定义，如 Figure 6.4 所示。 它们需要对原始数据集中的每一行重复一次。\n\n\n\n\nFigure 6.4: pivoted columns 的 column names 成为 new column 中的值。对于原始数据集的每一行，这些值需要重复一次。\n\n\n\n\n单元格值也会成为新变量中的值，其名称由 values_to 定义。 它们一排一排地展开。 Figure 6.5 说明了该过程。\n\n\n\n\nFigure 6.5: 值的数值被保留（不重复），但逐行展开。\n\n\n\n\n\n6.3.3 Many variables in column names\n当您将多条信息塞入 column names 中，并且您希望将这些信息存储在单独的新变量中时，就会出现更具挑战性的情况。 例如，采用 who2 数据集，这是跟你之前看到的 table1 是相同来源的数据：\n\nwho2\n#> # A tibble: 7,240 × 58\n#>   country      year sp_m_014 sp_m_1524 sp_m_2534 sp_m_3544 sp_m_4554\n#>   <chr>       <dbl>    <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 Afghanistan  1980       NA        NA        NA        NA        NA\n#> 2 Afghanistan  1981       NA        NA        NA        NA        NA\n#> 3 Afghanistan  1982       NA        NA        NA        NA        NA\n#> 4 Afghanistan  1983       NA        NA        NA        NA        NA\n#> 5 Afghanistan  1984       NA        NA        NA        NA        NA\n#> 6 Afghanistan  1985       NA        NA        NA        NA        NA\n#> # ℹ 7,234 more rows\n#> # ℹ 51 more variables: sp_m_5564 <dbl>, sp_m_65 <dbl>, sp_f_014 <dbl>, …\n\n该数据集由世界卫生组织收集，记录有关结核病诊断（tuberculosis diagnoses）的信息。 有两列已经是变量（variables）且易于解释：country 和 year。 接下来是 56 列，例如 sp_m_014、ep_m_4554、rel_m_3544。 如果你盯着这些列足够长的时间，你会发现其中存在一种模式。 每个列名称由三部分组成，并用 _ 分隔。 第一部分 sp/rel/ep 描述了用于诊断的方法（diagnosis），第二部分 m/f 是性别（gender）（在此数据集中编码为二进制变量），第三部分 014/1524/2534/3544/4554/5564/65 是年龄范围（age）（例如，014 代表 0-14）。\n因此，在本例中，who2 中记录了六条信息：country 和 year（已经是列）；诊断方法（diagnosis）、性别类别（gender）和年龄范围类别（age）（包含在其他列名称中）；以及该类别中的患者数量（count）（单元格值）。 为了将这六条信息组织在六个单独的列中，我们使用 pivot_longer() 以及 names_to 的列名称向量将原始变量名称拆分为 names_sep 片段以及 values_to 的列名称：\n\nwho2 |> \n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"), \n    names_sep = \"_\",\n    values_to = \"count\"\n  )\n#> # A tibble: 405,440 × 6\n#>   country      year diagnosis gender age   count\n#>   <chr>       <dbl> <chr>     <chr>  <chr> <dbl>\n#> 1 Afghanistan  1980 sp        m      014      NA\n#> 2 Afghanistan  1980 sp        m      1524     NA\n#> 3 Afghanistan  1980 sp        m      2534     NA\n#> 4 Afghanistan  1980 sp        m      3544     NA\n#> 5 Afghanistan  1980 sp        m      4554     NA\n#> 6 Afghanistan  1980 sp        m      5564     NA\n#> # ℹ 405,434 more rows\n\nnames_sep 的替代方法是 names_pattern，你可以使用它从更复杂的命名场景中提取变量， 一旦您在 Chapter 16 中了解了正则表达式。\n从概念上讲，这只是您已经见过的更简单情况的一个微小变化。 Figure 6.6 显示了基本思想：现在，column names 不再 pivoting 成单个列，而是 pivot 成多个列。 您可以想象这发生在两个步骤中（first pivoting and then separating），但在幕后它发生在一步中，因为这样更快。\n\n\n\n\nFigure 6.6: 对名称中包含多条信息的列进行 Pivoting 意味着每个列名称现在会填充多个输出列中的值。\n\n\n\n\n\n6.3.4 Data and variable names in the column headers\n复杂性的下一步是列名包含变量值和变量名的混合。 以 household 数据集为例：\n\nhousehold\n#> # A tibble: 5 × 5\n#>   family dob_child1 dob_child2 name_child1 name_child2\n#>    <int> <date>     <date>     <chr>       <chr>      \n#> 1      1 1998-11-26 2000-01-29 Susan       Jose       \n#> 2      2 1996-06-22 NA         Mark        <NA>       \n#> 3      3 2002-07-11 2004-04-05 Sam         Seth       \n#> 4      4 2004-10-10 2009-08-27 Craig       Khai       \n#> 5      5 2000-12-05 2005-02-28 Parker      Gracie\n\n该数据集包含有关五个家庭的数据，其中最多包含两个孩子的姓名和出生日期。 此数据集中的新挑战是列名称包含两个变量的名称（dob、name）和另一个变量的值（child，值为 1 或 2）。 为了解决这个问题，我们再次需要向 names_to 提供一个向量，但这次我们使用特殊的 “.value” 语句；这不是变量的名称，而是告诉 pivot_longer() 执行不同操作的唯一值。 这会覆盖通常的 values_to 参数，以使用 pivoted column name 的第一个组成部分作为输出中的变量名称。\n\nhousehold |> \n  pivot_longer(\n    cols = !family, \n    names_to = c(\".value\", \"child\"), \n    names_sep = \"_\", \n    values_drop_na = TRUE\n  )\n#> # A tibble: 9 × 4\n#>   family child  dob        name \n#>    <int> <chr>  <date>     <chr>\n#> 1      1 child1 1998-11-26 Susan\n#> 2      1 child2 2000-01-29 Jose \n#> 3      2 child1 1996-06-22 Mark \n#> 4      3 child1 2002-07-11 Sam  \n#> 5      3 child2 2004-04-05 Seth \n#> 6      4 child1 2004-10-10 Craig\n#> # ℹ 3 more rows\n\n我们再次使用 values_drop_na = TRUE，因为输入的形状强制创建显式缺失变量（例如，对于只有一个孩子的家庭）。\nFigure 6.7 通过一个更简单的示例说明了基本思想。 当您在 names_to 中使用 \".value\" 时，输入中的列名称将影响输出中的值和变量名称。\n\n\n\n\nFigure 6.7: 使用 names_to = c(\".value\", \"num\") 进行 Pivoting 将列名称分为两个部分：第一部分确定输出列名称（x or y），第二部分确定 num 列的值。"
  },
  {
    "objectID": "data-tidy.html#widening-data",
    "href": "data-tidy.html#widening-data",
    "title": "6  Data tidying",
    "section": "\n6.4 Widening data",
    "text": "6.4 Widening data\n到目前为止，我们已经使用 pivot_longer() 来解决值以列名结束的常见问题。 接下来，我们将转向 pivot_wider()，它通过增加列和减少行来使数据集更宽（wider），并且当一个观测（observation）分布在多行上时会有所帮助。 这种情况在野外似乎不太常见，但在处理政府数据时似乎确实经常出现。\n我们首先查看 cms_patient_experience，这是来自 Medicare 和 Medicaid 服务中心的数据集，用于收集有关患者体验的数据：\n\ncms_patient_experience\n#> # A tibble: 500 × 5\n#>   org_pac_id org_nm                     measure_cd   measure_title   prf_rate\n#>   <chr>      <chr>                      <chr>        <chr>              <dbl>\n#> 1 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_1  CAHPS for MIPS…       63\n#> 2 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_2  CAHPS for MIPS…       87\n#> 3 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_3  CAHPS for MIPS…       86\n#> 4 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_5  CAHPS for MIPS…       57\n#> 5 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_8  CAHPS for MIPS…       85\n#> 6 0446157747 USC CARE MEDICAL GROUP INC CAHPS_GRP_12 CAHPS for MIPS…       24\n#> # ℹ 494 more rows\n\n所研究的核心单位是一个组织，但每个组织分布在六行中，每一行代表调查组织中进行的每个测量。 我们可以使用 distinct() 查看 measure_cd 和 measure_title 的完整值集：\n\ncms_patient_experience |> \n  distinct(measure_cd, measure_title)\n#> # A tibble: 6 × 2\n#>   measure_cd   measure_title                                                 \n#>   <chr>        <chr>                                                         \n#> 1 CAHPS_GRP_1  CAHPS for MIPS SSM: Getting Timely Care, Appointments, and In…\n#> 2 CAHPS_GRP_2  CAHPS for MIPS SSM: How Well Providers Communicate            \n#> 3 CAHPS_GRP_3  CAHPS for MIPS SSM: Patient's Rating of Provider              \n#> 4 CAHPS_GRP_5  CAHPS for MIPS SSM: Health Promotion and Education            \n#> 5 CAHPS_GRP_8  CAHPS for MIPS SSM: Courteous and Helpful Office Staff        \n#> 6 CAHPS_GRP_12 CAHPS for MIPS SSM: Stewardship of Patient Resources\n\n这些列都不会成为特别好的变量名称：measure_cd 不会暗示变量的含义，而 measure_title 是一个包含空格的长句子。 我们现在将使用 measure_cd 作为新列名称的来源，但在实际分析中，您可能希望创建自己的既短又有意义的变量名称。\npivot_wider() 与 pivot_longer() 具有相反的接口：我们不需要选择新的列名，而是需要提供定义值 (values_from) 和列名 (names_from) 的现有列：\n\ncms_patient_experience |> \n  pivot_wider(\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n#> # A tibble: 500 × 9\n#>   org_pac_id org_nm                   measure_title   CAHPS_GRP_1 CAHPS_GRP_2\n#>   <chr>      <chr>                    <chr>                 <dbl>       <dbl>\n#> 1 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          63          NA\n#> 2 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          87\n#> 3 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#> 4 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#> 5 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#> 6 0446157747 USC CARE MEDICAL GROUP … CAHPS for MIPS…          NA          NA\n#> # ℹ 494 more rows\n#> # ℹ 4 more variables: CAHPS_GRP_3 <dbl>, CAHPS_GRP_5 <dbl>, …\n\n输出看起来不太正确；我们似乎仍然为每个组织有多行。 这是因为，我们还需要告诉 pivot_wider() 哪一列或多列具有唯一标识每一行的值；在本例中，这些是以 \"org\" 开头的变量：\n\ncms_patient_experience |> \n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\n#> # A tibble: 95 × 8\n#>   org_pac_id org_nm           CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5\n#>   <chr>      <chr>                  <dbl>       <dbl>       <dbl>       <dbl>\n#> 1 0446157747 USC CARE MEDICA…          63          87          86          57\n#> 2 0446162697 ASSOCIATION OF …          59          85          83          63\n#> 3 0547164295 BEAVER MEDICAL …          49          NA          75          44\n#> 4 0749333730 CAPE PHYSICIANS…          67          84          85          65\n#> 5 0840104360 ALLIANCE PHYSIC…          66          87          87          64\n#> 6 0840109864 REX HOSPITAL INC          73          87          84          67\n#> # ℹ 89 more rows\n#> # ℹ 2 more variables: CAHPS_GRP_8 <dbl>, CAHPS_GRP_12 <dbl>\n\n这给了我们我们正在寻找的输出。\n\n6.4.1 How does pivot_wider() work?\n为了理解 pivot_wider() 的工作原理，让我们再次从一个非常简单的数据集开始。 这次我们有两名 id 为 A 和 B 的患者，我们对患者 A 进行了三次血压（blood pressure）测量，对患者 B 进行了两次血压（blood pressure）测量：\n\ndf <- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"B\",        \"bp1\",    140,\n  \"B\",        \"bp2\",    115, \n  \"A\",        \"bp2\",    120,\n  \"A\",        \"bp3\",    105\n)\n\n我们将从 value 列中获取值并从 measurement 列中获取名称：\n\ndf |> \n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n#> # A tibble: 2 × 4\n#>   id      bp1   bp2   bp3\n#>   <chr> <dbl> <dbl> <dbl>\n#> 1 A       100   120   105\n#> 2 B       140   115    NA\n\n要开始该过程，pivot_wider() 需要首先弄清楚行和列中的内容。 新的列名称将是 measurement 中唯一的值。\n\ndf |> \n  distinct(measurement) |> \n  pull()\n#> [1] \"bp1\" \"bp2\" \"bp3\"\n\n默认情况下，输出中的行由未进入新名称或值的所有变量确定。 这些称为 id_cols。 这里只有一列，但一般可以有任意数量。\n\ndf |> \n  select(-measurement, -value) |> \n  distinct()\n#> # A tibble: 2 × 1\n#>   id   \n#>   <chr>\n#> 1 A    \n#> 2 B\n\n然后，pivot_wider() 结合这些结果来生成一个空 data frame：\n\ndf |> \n  select(-measurement, -value) |> \n  distinct() |> \n  mutate(x = NA, y = NA, z = NA)\n#> # A tibble: 2 × 4\n#>   id    x     y     z    \n#>   <chr> <lgl> <lgl> <lgl>\n#> 1 A     NA    NA    NA   \n#> 2 B     NA    NA    NA\n\n然后，它使用输入中的数据填充所有缺失值。 在这种情况下，并非输出中的每个单元格在输入中都有对应的值，因为患者 B 没有第三次血压测量，因此该单元格仍然缺失。 我们将在 Chapter 19 中回到这个观点：pivot_wider() 可以”制造”缺失值。\n您可能还想知道如果输入中有多行对应于输出中的一个单元格，会发生什么情况。 下面的示例有两行对应于 id “A” 和 measurement “bp1”：\n\ndf <- tribble(\n  ~id, ~measurement, ~value,\n  \"A\",        \"bp1\",    100,\n  \"A\",        \"bp1\",    102,\n  \"A\",        \"bp2\",    120,\n  \"B\",        \"bp1\",    140, \n  \"B\",        \"bp2\",    115\n)\n\n如果我们尝试对此进行 pivot，我们会得到一个包含 list-columns 的输出，您将在 Chapter 24 中了解更多信息：\n\ndf |>\n  pivot_wider(\n    names_from = measurement,\n    values_from = value\n  )\n#> Warning: Values from `value` are not uniquely identified; output will contain\n#> list-cols.\n#> • Use `values_fn = list` to suppress this warning.\n#> • Use `values_fn = {summary_fun}` to summarise duplicates.\n#> • Use the following dplyr code to identify duplicates.\n#>   {data} %>%\n#>   dplyr::group_by(id, measurement) %>%\n#>   dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n#>   dplyr::filter(n > 1L)\n#> # A tibble: 2 × 3\n#>   id    bp1       bp2      \n#>   <chr> <list>    <list>   \n#> 1 A     <dbl [2]> <dbl [1]>\n#> 2 B     <dbl [1]> <dbl [1]>\n\n由于您还不知道如何处理此类数据，因此您需要按照警告中的提示找出问题所在：\n\ndf |> \n  group_by(id, measurement) |> \n  summarize(n = n(), .groups = \"drop\") |> \n  filter(n > 1)\n#> # A tibble: 1 × 3\n#>   id    measurement     n\n#>   <chr> <chr>       <int>\n#> 1 A     bp1             2\n\n然后，您需要找出数据出了什么问题，并修复潜在的损坏，或者使用您的 grouping 和 summarizing 技能来确保行和列值的每个组合只有一行。"
  },
  {
    "objectID": "data-tidy.html#summary",
    "href": "data-tidy.html#summary",
    "title": "6  Data tidying",
    "section": "\n6.5 Summary",
    "text": "6.5 Summary\n在本章中，您了解了整洁数据（tidy data）：列中包含变量、行中包含观测值的数据。 Tidy data 使 tidyverse 中的工作变得更加容易，因为它是大多数函数都可以理解的一致结构，主要的挑战是将数据从您收到的任何结构转换为 tidy 格式。 为此，您了解了 pivot_longer() 和 pivot_wider()，它们可以让您整理许多杂乱的数据集。 我们在这里提供的示例是从 vignette(\"pivot\", package = \"tidyr\") 中精选出来的，因此，如果您遇到本章无法帮助您解决的问题，那么该 vignette 是下一步尝试的好地方。\n另一个挑战是，对于给定的数据集，不可能将较长或较宽的版本标记为 “tidy”。 这在一定程度上反映了我们对 tidy data 的定义，我们说 tidy data 在每一列中都有一个变量，但我们实际上并没有定义变量是什么（而且很难做到这一点）。 务实一点并说变量是让你的分析最容易的任何东西都是完全可以的。 因此，如果您不知道如何进行一些计算，请考虑改变数据的组织方式；不要害怕根据需要进行整理、改造和重新整理！\n如果您喜欢本章并且想要了解有关基础理论的更多信息，您可以在《Journal of Statistical Software》上发表的 Tidy Data 论文中了解有关历史和理论基础的更多信息。\n现在您正在编写大量 R 代码，是时候了解有关将代码组织到文件和目录中的更多信息了。 在下一章中，您将了解脚本和项目的所有优点，以及它们提供的一些使您的生活更轻松的工具。"
  },
  {
    "objectID": "data-tidy.html#footnotes",
    "href": "data-tidy.html#footnotes",
    "title": "6  Data tidying",
    "section": "",
    "text": "只要歌曲在 2000 年的某个时刻进入了前 100 名，并且在出现后的 72 周内进行了追踪，就会包括该歌曲。↩︎\nWe’ll come back to this idea in Chapter 19.↩︎"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "11  Exploratory data analysis",
    "section": "\n11.1 Introduction",
    "text": "11.1 Introduction\n本章将向您展示如何使用可视化和转换以系统的方式探索数据，统计学家将这项任务称为探索性数据分析（exploratory data analysis），简称 EDA。 EDA 是一个迭代循环。 你：\n\n提出有关您的数据的问题。\n通过可视化、转换和建模数据来搜索答案。\n使用您学到的知识来完善您的问题和/或产生新问题。\n\nEDA 不是一个具有严格规则的正式流程。 最重要的是，EDA 是一种心态。 在 EDA 的初始阶段，您应该随意调查您想到的每个想法。 其中一些想法会成功，而另一些则会是死胡同。 随着探索的继续，您将获得一些特别富有成效的见解，最终将其写下来并与其他人交流。\nEDA 是任何数据分析的重要组成部分，即使主要的研究问题已经摆在您面前，因为您始终需要调查数据的质量。 数据清理只是 EDA 的一种应用：您询问数据是否符合您的期望。 要进行数据清理，您需要部署 EDA 的所有工具：可视化、转换和建模。\n\n11.1.1 Prerequisites\n在本章中，我们将结合您所学到的有关 dplyr 和 ggplot2 的知识来交互式地提出问题，用数据回答它们，然后提出新问题。\n\nlibrary(tidyverse)\n#> Warning: package 'tidyverse' was built under R version 4.2.3\n#> Warning: package 'ggplot2' was built under R version 4.2.3\n#> Warning: package 'tibble' was built under R version 4.2.3\n#> Warning: package 'tidyr' was built under R version 4.2.3\n#> Warning: package 'readr' was built under R version 4.2.3\n#> Warning: package 'purrr' was built under R version 4.2.3\n#> Warning: package 'dplyr' was built under R version 4.2.3\n#> Warning: package 'stringr' was built under R version 4.2.2\n#> Warning: package 'forcats' was built under R version 4.2.3\n#> Warning: package 'lubridate' was built under R version 4.2.3"
  },
  {
    "objectID": "EDA.html#questions",
    "href": "EDA.html#questions",
    "title": "11  Exploratory data analysis",
    "section": "\n11.2 Questions",
    "text": "11.2 Questions\n\n“There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox\n\n\n“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey\n\n您在 EDA 期间的目标是加深对数据的理解。 最简单的方法是使用问题作为指导调查的工具。 当您提出问题时，该问题会将您的注意力集中在数据集的特定部分上，并帮助您决定要进行哪些图形、模型或转换。\nEDA 从根本上来说是一个创造性的过程。 与大多数创意过程一样，提出高质量问题的关键是产生大量问题。 在分析开始时很难提出有启发性的问题，因为您不知道可以从数据集中收集哪些见解。 另一方面，您提出的每个新问题都会让您接触到数据的新方面，并增加您做出发现的机会。 如果您根据发现的内容提出一个新问题来跟进每个问题，则可以快速深入了解数据中最有趣的部分，并提出一组发人深省的问题。\n对于应该提出哪些问题来指导您的研究，没有任何规定。 然而，有两种类型的问题对于在数据中进行发现总是有用的。 您可以将这些问题大致表述为：\n\n我的变量中发生什么类型的变化？\n我的变量之间发生什么类型的共同变化？\n\n本章的其余部分将讨论这两个问题。 我们将解释什么是变化（variation）和共同变化（covariation），并向您展示回答每个问题的几种方法。"
  },
  {
    "objectID": "EDA.html#variation",
    "href": "EDA.html#variation",
    "title": "11  Exploratory data analysis",
    "section": "\n11.3 Variation",
    "text": "11.3 Variation\nVariation 是变量值在不同测量中发生变化的趋势。 在现实生活中你可以很容易地看到 variation；如果您两次测量任何连续变量，您将得到两个不同的结果。 即使您测量的是恒定量情况也是如此，例如光速。 您的每次测量都会包含少量误差，该误差因测量而异。 如果您在不同的对象（例如，不同人的眼睛颜色）或在不同的时间（例如，不同时刻的电子能级）进行测量，变量也会有所不同。 每个变量都有自己的变化模式，这可以揭示有关同一观察的测量之间以及不同观察之间的变化的有趣信息。 理解该模式的最佳方法是可视化变量值的分布，您已在 ?sec-data-visualization 中了解了这一点。\n我们将通过可视化 diamonds 数据集中 ~54,000 颗钻石的重量（carat）分布来开始我们的探索。 由于 carat 是一个数值变量，我们可以使用直方图（histogram）：\n\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\n\n现在您可以可视化 variation，您应该在图中寻找什么？ 您应该提出什么类型的后续问题？ 我们在下面列出了您将在图表中找到的最有用的信息类型，以及针对每种信息类型的一些后续问题。 提出好的后续问题的关键是依靠你的好奇心（你想了解更多什么？）以及你的怀疑精神（这怎么可能误导？）。\n\n11.3.1 Typical values\n在条形图和直方图中，高条形显示变量的常见值，较短的条形显示不太常见的值。 没有条形的位置显示数据中未看到的值。 要将这些信息转化为有用的问题，请寻找任何意想不到的东西：\n\n哪些值最常见？ 为什么？\n哪些值是稀有的？ 为什么？ 这符合您的期望吗？\n你能看到任何不寻常的模式吗？ 什么可以解释它们？\n\n让我们看一下较小钻石的 carat 分布。\n\nsmaller <- diamonds |> \n  filter(carat < 3)\n\nggplot(smaller, aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\n\n\n\n\n该直方图提出了几个有趣的问题：\n\n为什么整克拉和常见分数克拉的钻石更多？\n为什么每个峰稍右侧的菱形数量多于每个峰左侧的菱形数量？\n\n可视化还可以揭示 clusters，这表明数据中存在 subgroups。 要了解 subgroups，请询问：\n\n每个 subgroup 内的观察结果有何相似之处？\n不同 clusters 中的观察结果有何不同？\n您如何解释或描述这些 clusters？\n为什么 clusters 的出现可能会产生误导？\n\n其中一些问题可以通过数据来回答，而另一些则需要有关数据的领域专业知识。 其中许多会提示您探索变量之间的关系，例如，看看一个变量的值是否可以解释另一个变量的行为。 我们很快就会谈到这一点。\n\n11.3.2 Unusual values\n异常值（Outliers）是指不寻常的观察结果；数据点似乎不符合模式。 有时异常值是数据输入错误，有时它们只是在此数据收集中碰巧观察到的极端值，有时它们表明了重要的新发现。 当您有大量数据时，有时很难在直方图中看到异常值。 例如，从 diamonds 数据集中获取 y 变量的分布。 异常值的唯一证据是 x-axis 上异常宽的限制。\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5)\n\n\n\n\n常见的 bin 中有太多的观察结果，而稀有的 bin 则非常短，因此很难看到它们（尽管如果你专心地盯着 0，也许你会发现一些东西）。 为了更容易看到不寻常的值，我们需要使用 coord_cartesian() 缩放到 y-axis 的小值：\n\nggplot(diamonds, aes(x = y)) + \n  geom_histogram(binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\n\n\n\n\ncoord_cartesian() 还有一个 xlim() 参数，用于当您需要放大 x 轴时。 ggplot2 还具有 xlim() 和 ylim() 函数，它们的工作方式略有不同：它们丢弃超出限制的数据。\n这让我们可以看到存在三个不寻常的值：0, ~30, and ~60。 我们用 dplyr 将它们取出：\n\nunusual <- diamonds |> \n  filter(y < 3 | y > 20) |> \n  select(price, x, y, z) |>\n  arrange(y)\nunusual\n#> # A tibble: 9 × 4\n#>   price     x     y     z\n#>   <int> <dbl> <dbl> <dbl>\n#> 1  5139  0      0    0   \n#> 2  6381  0      0    0   \n#> 3 12800  0      0    0   \n#> 4 15686  0      0    0   \n#> 5 18034  0      0    0   \n#> 6  2130  0      0    0   \n#> 7  2130  0      0    0   \n#> 8  2075  5.15  31.8  5.12\n#> 9 12210  8.09  58.9  8.06\n\ny 变量测量这些钻石的三个尺寸之一，以 mm 为单位。 我们知道钻石的宽度不可能是 0mm，所以这些值一定是不正确的。 通过 EDA，我们发现了编码为 0 的缺失数据，这是我们通过简单搜索 NAs 永远无法找到的。 展望未来，我们可能会选择将这些值重新编码为 NAs，以防止误导计算。 我们可能还会怀疑 32mm 和 59mm 的测量结果令人难以置信：这些钻石超过一英寸长，但价格却不用花费数十万美元！\n在有或没有异常值的情况下重复分析是一个很好的做法。 如果它们对结果的影响很小，并且您无法弄清楚它们为什么在那里，那么忽略它们并继续前进是合理的。 然而，如果它们对你的结果有重大影响，你不应该无缘无故地放弃它们。 您需要找出导致它们的原因（例如数据输入错误），并披露您在文章中删除了它们。\n\n11.3.3 Exercises\n\n探索钻石中每个 x、y 和 z 变量的分布。 你学到了什么？ 想象一下钻石，以及如何确定长度、宽度和深度的尺寸。\n探索 price 分布。 您是否发现任何不寻常或令人惊讶的事情？ （提示：仔细考虑 binwidth 并确保尝试广泛的值。）\n0.99 克拉有多少颗钻石？ 1 克拉是多少？ 您认为造成这种差异的原因是什么？\n放大直方图时，比较和对比 coord_cartesian() 与 xlim() 或 ylim()。 如果未设置 binwidth 会发生什么？ 如果您尝试缩放，只显示半条，会发生什么情况？"
  },
  {
    "objectID": "EDA.html#sec-unusual-values-eda",
    "href": "EDA.html#sec-unusual-values-eda",
    "title": "11  Exploratory data analysis",
    "section": "\n11.4 Unusual values",
    "text": "11.4 Unusual values\n如果您在数据集中遇到了异常值，并且只想继续进行其余的分析，您有两种选择。\n\n\n删除具有奇怪值的整行：\n\ndiamonds2 <- diamonds |> \n  filter(between(y, 3, 20))\n\n我们不推荐此选项，因为一个无效值并不意味着该观察的所有其他值也无效。 此外，如果您的数据质量较低，当您将此方法应用于每个变量时，您可能会发现您没有任何数据了！\n\n\n相反，我们建议用缺失值替换异常值。 最简单的方法是使用 mutate() 用修改后的副本替换变量。 您可以使用 if_else() 函数将异常值替换为 NA：\n\ndiamonds2 <- diamonds |> \n  mutate(y = if_else(y < 3 | y > 20, NA, y))\n\n\n\n应该在哪里绘制缺失值并不明显，因此 ggplot2 不会将它们包含在图中，但它确实警告它们已被删除：\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point()\n#> Warning: Removed 9 rows containing missing values (`geom_point()`).\n\n\n\n\n要抑制该警告，请设置 na.rm = TRUE：\n\nggplot(diamonds2, aes(x = x, y = y)) + \n  geom_point(na.rm = TRUE)\n\n其他时候，您想要了解是什么使具有缺失值的观测值与具有记录值的观测值不同。 例如，在 nycflights13::flights1 中，dep_time 变量中缺失值表示航班已取消。 因此，您可能需要比较已取消和未取消的预定出发时间。 您可以通过创建一个新变量，使用 is.na() 检查 dep_time 是否丢失来做到这一点。\n\nnycflights13::flights |> \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + (sched_min / 60)\n  ) |> \n  ggplot(aes(x = sched_dep_time)) + \n  geom_freqpoly(aes(color = cancelled), binwidth = 1/4)\n\n\n\n\n然而，这个图并不好，因为未取消的航班比取消的航班多得多。 在下一节中，我们将探讨一些改进这种比较的技术。\n\n11.4.1 Exercises\n\n直方图中缺失值会怎样？ 条形图中缺失值会发生什么情况？ 为什么直方图和条形图中缺失值的处理方式存在差异？\nna.rm = TRUE 在 mean() 和 sum() 中做了什么？\n重新创建 scheduled_dep_time 的频率图，根据航班是否取消进行着色。 并通过 cancelled 变量进行分面（facet）。 在分面函数中尝试不同的 scales 变量值，以减轻未取消航班多于已取消航班的影响。"
  },
  {
    "objectID": "EDA.html#covariation",
    "href": "EDA.html#covariation",
    "title": "11  Exploratory data analysis",
    "section": "\n11.5 Covariation",
    "text": "11.5 Covariation\nIf variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualize the relationship between two or more variables.\n\n11.5.1 A categorical and a numerical variable\nFor example, let’s explore how the price of a diamond varies with its quality (measured by cut) using geom_freqpoly():\n\nggplot(diamonds, aes(x = price)) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nNote that ggplot2 uses an ordered color scale for cut because it’s defined as an ordered factor variable in the data. You’ll learn more about these in Section 17.6.\nThe default appearance of geom_freqpoly() is not that useful here because the height, determined by the overall count, differs so much across cuts, making it hard to see the differences in the shapes of their distributions.\nTo make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we’ll display the density, which is the count standardized so that the area under each frequency polygon is one.\n\nggplot(diamonds, aes(x = price, y = after_stat(density))) + \n  geom_freqpoly(aes(color = cut), binwidth = 500, linewidth = 0.75)\n\n\n\n\nNote that we’re mapping the density the y, but since density is not a variable in the diamonds dataset, we need to first calculate it. We use the after_stat() function to do so.\nThere’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot.\nA visually simpler plot for exploring this relationship is using side-by-side boxplots.\n\nggplot(diamonds, aes(x = cut, y = price)) +\n  geom_boxplot()\n\n\n\n\nWe see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counter-intuitive finding that better quality diamonds are typically cheaper! In the exercises, you’ll be challenged to figure out why.\ncut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is with fct_reorder(). You’ll learn more about that function in Section 17.4, but we want to give you a quick preview here because it’s so useful. For example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes:\n\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\nTo make the trend easier to see, we can reorder class based on the median value of hwy:\n\nggplot(mpg, aes(x = fct_reorder(class, hwy, median), y = hwy)) +\n  geom_boxplot()\n\n\n\n\nIf you have long variable names, geom_boxplot() will work better if you flip it 90°. You can do that by exchanging the x and y aesthetic mappings.\n\nggplot(mpg, aes(x = hwy, y = fct_reorder(class, hwy, median))) +\n  geom_boxplot()\n\n\n\n\n\n11.5.1.1 Exercises\n\nUse what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.\nBased on EDA, what variable in the diamonds dataset appears to be most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\nInstead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to exchanging the variables?\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs. cut. What do you learn? How do you interpret the plots?\nCreate a visualization of diamond prices vs. a categorical variable from the diamonds dataset using geom_violin(), then a faceted geom_histogram(), then a colored geom_freqpoly(), and then a colored geom_density(). Compare and contrast the four plots. What are the pros and cons of each method of visualizing the distribution of a numerical variable based on the levels of a categorical variable?\nIf you have a small dataset, it’s sometimes useful to use geom_jitter() to avoid overplotting to more easily see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.\n\n11.5.2 Two categorical variables\nTo visualize the covariation between categorical variables, you’ll need to count the number of observations for each combination of levels of these categorical variables. One way to do that is to rely on the built-in geom_count():\n\nggplot(diamonds, aes(x = cut, y = color)) +\n  geom_count()\n\n\n\n\nThe size of each circle in the plot displays how many observations occurred at each combination of values. Covariation will appear as a strong correlation between specific x values and specific y values.\nAnother approach for exploring the relationship between these variables is computing the counts with dplyr:\n\ndiamonds |> \n  count(color, cut)\n#> # A tibble: 35 × 3\n#>   color cut           n\n#>   <ord> <ord>     <int>\n#> 1 D     Fair        163\n#> 2 D     Good        662\n#> 3 D     Very Good  1513\n#> 4 D     Premium    1603\n#> 5 D     Ideal      2834\n#> 6 E     Fair        224\n#> # ℹ 29 more rows\n\nThen visualize with geom_tile() and the fill aesthetic:\n\ndiamonds |> \n  count(color, cut) |>  \n  ggplot(aes(x = color, y = cut)) +\n  geom_tile(aes(fill = n))\n\n\n\n\nIf the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, you might want to try the heatmaply package, which creates interactive plots.\n\n11.5.2.1 Exercises\n\nHow could you rescale the count dataset above to more clearly show the distribution of cut within color, or color within cut?\nWhat different data insights do you get with a segmented bar chart if color is mapped to the x aesthetic and cut is mapped to the fill aesthetic? Calculate the counts that fall into each of the segments.\nUse geom_tile() together with dplyr to explore how average flight departure delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?\n\n11.5.3 Two numerical variables\nYou’ve already seen one great way to visualize the covariation between two numerical variables: draw a scatterplot with geom_point(). You can see covariation as a pattern in the points. For example, you can see a positive relationship between the carat size and price of a diamond: diamonds with more carats have a higher price. The relationship is exponential.\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_point()\n\n\n\n\n(In this section we’ll use the smaller dataset to stay focused on the bulk of the diamonds that are smaller than 3 carats)\nScatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black, making it hard to judge differences in the density of the data across the 2-dimensional space as well as making it hard to spot the trend. You’ve already seen one way to fix the problem: using the alpha aesthetic to add transparency.\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_point(alpha = 1 / 100)\n\n\n\n\nBut using transparency can be challenging for very large datasets. Another solution is to use bin. Previously you used geom_histogram() and geom_freqpoly() to bin in one dimension. Now you’ll learn how to use geom_bin2d() and geom_hex() to bin in two dimensions.\ngeom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().\n\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_bin2d()\n\n# install.packages(\"hexbin\")\nggplot(smaller, aes(x = carat, y = price)) +\n  geom_hex()\n#> Warning: Computation failed in `stat_binhex()`\n#> Caused by error in `compute_group()`:\n#> ! The package \"hexbin\" is required for `stat_binhex()`\n\n\n\n\n\n\n\n\n\n\n\nAnother option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualizing the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot:\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_width(carat, 0.1)))\n\n\n\n\ncut_width(x, width), as used above, divides x into bins of width width. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summaries a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with varwidth = TRUE.\n\n11.5.3.1 Exercises\n\nInstead of summarizing the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs. cut_number()? How does that impact a visualization of the 2d distribution of carat and price?\nVisualize the distribution of carat, partitioned by price.\nHow does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?\nCombine two of the techniques you’ve learned to visualize the combined distribution of cut, carat, and price.\n\nTwo dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the following plot have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. Why is a scatterplot a better display than a binned plot for this case?\n\ndiamonds |> \n  filter(x >= 4) |> \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))\n\n\n\nInstead of creating boxes of equal width with cut_width(), we could create boxes that contain roughly equal number of points with cut_number(). What are the advantages and disadvantages of this approach?\n\nggplot(smaller, aes(x = carat, y = price)) + \n  geom_boxplot(aes(group = cut_number(carat, 20)))"
  },
  {
    "objectID": "EDA.html#patterns-and-models",
    "href": "EDA.html#patterns-and-models",
    "title": "11  Exploratory data analysis",
    "section": "\n11.6 Patterns and models",
    "text": "11.6 Patterns and models\nIf a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:\n\nCould this pattern be due to coincidence (i.e. random chance)?\nHow can you describe the relationship implied by the pattern?\nHow strong is the relationship implied by the pattern?\nWhat other variables might affect the relationship?\nDoes the relationship change if you look at individual subgroups of the data?\n\nPatterns in your data provide clues about relationships, i.e., they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.\nModels are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. Note that instead of using the raw values of price and carat, we log transform them first, and fit a model to the log-transformed values. Then, we exponentiate the residuals to put them back in the scale of raw prices.\n\nlibrary(tidymodels)\n#> Warning: package 'tidymodels' was built under R version 4.2.3\n#> Warning: package 'broom' was built under R version 4.2.3\n#> Warning: package 'dials' was built under R version 4.2.3\n#> Warning: package 'scales' was built under R version 4.2.1\n#> Warning: package 'infer' was built under R version 4.2.3\n#> Warning: package 'modeldata' was built under R version 4.2.3\n#> Warning: package 'parsnip' was built under R version 4.2.3\n#> Warning: package 'recipes' was built under R version 4.2.3\n#> Warning: package 'rsample' was built under R version 4.2.2\n#> Warning: package 'tune' was built under R version 4.2.3\n#> Warning: package 'workflows' was built under R version 4.2.3\n#> Warning: package 'workflowsets' was built under R version 4.2.3\n#> Warning: package 'yardstick' was built under R version 4.2.3\n\ndiamonds <- diamonds |>\n  mutate(\n    log_price = log(price),\n    log_carat = log(carat)\n  )\n\ndiamonds_fit <- linear_reg() |>\n  fit(log_price ~ log_carat, data = diamonds)\n\ndiamonds_aug <- augment(diamonds_fit, new_data = diamonds) |>\n  mutate(.resid = exp(.resid))\n\nggplot(diamonds_aug, aes(x = carat, y = .resid)) + \n  geom_point()\n\n\n\n\nOnce you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.\n\nggplot(diamonds_aug, aes(x = cut, y = .resid)) + \n  geom_boxplot()\n\n\n\n\nWe’re not discussing modelling in this book because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand."
  },
  {
    "objectID": "EDA.html#summary",
    "href": "EDA.html#summary",
    "title": "11  Exploratory data analysis",
    "section": "\n11.7 Summary",
    "text": "11.7 Summary\nIn this chapter you’ve learned a variety of tools to help you understand the variation within your data. You’ve seen techniques that work with a single variable at a time and with a pair of variables. This might seem painfully restrictive if you have tens or hundreds of variables in your data, but they’re foundation upon which all other techniques are built.\nIn the next chapter, we’ll focus on the tools we can use to communicate our results."
  }
]